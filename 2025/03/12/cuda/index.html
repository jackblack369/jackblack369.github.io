<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="version1234GPU  	driver	cuda	|	vllm	 	torch          image h100	550			12.4  |h200	565			12.7	|	0.7.3		2.5.1+cu124h200	570			12.8	|	0.7.3		2.5.1+cu124    nvcr.io&#x2F;nvidia&#x2F;pytorch:25.01-py3  installCUDA T">
<meta property="og:type" content="article">
<meta property="og:title" content="nvidia cuda">
<meta property="og:url" content="https://www.willshirley.top/2025/03/12/cuda/index.html">
<meta property="og:site_name" content="Clean &amp; Focus">
<meta property="og:description" content="version1234GPU  	driver	cuda	|	vllm	 	torch          image h100	550			12.4  |h200	565			12.7	|	0.7.3		2.5.1+cu124h200	570			12.8	|	0.7.3		2.5.1+cu124    nvcr.io&#x2F;nvidia&#x2F;pytorch:25.01-py3  installCUDA T">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-03-12T07:59:26.000Z">
<meta property="article:modified_time" content="2025-12-30T02:42:30.774Z">
<meta property="article:author" content="brook">
<meta property="article:tag" content="nvidia">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>nvidia cuda</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Clean & Focus" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/archives/">Log</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2025/03/20/clang-uml/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2025/03/12/k8s-point%20ctr/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://www.willshirley.top/2025/03/12/cuda/&text=nvidia cuda"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://www.willshirley.top/2025/03/12/cuda/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=nvidia cuda&body=Check out this article: https://www.willshirley.top/2025/03/12/cuda/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://www.willshirley.top/2025/03/12/cuda/&title=nvidia cuda"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://www.willshirley.top/2025/03/12/cuda/&title=nvidia cuda"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://www.willshirley.top/2025/03/12/cuda/&title=nvidia cuda"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://www.willshirley.top/2025/03/12/cuda/&t=nvidia cuda"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#version"><span class="toc-number">1.</span> <span class="toc-text">version</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#install"><span class="toc-number">2.</span> <span class="toc-text">install</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-Toolkit"><span class="toc-number">2.1.</span> <span class="toc-text">CUDA Toolkit</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#post-operate"><span class="toc-number">2.1.1.</span> <span class="toc-text">post operate</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#driver"><span class="toc-number">2.2.</span> <span class="toc-text">driver</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nvidia-fabricmanager"><span class="toc-number">2.3.</span> <span class="toc-text">nvidia-fabricmanager</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ENV"><span class="toc-number">3.</span> <span class="toc-text">ENV</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cuda-kernel-model"><span class="toc-number">4.</span> <span class="toc-text">cuda kernel model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#command"><span class="toc-number">5.</span> <span class="toc-text">command</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#nvidia-smi"><span class="toc-number">5.1.</span> <span class="toc-text">nvidia-smi</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vllm"><span class="toc-number">6.</span> <span class="toc-text">vllm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#install-offline"><span class="toc-number">6.1.</span> <span class="toc-text">install offline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#running-time"><span class="toc-number">6.2.</span> <span class="toc-text">running time</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sglang"><span class="toc-number">7.</span> <span class="toc-text">sglang</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#install-offline-1"><span class="toc-number">7.1.</span> <span class="toc-text">install offline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#runtime"><span class="toc-number">7.2.</span> <span class="toc-text">runtime</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torch"><span class="toc-number">8.</span> <span class="toc-text">torch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#install-offline-2"><span class="toc-number">8.1.</span> <span class="toc-text">install offline</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#best-practices"><span class="toc-number">9.</span> <span class="toc-text">best practices</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-LAUNCH-BLOCKING-1"><span class="toc-number">9.1.</span> <span class="toc-text">CUDA_LAUNCH_BLOCKING&#x3D;1</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        nvidia cuda
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">brook</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-03-12T07:59:26.000Z" class="dt-published" itemprop="datePublished">2025-03-12</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/cuda/">cuda</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/nvidia/" rel="tag">nvidia</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="version"><a href="#version" class="headerlink" title="version"></a>version</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GPU  	driver	cuda	|	vllm	 	torch          image </span><br><span class="line">h100	550			12.4  |</span><br><span class="line">h200	565			12.7	|	0.7.3		2.5.1+cu124</span><br><span class="line">h200	570			12.8	|	0.7.3		2.5.1+cu124    nvcr.io/nvidia/pytorch:25.01-py3</span><br></pre></td></tr></table></figure>

<h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><h2 id="CUDA-Toolkit"><a href="#CUDA-Toolkit" class="headerlink" title="CUDA Toolkit"></a>CUDA Toolkit</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></p>
</blockquote>
<ul>
<li><p>local</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</span><br><span class="line"></span><br><span class="line">sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600</span><br><span class="line"></span><br><span class="line">wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda-repo-ubuntu2204-12-8-local_12.8.1-570.124.06-1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu2204-12-8-local_12.8.1-570.124.06-1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo cp /var/cuda-repo-ubuntu2204-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get -y install cuda-toolkit-12-8</span><br></pre></td></tr></table></figure></li>
<li><p>online</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb</span><br><span class="line">sudo dpkg -i cuda-keyring_1.1-1_all.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install cuda-toolkit-12-8</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="post-operate"><a href="#post-operate" class="headerlink" title="post operate"></a>post operate</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> config env</span></span><br><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NVIDIA persistence daemon</span></span><br><span class="line">sudo systemctl start nvidia-persistenced</span><br></pre></td></tr></table></figure>

<h2 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y cuda-drivers	</span><br></pre></td></tr></table></figure>

<h2 id="nvidia-fabricmanager"><a href="#nvidia-fabricmanager" class="headerlink" title="nvidia-fabricmanager"></a>nvidia-fabricmanager</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y nvidia-fabricmanager-570</span><br><span class="line"></span><br><span class="line">sudo systemctl start nvidia-fabricmanager</span><br></pre></td></tr></table></figure>

<h1 id="ENV"><a href="#ENV" class="headerlink" title="ENV"></a>ENV</h1><blockquote>
<p>resolve issues: Error 802: system not yet initialized</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sort GPUs, by ordering their IDs with IDs on the PCIe bus.</span></span><br><span class="line">export CUDA_DEVICE_ORDER=&quot;PCI_BUS_ID&quot; </span><br><span class="line"><span class="meta">#</span><span class="bash"> perform an availability check using NVML (NVIDIA Management Library). NVML is an API layer <span class="keyword">for</span> obtaining data directly from the NVIDIA-smi utility.</span></span><br><span class="line">export PYTORCH_NVML_BASED_CUDA_CHECK=1 </span><br><span class="line"><span class="meta">#</span><span class="bash"> force show the system the IDs of available GPUs.</span></span><br><span class="line">export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7</span><br></pre></td></tr></table></figure>

<h1 id="cuda-kernel-model"><a href="#cuda-kernel-model" class="headerlink" title="cuda kernel model"></a>cuda kernel model</h1><ul>
<li>check by <code>lsmod | grep nvidia</code></li>
</ul>
<table>
<thead>
<tr>
<th><strong>Module</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td>nvidia_uvm</td>
<td>NVIDIA’s Unified Memory driver</td>
</tr>
<tr>
<td>nvidia_drm</td>
<td>Direct Rendering Manager support</td>
</tr>
<tr>
<td>nvidia_modeset</td>
<td>Kernel mode-setting support</td>
</tr>
<tr>
<td>nvidia</td>
<td>Main NVIDIA driver module</td>
</tr>
</tbody></table>
<h1 id="command"><a href="#command" class="headerlink" title="command"></a>command</h1><h2 id="nvidia-smi"><a href="#nvidia-smi" class="headerlink" title="nvidia-smi"></a>nvidia-smi</h2><ul>
<li><p>Enable Persistence Mode</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi -pm 1</span><br></pre></td></tr></table></figure></li>
<li><p>check state</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi conf-compute -grs</span><br><span class="line"><span class="meta">#</span><span class="bash"> Confidential Compute GPUs Ready state: not-ready</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Confidential Compute GPUs Ready state: ready</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> above state is not-ready, execute below cmd</span></span><br><span class="line">nvidia-smi conf-compute -srs 1</span><br></pre></td></tr></table></figure></li>
<li><p>cuda 12.8</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nvidia-smi</span></span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |</span><br><span class="line">| N/A   23C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H200                    On  |   00000000:3B:00.0 Off |                    0 |</span><br><span class="line">| N/A   21C    P0             75W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H200                    On  |   00000000:4C:00.0 Off |                    0 |</span><br><span class="line">| N/A   23C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H200                    On  |   00000000:5D:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H200                    On  |   00000000:9B:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             75W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H200                    On  |   00000000:BB:00.0 Off |                    0 |</span><br><span class="line">| N/A   23C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H200                    On  |   00000000:CB:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H200                    On  |   00000000:DB:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                              |</span><br><span class="line">|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |</span><br><span class="line">|        ID   ID                                                               Usage      |</span><br><span class="line">|=========================================================================================|</span><br><span class="line">|  No running processes found                                                             |</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></li>
<li><p>cuda 12.7</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nvidia-smi</span></span><br><span class="line">Fri Mar 14 10:23:56 2025       </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            111W /  700W |  134402MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H200                    On  |   00000000:3B:00.0 Off |                    0 |</span><br><span class="line">| N/A   25C    P0            116W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H200                    On  |   00000000:4C:00.0 Off |                    0 |</span><br><span class="line">| N/A   25C    P0            112W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H200                    On  |   00000000:5D:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            115W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H200                    On  |   00000000:9B:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            115W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H200                    On  |   00000000:BB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            114W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H200                    On  |   00000000:CB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            113W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H200                    On  |   00000000:DB:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0            114W /  700W |  131840MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                              |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |</span><br><span class="line">|        ID   ID                                                               Usage      |</span><br><span class="line">|=========================================================================================|</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>



<h1 id="vllm"><a href="#vllm" class="headerlink" title="vllm"></a>vllm</h1><h2 id="install-offline"><a href="#install-offline" class="headerlink" title="install offline"></a>install offline</h2><p>On your local machine, create a virtual environment:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv vllm_env</span><br><span class="line">source vllm_env/bin/activate</span><br></pre></td></tr></table></figure>

<p>1️⃣ <strong>On your local machine:</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip download --dest=./vllm_deps vllm</span><br></pre></td></tr></table></figure>

<p>2️⃣ <strong>Transfer dependencies to the remote server:</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r vllm_deps user@remote_server:/path/to/destination/</span><br></pre></td></tr></table></figure>

<p>3️⃣ <strong>On the remote server:</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /path/to/destination/vllm_deps</span><br><span class="line">pip install --no-index --find-links=. vllm*</span><br><span class="line">	•	--no-index tells pip not to use the internet.</span><br><span class="line">	•	--find-links=./vllm_deps tells pip to look for packages in this directory.</span><br><span class="line">	•	vllm* ensures pip finds the correct package in that folder.</span><br></pre></td></tr></table></figure>

<h2 id="running-time"><a href="#running-time" class="headerlink" title="running time"></a>running time</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vllm serve /mnt/dingofs-test/DeepSeek-R1 --host 0.0.0.0 --port 8000 --served-model-name deepseek-r1 --tensor-parallel-size 8 --gpu-memory-utilization 0.85 --max-model-len 128000 --max-num-batched-tokens 32000 --max-num-seqs 1024 --trust-remote-code --enable-reasoning --reasoning-parser deepseek_r1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">Sat Mar 15 20:33:04 2025       </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            115W /  700W |   84474MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H200                    On  |   00000000:3B:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0            113W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H200                    On  |   00000000:4C:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            114W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H200                    On  |   00000000:5D:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            117W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H200                    On  |   00000000:9B:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            114W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H200                    On  |   00000000:BB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            116W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H200                    On  |   00000000:CB:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            115W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H200                    On  |   00000000:DB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            114W /  700W |   84282MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                              |</span><br><span class="line">|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |</span><br><span class="line">|        ID   ID                                                               Usage      |</span><br><span class="line">|=========================================================================================|</span><br><span class="line">|    0   N/A  N/A          915920      C   ...niconda3/envs/vllm/bin/python      84464MiB |</span><br><span class="line">|    1   N/A  N/A          916338      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    2   N/A  N/A          916339      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    3   N/A  N/A          916340      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    4   N/A  N/A          916341      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    5   N/A  N/A          916342      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    6   N/A  N/A          916343      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    7   N/A  N/A          916344      C   ...niconda3/envs/vllm/bin/python      84272MiB |</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<ul>
<li><p>log</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INFO 03-15 20:36:48 worker.py:267] Memory profiling takes 7.63 seconds</span><br><span class="line">INFO 03-15 20:36:48 worker.py:267] the current vLLM instance can use total_gpu_memory (139.81GiB) x gpu_memory_utilization (0.85) = 118.84GiB</span><br><span class="line">INFO 03-15 20:36:48 worker.py:267] model weights take 83.88GiB; non_torch_memory takes 7.16GiB; PyTorch activation peak memory takes 6.37GiB; the rest of the memory reserved </span><br><span class="line">for KV Cache is 21.43GiB.</span><br><span class="line">INFO 03-15 20:36:48 executor_base.py:111] # cuda blocks: 18418, # CPU blocks: 3437</span><br><span class="line">INFO 03-15 20:36:48 executor_base.py:116] Maximum concurrency for 128000 tokens per request: 2.30x</span><br></pre></td></tr></table></figure></li>
<li><p>chat</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions \</span><br><span class="line">    -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;deepseek-r1&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;introduce yourself&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="sglang"><a href="#sglang" class="headerlink" title="sglang"></a>sglang</h1><h2 id="install-offline-1"><a href="#install-offline-1" class="headerlink" title="install offline"></a>install offline</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> prepare env</span></span><br><span class="line">python3 -m venv sglang_env</span><br><span class="line">source sglang_env/bin/activate</span><br><span class="line"><span class="meta">#</span><span class="bash"> optional use uv</span></span><br><span class="line">pip install --upgrade pip</span><br><span class="line"><span class="meta">#</span><span class="bash">pip install uv</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> download deps</span></span><br><span class="line">mkdir -p ./sglang_deps</span><br><span class="line">pip download &quot;sglang[all]&gt;=0.4.4.post1&quot; --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python -d ./sglang_deps</span><br><span class="line"><span class="meta">#</span><span class="bash"> scp deps to remote</span></span><br><span class="line">scp -r sglang_deps user@remote_server:/path/to/destination/</span><br><span class="line"><span class="meta">#</span><span class="bash"> install sglang on remote</span></span><br><span class="line">cd /path/to/remote/sglang_deps</span><br><span class="line">pip install --no-index --find-links=. &quot;sglang[all]&gt;=0.4.4.post1&quot;</span><br></pre></td></tr></table></figure>

<h2 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m sglang.launch_server --model /mnt/3fs/DeepSeek-R1 --tp 8 --trust-remote-code --port 30000</span><br></pre></td></tr></table></figure>

<ul>
<li><p>chat</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:30000/v1/chat/completions \</span><br><span class="line">    -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;deepseek-r1&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;introduce yourself&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h1><h2 id="install-offline-2"><a href="#install-offline-2" class="headerlink" title="install offline"></a>install offline</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/torch_deps</span><br><span class="line">pip download --dest=~/torch_deps torch==2.5.1 --extra-index-url https://download.pytorch.org/whl/nightly/cu128</span><br><span class="line"></span><br><span class="line">scp -r ~/torch_deps user@remote_server:/path/to/remote/directory</span><br><span class="line">cd /path/to/remote/directory</span><br><span class="line">pip install --no-index --find-links=./ torch</span><br></pre></td></tr></table></figure>

<ul>
<li><p>check</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c &quot;import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())&quot;</span><br></pre></td></tr></table></figure>

<p>or</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())  <span class="comment"># print false</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.device_count())  <span class="comment"># print 8</span></span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># print 2.5.1+cu124</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># print 12.4</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="best-practices"><a href="#best-practices" class="headerlink" title="best practices"></a>best practices</h1><h2 id="CUDA-LAUNCH-BLOCKING-1"><a href="#CUDA-LAUNCH-BLOCKING-1" class="headerlink" title="CUDA_LAUNCH_BLOCKING=1"></a><code>CUDA_LAUNCH_BLOCKING=1</code></h2><blockquote>
<p>CUDA_LAUNCH_BLOCKING=1 will tell CUDA: “Wait (block) for each GPU kernel to finish before moving to the next line of Python code.”</p>
</blockquote>
<p>Normally, CUDA operations are <strong>asynchronous</strong>—errors may not happen exactly where the code looks wrong, because the kernel may fail later. This can make debugging frustrating.</p>
<p>should never use it in production or performance benchmarking—just for debugging.</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/search/">Search</a></li>
        
          <li><a href="/tags/">Tag</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/archives/">Log</a></li>
        
          <li><a href="/about/">About</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#version"><span class="toc-number">1.</span> <span class="toc-text">version</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#install"><span class="toc-number">2.</span> <span class="toc-text">install</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-Toolkit"><span class="toc-number">2.1.</span> <span class="toc-text">CUDA Toolkit</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#post-operate"><span class="toc-number">2.1.1.</span> <span class="toc-text">post operate</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#driver"><span class="toc-number">2.2.</span> <span class="toc-text">driver</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nvidia-fabricmanager"><span class="toc-number">2.3.</span> <span class="toc-text">nvidia-fabricmanager</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ENV"><span class="toc-number">3.</span> <span class="toc-text">ENV</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cuda-kernel-model"><span class="toc-number">4.</span> <span class="toc-text">cuda kernel model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#command"><span class="toc-number">5.</span> <span class="toc-text">command</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#nvidia-smi"><span class="toc-number">5.1.</span> <span class="toc-text">nvidia-smi</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vllm"><span class="toc-number">6.</span> <span class="toc-text">vllm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#install-offline"><span class="toc-number">6.1.</span> <span class="toc-text">install offline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#running-time"><span class="toc-number">6.2.</span> <span class="toc-text">running time</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sglang"><span class="toc-number">7.</span> <span class="toc-text">sglang</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#install-offline-1"><span class="toc-number">7.1.</span> <span class="toc-text">install offline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#runtime"><span class="toc-number">7.2.</span> <span class="toc-text">runtime</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torch"><span class="toc-number">8.</span> <span class="toc-text">torch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#install-offline-2"><span class="toc-number">8.1.</span> <span class="toc-text">install offline</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#best-practices"><span class="toc-number">9.</span> <span class="toc-text">best practices</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA-LAUNCH-BLOCKING-1"><span class="toc-number">9.1.</span> <span class="toc-text">CUDA_LAUNCH_BLOCKING&#x3D;1</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://www.willshirley.top/2025/03/12/cuda/&text=nvidia cuda"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://www.willshirley.top/2025/03/12/cuda/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=nvidia cuda&body=Check out this article: https://www.willshirley.top/2025/03/12/cuda/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://www.willshirley.top/2025/03/12/cuda/&title=nvidia cuda"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://www.willshirley.top/2025/03/12/cuda/&title=nvidia cuda"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://www.willshirley.top/2025/03/12/cuda/&title=nvidia cuda"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://www.willshirley.top/2025/03/12/cuda/&t=nvidia cuda"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2026
    brook
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/archives/">Log</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
