<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>source is the essence</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2023-09-25T11:59:24.440Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>brook</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ignite sqlline</title>
    <link href="http://yoursite.com/2023/09/25/ignite%20sqlline/"/>
    <id>http://yoursite.com/2023/09/25/ignite%20sqlline/</id>
    <published>2023-09-25T11:00:45.000Z</published>
    <updated>2023-09-25T11:59:24.440Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>TIPS:</p><ol><li><p>Keep in mind that, in Ignite, the concepts of a SQL table and a key-value cache are two equivalent representations of the same (internal) data structure. You can access your data using either the key-value API or SQL statements, or both.</p></li><li><p>A cache is a collection of key-value pairs that can be accessed through the key-value API. A SQL table in Ignite corresponds to the notion of tables in traditional RDBMSs with some additional constraints; for example, each SQL table must have a primary key.</p><p>A table with a primary key can be presented as a key-value cache, in which the primary key column serves as the key, and the rest of the table columns represent the fields of the object (the value).</p></li></ol></blockquote><h2 id="登录"><a href="#登录" class="headerlink" title="登录"></a>登录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">IGNITE_HOME/bin/sqlline.sh --verbose=<span class="literal">true</span> -u jdbc:ignite:thin://ip地址/PUBLIC</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;TIPS:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Keep in mind that, in Ignite, the concepts of a SQL table and a key-value cache are two equivalent rep</summary>
      
    
    
    
    <category term="ignite" scheme="http://yoursite.com/categories/ignite/"/>
    
    
    <category term="command" scheme="http://yoursite.com/tags/command/"/>
    
  </entry>
  
  <entry>
    <title>kafka plain &amp; sasl_scram</title>
    <link href="http://yoursite.com/2023/09/11/kafka%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AEplain%E4%B8%8Esasl_scram/"/>
    <id>http://yoursite.com/2023/09/11/kafka%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AEplain%E4%B8%8Esasl_scram/</id>
    <published>2023-09-11T11:58:47.000Z</published>
    <updated>2023-09-12T13:30:21.683Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>已验证 kafka 2.3.1</p><p>此方案可以动态创建用户，或修改用户账号信息</p></blockquote><blockquote><p>SASL（Simple Authentication and Security Layer）</p><p>参考 <a href="https://kafka.apache.org/documentation/#security_sasl_scram">https://kafka.apache.org/documentation/#security_sasl_scram</a></p></blockquote><h1 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h1><blockquote><p>环境 zookeeper端口22181 ，kafka broker端口39092</p><p>无需重启zookeeper</p></blockquote><h2 id="第一步：创建-SCRAM-证书"><a href="#第一步：创建-SCRAM-证书" class="headerlink" title="第一步：创建 SCRAM 证书"></a>第一步：创建 SCRAM 证书</h2><blockquote><p>在broker启动之前</p></blockquote><ul><li><p>创建admin用户证书</p><p>启动之前（必须）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper 172.20.58.93:22181 --alter --add-config &#x27;SCRAM-SHA-256=[password=datacanvas],SCRAM-SHA-512=[password=datacanvas]&#x27; --entity-type users --entity-name admin</span><br></pre></td></tr></table></figure><blockquote><p>会在 zookeeper生产目录 config，上面zookeeper参数值与kafka server.properties的zookeeper connect配置一致，也和offsetExplorer的chroot path一致</p></blockquote></li><li><p>列出用户已有证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper 172.20.58.93:22181 --describe --entity-type users --entity-name alice</span><br></pre></td></tr></table></figure></li><li><p>删除用户证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper 172.20.58.93:22181 --alter --delete-config &#x27;SCRAM-SHA-512&#x27; --entity-type users --entity-name alice</span><br></pre></td></tr></table></figure></li><li><p>创建普通用户alice证书</p><blockquote><p>可在启动之前或启动之后皆可</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper 172.20.58.93:22181 --alter --add-config &#x27;SCRAM-SHA-256=[iterations=8192,password=alice-secret]&#x27; --entity-type users --entity-name alice</span><br></pre></td></tr></table></figure></li></ul><h2 id="第一步：准备kafka-server-jaas-conf文件"><a href="#第一步：准备kafka-server-jaas-conf文件" class="headerlink" title="第一步：准备kafka_server_jaas.conf文件"></a>第一步：准备kafka_server_jaas.conf文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">    org.apache.kafka.common.security.scram.ScramLoginModule required</span><br><span class="line">    username=&quot;admin&quot;</span><br><span class="line">    password=&quot;datacanvas&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="第二步：设置kafka-server-start-sh"><a href="#第二步：设置kafka-server-start-sh" class="headerlink" title="第二步：设置kafka-server-start.sh"></a>第二步：设置kafka-server-start.sh</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_OPTS=<span class="string">&quot;-Djava.security.auth.login.config=/home/sasl/kafka_2.13-3.3.2/config/kafka_server_jaas.conf&quot;</span></span><br></pre></td></tr></table></figure><ul><li>注意修改路径</li></ul><h2 id="第三步：设置server-properties"><a href="#第三步：设置server-properties" class="headerlink" title="第三步：设置server.properties"></a>第三步：设置server.properties</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:29092</span></span><br><span class="line"><span class="meta">advertised.listeners</span>=<span class="string">PLAINTEXT://172.20.58.93:9092,SASL_PLAINTEXT://172.20.58.93:29092</span></span><br><span class="line"><span class="meta">security.inter.broker.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">sasl.mechanism.inter.broker.protocol</span>=<span class="string">SCRAM-SHA-256</span></span><br><span class="line"><span class="meta">sasl.enabled.mechanisms</span>=<span class="string">SCRAM-SHA-256</span></span><br></pre></td></tr></table></figure><ul><li><p>listeners设置了两种协议 明文连接PLAINTEXT 和 安全连接 SASL_SSL</p><ul><li><code>0.0.0.0</code> 表示 Kafka Broker 将监听所有可用的网络接口，这意味着它将接受来自任何 IP 地址的连接请求。</li></ul></li><li><p>注意zookeeer存储的位置 (/brokers)</p></li><li><p>设置默认副本数 default.replication.factor=3 和 num.partitions=1</p></li><li><p>测试使用<code>log.dirs=/home/sasl/data/kafka-logs</code></p></li><li><p> ACLs相关配置</p></li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer 旧版本配置</span></span><br><span class="line"><span class="meta">authorizer.class.name</span>=<span class="string">kafka.security.authorizer.AclAuthorizer</span></span><br><span class="line"><span class="comment"># 这里添加ANONYMOUS为超级用户，主要为了listener for plain(如何只用sasl，可以不配置ANONYMOUS)</span></span><br><span class="line"><span class="meta">super.users</span>=<span class="string">User:admin;User:ANONYMOUS </span></span><br><span class="line"><span class="meta">allow.everyone.if.no.acl.found</span>=<span class="string">false </span></span><br></pre></td></tr></table></figure><blockquote><p> 默认为true,默认情况只通过用户密码认证管控用户，acl只会对–deny-principal起效（所以默认同时使用 plain和scram，需要保持默认true。如果单独使用scram，则需要设置为false）</p></blockquote><h2 id="第四步：设置acl-config-properties"><a href="#第四步：设置acl-config-properties" class="headerlink" title="第四步：设置acl-config.properties"></a>第四步：设置acl-config.properties</h2><p>在config目录新增acl-config.properties设置 admin信息</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;admin&quot; password=&quot;datacanvas&quot;;</span></span><br><span class="line"><span class="meta">security.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">sasl.mechanism</span>=<span class="string">SCRAM-SHA-256</span></span><br></pre></td></tr></table></figure><h2 id="第五步：启动kafka-server"><a href="#第五步：启动kafka-server" class="headerlink" title="第五步：启动kafka server"></a>第五步：启动kafka server</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">bin/kafka-server-start.sh</span> <span class="string">-daemon ./config/server.properties</span></span><br></pre></td></tr></table></figure><h2 id="第六步：配置ACL授权"><a href="#第六步：配置ACL授权" class="headerlink" title="第六步：配置ACL授权"></a>第六步：配置ACL授权</h2><ul><li><p>授权bigdata用户可以访问主题前缀为ODS的数据，且限制消费组 GROUP-BIGDATA</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-acls.sh --bootstrap-server 172.20.58.93:29092 --command-config /path/to/config/acl-config.properties --add --allow-principal User:bigdata --operation Read --group GROUP-BIGDATA --topic ODS --resource-pattern-type prefixed</span><br></pre></td></tr></table></figure><ul><li>–resource-pattern-type prefixed 指定ODS前缀</li></ul></li><li><p>移除权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config /home/sasl/kafka_2.13-3.3.2/config/acl-config.properties --remove --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic Test-topic </span><br></pre></td></tr></table></figure></li></ul><ul><li><p>禁止删除指定主题的权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-acls.sh --bootstrap-server 172.20.58.93:29092 --command-config /home/sasl/kafka_2.13-3.3.2/config/acl-config.properties --add --deny-principal User:bigdata --operation Write --operation Delete --topic ODS --resource-pattern-type prefixed</span><br></pre></td></tr></table></figure></li></ul><hr><h1 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h1><ul><li><p>PLAIN连接保持原先操作</p></li><li><p>SASL_PLAINTEXT</p><p>连接配置添加用户登录信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;admin&quot; password=&quot;datacanvas&quot;;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;已验证 kafka 2.3.1&lt;/p&gt;
&lt;p&gt;此方案可以动态创建用户，或修改用户账号信息&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;SASL（Simple Authentication and Security Layer</summary>
      
    
    
    
    <category term="kafka" scheme="http://yoursite.com/categories/kafka/"/>
    
    
    <category term="config" scheme="http://yoursite.com/tags/config/"/>
    
  </entry>
  
  <entry>
    <title>kafka plain &amp; sasl_plain</title>
    <link href="http://yoursite.com/2023/09/11/kafka%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AEplain%E4%B8%8Esasl_plain/"/>
    <id>http://yoursite.com/2023/09/11/kafka%E5%90%8C%E6%97%B6%E9%85%8D%E7%BD%AEplain%E4%B8%8Esasl_plain/</id>
    <published>2023-09-11T11:57:52.000Z</published>
    <updated>2023-09-18T09:32:44.572Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>已验证Kafka 3.3.2</p><p>此方案的缺点，在sasl_plaintext模式下，不能动态创建用户，或修改用户账号信息</p><p>优点是，无需在zookeeper上配置jaas</p></blockquote><h1 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h1><h2 id="第一步：准备kafka-server-jaas-conf文件"><a href="#第一步：准备kafka-server-jaas-conf文件" class="headerlink" title="第一步：准备kafka_server_jaas.conf文件"></a>第一步：准备kafka_server_jaas.conf文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">    org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">    username=&quot;admin&quot;</span><br><span class="line">    password=&quot;datacanvas&quot;</span><br><span class="line">    user_admin=&quot;datacanvas&quot;</span><br><span class="line">    user_qlb=&quot;qlbrtdsp&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="第二步：设置kafka-server-start-sh"><a href="#第二步：设置kafka-server-start-sh" class="headerlink" title="第二步：设置kafka-server-start.sh"></a>第二步：设置kafka-server-start.sh</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_OPTS=<span class="string">&quot;-Djava.security.auth.login.config=/home/sasl/kafka_2.13-3.3.2/config/kafka_server_jaas.conf&quot;</span></span><br></pre></td></tr></table></figure><ul><li>注意修改路径</li></ul><h2 id="第三步：设置server-properties"><a href="#第三步：设置server-properties" class="headerlink" title="第三步：设置server.properties"></a>第三步：设置server.properties</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:29092</span></span><br><span class="line"><span class="meta">advertised.listeners</span>=<span class="string">PLAINTEXT://172.20.58.93:9092,SASL_PLAINTEXT://172.20.58.93:29092</span></span><br><span class="line"><span class="meta">security.inter.broker.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">sasl.mechanism.inter.broker.protocol</span>=<span class="string">PLAIN</span></span><br><span class="line"><span class="meta">sasl.enabled.mechanisms</span>=<span class="string">PLAIN</span></span><br></pre></td></tr></table></figure><ul><li><p>listeners设置了两种协议 明文连接PLAINTEXT 和 安全连接 SASL_PLAINTEXT</p><ul><li><code>0.0.0.0</code> 表示 Kafka Broker 将监听所有可用的网络接口，这意味着它将接受来自任何 IP 地址的连接请求。</li></ul></li><li><p>注意zookeeer存储的位置 (/brokers)</p></li><li><p>设置默认副本数 default.replication.factor=3 和 num.partitions=1</p></li><li><p>测试使用<code>log.dirs=/home/sasl/data/kafka-logs</code></p></li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer 旧版本配置</span></span><br><span class="line"><span class="meta">authorizer.class.name</span>=<span class="string">kafka.security.authorizer.AclAuthorizer</span></span><br><span class="line"><span class="meta">super.users</span>=<span class="string">User:admin;User:ANONYMOUS</span></span><br><span class="line"><span class="meta">allow.everyone.if.no.acl.found</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure><h2 id="第四步：设置acl-config-properties"><a href="#第四步：设置acl-config-properties" class="headerlink" title="第四步：设置acl-config.properties"></a>第四步：设置acl-config.properties</h2><p>在config目录新增acl-config.properties设置 admin信息</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;datacanvas&quot;;</span></span><br><span class="line"><span class="meta">security.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">sasl.mechanism</span>=<span class="string">PLAIN</span></span><br></pre></td></tr></table></figure><h2 id="第五步：启动kafka-server"><a href="#第五步：启动kafka-server" class="headerlink" title="第五步：启动kafka server"></a>第五步：启动kafka server</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">bin/kafka-server-start.sh</span> <span class="string">-daemon ./config/server.properties</span></span><br></pre></td></tr></table></figure><h2 id="第六步：配置ACL授权"><a href="#第六步：配置ACL授权" class="headerlink" title="第六步：配置ACL授权"></a>第六步：配置ACL授权</h2><ul><li><p>授权bigdata用户可以访问主题前缀为ODS的数据，且限制消费组 GROUP-BIGDATA</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-acls.sh --bootstrap-server 172.20.58.93:29092 --command-config /home/sasl/kafka_2.13-3.3.2/config/acl-config.properties --add --allow-principal User:bigdata --operation Read --topic ODS* --group GROUP-BIGDATA --group &#x27;GROUP-BIGDATA&#x27;</span><br></pre></td></tr></table></figure></li></ul><hr><h1 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h1><ul><li><p>PLAINTEXT连接保持原先操作</p></li><li><p>SASL_PLAINTEXT</p><p>连接配置添加用户登录信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;datacanvas&quot;;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;已验证Kafka 3.3.2&lt;/p&gt;
&lt;p&gt;此方案的缺点，在sasl_plaintext模式下，不能动态创建用户，或修改用户账号信息&lt;/p&gt;
&lt;p&gt;优点是，无需在zookeeper上配置jaas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;</summary>
      
    
    
    
    <category term="kafka" scheme="http://yoursite.com/categories/kafka/"/>
    
    
    <category term="config" scheme="http://yoursite.com/tags/config/"/>
    
  </entry>
  
  <entry>
    <title>flink Q&amp;A</title>
    <link href="http://yoursite.com/2023/09/01/flink%20Q&amp;A/"/>
    <id>http://yoursite.com/2023/09/01/flink%20Q&amp;A/</id>
    <published>2023-09-01T03:29:50.000Z</published>
    <updated>2023-09-06T07:25:21.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="依赖冲突"><a href="#依赖冲突" class="headerlink" title="依赖冲突"></a>依赖冲突</h2><p><a href="https://flink.apache.org/getting-help/#i-see-a-classcastexception-x-cannot-be-cast-to-x">https://flink.apache.org/getting-help/#i-see-a-classcastexception-x-cannot-be-cast-to-x</a></p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/debugging/debugging_classloading">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/debugging/debugging_classloading</a></p><p>排查办法</p><ul><li>使用arthas定位依赖类所在jar包</li></ul><p>解决办法</p><ul><li><p>对冲突jar包使用maven shaded 的exclude或relocation 进行操作</p></li><li><p>使用7zip等解压软件对jar文件解压删除（人工shaded）</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;依赖冲突&quot;&gt;&lt;a href=&quot;#依赖冲突&quot; class=&quot;headerlink&quot; title=&quot;依赖冲突&quot;&gt;&lt;/a&gt;依赖冲突&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://flink.apache.org/getting-help/#i-see-a-classc</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="Q&amp;A" scheme="http://yoursite.com/tags/Q-A/"/>
    
  </entry>
  
  <entry>
    <title>JVM class loader</title>
    <link href="http://yoursite.com/2023/08/31/JVM%20classLoader/"/>
    <id>http://yoursite.com/2023/08/31/JVM%20classLoader/</id>
    <published>2023-08-31T08:06:42.000Z</published>
    <updated>2023-08-31T08:06:57.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="class冲突"><a href="#class冲突" class="headerlink" title="class冲突"></a>class冲突</h2><p>可以打印出类的加载顺序，可以用来排查 class 的冲突问题：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+TraceClassLoading</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;class冲突&quot;&gt;&lt;a href=&quot;#class冲突&quot; class=&quot;headerlink&quot; title=&quot;class冲突&quot;&gt;&lt;/a&gt;class冲突&lt;/h2&gt;&lt;p&gt;可以打印出类的加载顺序，可以用来排查 class 的冲突问题：&lt;/p&gt;
&lt;figure class=</summary>
      
    
    
    
    <category term="jvm" scheme="http://yoursite.com/categories/jvm/"/>
    
    
    <category term="command" scheme="http://yoursite.com/tags/command/"/>
    
  </entry>
  
  <entry>
    <title>dameng snippet</title>
    <link href="http://yoursite.com/2023/08/21/dameng%20snippet/"/>
    <id>http://yoursite.com/2023/08/21/dameng%20snippet/</id>
    <published>2023-08-21T06:45:05.000Z</published>
    <updated>2023-08-21T06:45:27.552Z</updated>
    
    <content type="html"><![CDATA[<ul><li>连接url带上schema信息</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;连接url带上schema信息&lt;/li&gt;
&lt;/ul&gt;
</summary>
      
    
    
    
    <category term="snippet" scheme="http://yoursite.com/categories/snippet/"/>
    
    
    <category term="dameng" scheme="http://yoursite.com/tags/dameng/"/>
    
  </entry>
  
  <entry>
    <title>flink upsert stream</title>
    <link href="http://yoursite.com/2023/08/10/flink%20upsert%20stream/"/>
    <id>http://yoursite.com/2023/08/10/flink%20upsert%20stream/</id>
    <published>2023-08-10T02:47:36.000Z</published>
    <updated>2023-10-12T07:03:40.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="upsert-kafka"><a href="#upsert-kafka" class="headerlink" title="upsert kafka"></a>upsert kafka</h1><p>The Kafka connector in Flink SQL can work in two streaming modes. </p><p><strong>Upsert mode</strong> allows us to get the latest value for a specific entity automatically without any manual deduplication. One of the typical scenarios where you can leverage this mode is a SQL join of two tables, where one of the tables is keeping history of changes per some entity id. Once you join on such an entity id which is non-unique by design, you get unwanted rows, but you usually want to see the latest value of that entity. With upsert mode, Flink automatically normalizes before the tables are joined. Eventually it allows you to easily answer typical business questions on getting a real-time view of the shared resources like cars, planes, workers, etc.</p><p><strong>Append mode</strong> is still an option to go with, if a business query does not need to filter out all historical events, but rather show the history of changes at the end. In this scenario, query may run faster with append mode, as Flink does not need to do any changelog normalization.</p><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul><li>upsert-kafka 可以配合 checkpoint使用，这样不需要再从头消费之前的数据</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;upsert-kafka&quot;&gt;&lt;a href=&quot;#upsert-kafka&quot; class=&quot;headerlink&quot; title=&quot;upsert kafka&quot;&gt;&lt;/a&gt;upsert kafka&lt;/h1&gt;&lt;p&gt;The Kafka connector in Flink S</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="learn" scheme="http://yoursite.com/tags/learn/"/>
    
  </entry>
  
  <entry>
    <title>flink duduplication</title>
    <link href="http://yoursite.com/2023/08/02/flink%20deduplication/"/>
    <id>http://yoursite.com/2023/08/02/flink%20deduplication/</id>
    <published>2023-08-02T06:37:35.000Z</published>
    <updated>2023-08-02T06:47:54.512Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [column_list]</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"></span><br><span class="line">   <span class="keyword">SELECT</span> [column_list],</span><br><span class="line"></span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> ([<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2...]]</span><br><span class="line"></span><br><span class="line">       <span class="keyword">ORDER</span> <span class="keyword">BY</span> col1 [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>][, col2 [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>]...]) <span class="keyword">AS</span> rownum</span><br><span class="line"></span><br><span class="line">   <span class="keyword">FROM</span> table_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">&lt;=</span> N [<span class="keyword">AND</span> conditions]</span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>ROW_NUMBER()</td><td>计算行号的OVER窗口函数。行号从1开始计算。</td></tr><tr><td>PARTITION BY col1[, col2..]</td><td>可选。指定分区的列，即去重的KEYS。</td></tr><tr><td>ORDER BY timeAttributeCol asc desc</td><td>指定排序的列，必须是一个的字段（即Proctime或Rowtime）。可以指定顺序（Keep FirstRow）或者倒序 （Keep LastRow）。</td></tr><tr><td>rownum</td><td>外层查询中对排名进行过滤，只取前N条</td></tr></tbody></table><ul><li>Deduplicate Keep FirstRow保留首行的去重策略：保留KEY下第一条出现的数据，之后出现该KEY下的数据会被丢弃掉。因为STATE中只存储了KEY数据，所以性能较优。</li><li>Deduplicate Keep LastRow保留末行的去重策略：保留KEY下最后一条出现的数据。因此过程中会产生变更的记录，会向下游发送变更的消息。因此，sink表需要支持retract操作。</li></ul></li></ul><p>在 Deduplication 关于是否会出现回撤流</p><ol><li>⭐ Order by 事件时间 DESC：会出现回撤流，因为当前 key 下 <code>可能会有</code> 比当前事件时间还大的数据</li><li>⭐ Order by 事件时间 ASC：会出现回撤流，因为当前 key 下 <code>可能会有</code> 比当前事件时间还小的数据</li><li>⭐ Order by 处理时间 DESC：会出现回撤流，因为当前 key 下 <code>可能会有</code> 比当前处理时间还大的数据</li><li>⭐ Order by 处理时间 ASC：不会出现回撤流，因为当前 key 下 <code>不可能会有</code> 比当前处理时间还小的数据</li></ol><hr><p>reference</p><p><a href="https://www.modb.pro/db/232004">https://www.modb.pro/db/232004</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;语法&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/spa</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="去重" scheme="http://yoursite.com/tags/%E5%8E%BB%E9%87%8D/"/>
    
  </entry>
  
  <entry>
    <title>python points</title>
    <link href="http://yoursite.com/2023/07/24/python%20points/"/>
    <id>http://yoursite.com/2023/07/24/python%20points/</id>
    <published>2023-07-24T02:37:43.000Z</published>
    <updated>2023-07-24T05:54:18.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h2><ul><li><p>安装tar.gz，解压之后，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 setup.py install</span><br></pre></td></tr></table></figure></li><li><p>查看指定依赖版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 show package_name</span><br></pre></td></tr></table></figure></li></ul><h2 id="信创"><a href="#信创" class="headerlink" title="信创"></a>信创</h2><ul><li><p>安装python3-devel</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnf install python3-devel</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;指令&quot;&gt;&lt;a href=&quot;#指令&quot; class=&quot;headerlink&quot; title=&quot;指令&quot;&gt;&lt;/a&gt;指令&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;安装tar.gz，解压之后，执行&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;</summary>
      
    
    
    
    <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
    <category term="points" scheme="http://yoursite.com/tags/points/"/>
    
  </entry>
  
  <entry>
    <title>starrocks point</title>
    <link href="http://yoursite.com/2023/07/07/starrocks%20points/"/>
    <id>http://yoursite.com/2023/07/07/starrocks%20points/</id>
    <published>2023-07-07T06:52:04.000Z</published>
    <updated>2023-07-10T06:36:25.372Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h1><h2 id="注意项"><a href="#注意项" class="headerlink" title="注意项"></a>注意项</h2><ul><li><p>安装前配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export STARROCKS_HOME=xxx</span><br></pre></td></tr></table></figure></li><li><p>启动mysql客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h &lt;fe_ip&gt; -P&lt;fe_query_port&gt; -uroot -p # 密码为空，直接回车即可</span><br></pre></td></tr></table></figure></li><li><p>启动FE时</p><ul><li><p>启动FE为LEADER节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./fe/bin/start_fe.sh --daemon</span><br></pre></td></tr></table></figure></li></ul></li><li><p>添加新FE</p><ul><li><p>在mysql command先将实例添加进集群，然后逐个启动实例</p></li><li><p>各个节点的时间一定要同步，不然FE的心跳超过5s时差，就会报错</p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;安装部署&quot;&gt;&lt;a href=&quot;#安装部署&quot; class=&quot;headerlink&quot; title=&quot;安装部署&quot;&gt;&lt;/a&gt;安装部署&lt;/h1&gt;&lt;h2 id=&quot;注意项&quot;&gt;&lt;a href=&quot;#注意项&quot; class=&quot;headerlink&quot; title=&quot;注意项&quot;&gt;&lt;/a&gt;注意</summary>
      
    
    
    
    <category term="olap" scheme="http://yoursite.com/categories/olap/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>XC point</title>
    <link href="http://yoursite.com/2023/03/13/xc/"/>
    <id>http://yoursite.com/2023/03/13/xc/</id>
    <published>2023-03-13T08:33:47.000Z</published>
    <updated>2023-03-13T08:34:12.630Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h1><ul><li>ARM架构</li></ul><p>​    飞腾CPU、鲲鹏CPU</p><ul><li>X86架构</li></ul><p>​    海光CPU（与AMD合作）</p><p>​    兆芯、龙芯、申威</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CPU&quot;&gt;&lt;a href=&quot;#CPU&quot; class=&quot;headerlink&quot; title=&quot;CPU&quot;&gt;&lt;/a&gt;CPU&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;ARM架构&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​    飞腾CPU、鲲鹏CPU&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X86架构&lt;/li&gt;</summary>
      
    
    
    
    <category term="point" scheme="http://yoursite.com/categories/point/"/>
    
    
    <category term="XC" scheme="http://yoursite.com/tags/XC/"/>
    
  </entry>
  
  <entry>
    <title>JVM Command</title>
    <link href="http://yoursite.com/2023/03/06/JVM%20command/"/>
    <id>http://yoursite.com/2023/03/06/JVM%20command/</id>
    <published>2023-03-06T03:23:15.000Z</published>
    <updated>2023-03-06T03:27:33.057Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实践一-查看进程参数"><a href="#实践一-查看进程参数" class="headerlink" title="实践一: 查看进程参数"></a>实践一: 查看进程参数</h1><blockquote><p>查看服务设置的jvm</p><p>jps -v</p><p>查看服务jvm的默认参数</p><p>jinfo -flags PID</p></blockquote><ul><li><p>参考kafka服务的运行参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:MaxGCPauseMillis=20</span><br><span class="line">-XX:InitiatingHeapOccupancyPercent=35</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br></pre></td></tr></table></figure><p><strong>-XX:MaxGCPauseMillis=200</strong></p><p>为所需的最长暂停时间设置目标值。默认值是 200 毫秒。这个数值是一个软目标，也就是说JVM会尽一切能力满足这个暂停要求，但是不能保证每次暂停一定在这个要求之内。</p><p>根据测试发现，如果我们将这个值设定成50毫秒或者更低的话，JVM为了达到这个要求会将年轻代内存空间设定的非常小，从而导致youngGC的频率大大增高。所以我们并不设定这个参数。</p><p><strong>-XX:InitiatingHeapOccupancyPercent=45</strong></p><p>设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。就是说当使用内存占到堆总大小的45%的时候，G1将开始<strong>并发标记阶段。</strong>为混合GC做准备，这个数值在测试的时候我想让混合GC晚一些处理所以设定成了70%，经过观察发现如果这个数值设定过大会导致JVM无法启动并发标记，直接进行FullGC处理。</p><p>G1的FullGC是单线程，一个22G的对GC完成需要8S的时间，所以这个值在调优的时候写的45%</p></li></ul><blockquote><p>之前查看ignite，12秒回收了71G</p></blockquote><h1 id="实践二：查看进程加载的类"><a href="#实践二：查看进程加载的类" class="headerlink" title="实践二：查看进程加载的类"></a>实践二：查看进程加载的类</h1><blockquote><p>jcmd命令要使用启动目标进程的用户执行</p></blockquote><ol><li><p>使用dump内存信息到heap.bin文件<br>使用命令<code>jmap -dump:live,format=b,file=heap.bin pid（进程号）</code>将进程pid的堆栈信息输出到heap.bin文件中</p></li><li><p>使用jhat 对heap.bin 文件进行分析<br>命令<code>jhat -J-mx512m heap.bin</code>， 如果解析过程中出现内存不足，需要加大内存如:<code>jhat -J-mx800m heap.bin</code></p></li><li><p>通过浏览器访问 <code>http://ip:7000/</code>即可看到分析结果。点击每个类，可以查看详细信息，包括该类是被哪个类加载器加载。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;实践一-查看进程参数&quot;&gt;&lt;a href=&quot;#实践一-查看进程参数&quot; class=&quot;headerlink&quot; title=&quot;实践一: 查看进程参数&quot;&gt;&lt;/a&gt;实践一: 查看进程参数&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;查看服务设置的jvm&lt;/p&gt;
&lt;p&gt;jps -</summary>
      
    
    
    
    <category term="jvm" scheme="http://yoursite.com/categories/jvm/"/>
    
    
    <category term="command" scheme="http://yoursite.com/tags/command/"/>
    
  </entry>
  
  <entry>
    <title>problems</title>
    <link href="http://yoursite.com/2022/09/27/problems/"/>
    <id>http://yoursite.com/2022/09/27/problems/</id>
    <published>2022-09-27T09:33:47.000Z</published>
    <updated>2022-09-27T09:56:02.191Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DB2"><a href="#DB2" class="headerlink" title="DB2"></a>DB2</h1><h2 id="DB2-SQL-Error-SQLCODE-805-SQLSTATE-51002-SQLERRMC-NULLID-SYSLH2DA-0X5359534C564C3031"><a href="#DB2-SQL-Error-SQLCODE-805-SQLSTATE-51002-SQLERRMC-NULLID-SYSLH2DA-0X5359534C564C3031" class="headerlink" title="DB2 SQL Error: SQLCODE=-805, SQLSTATE=51002, SQLERRMC=NULLID.SYSLH2DA 0X5359534C564C3031"></a>DB2 SQL Error: SQLCODE=-805, SQLSTATE=51002, SQLERRMC=NULLID.SYSLH2DA 0X5359534C564C3031</h2><ul><li><p>分析</p><p>the problem was a list of operations made with the same PreparedStatement, which was never closed.</p><p>许多操作使用同一个PreparedStatement，但是重来没有关闭</p></li><li><p>参考</p><p><a href="https://cloud.tencent.com/developer/article/1837198">https://cloud.tencent.com/developer/article/1837198</a></p></li></ul><h1 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h1><h2 id="lambda是否可打日志"><a href="#lambda是否可打日志" class="headerlink" title="lambda是否可打日志"></a>lambda是否可打日志</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DB2&quot;&gt;&lt;a href=&quot;#DB2&quot; class=&quot;headerlink&quot; title=&quot;DB2&quot;&gt;&lt;/a&gt;DB2&lt;/h1&gt;&lt;h2 id=&quot;DB2-SQL-Error-SQLCODE-805-SQLSTATE-51002-SQLERRMC-NULLID-SYSL</summary>
      
    
    
    
    <category term="problems" scheme="http://yoursite.com/categories/problems/"/>
    
    
    <category term="points" scheme="http://yoursite.com/tags/points/"/>
    
  </entry>
  
  <entry>
    <title>antlr point</title>
    <link href="http://yoursite.com/2022/04/20/antlr%20point/"/>
    <id>http://yoursite.com/2022/04/20/antlr%20point/</id>
    <published>2022-04-20T02:46:03.000Z</published>
    <updated>2022-04-20T02:46:15.949Z</updated>
    
    <content type="html"><![CDATA[<hr><p><a href="https://wizardforcel.gitbooks.io/antlr4-short-course/content/getting-started.html">简明教程</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;https://wizardforcel.gitbooks.io/antlr4-short-course/content/getting-started.html&quot;&gt;简明教程&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="java" scheme="http://yoursite.com/categories/java/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>java dynamic compile</title>
    <link href="http://yoursite.com/2022/04/07/java%20dynamic%20compile/"/>
    <id>http://yoursite.com/2022/04/07/java%20dynamic%20compile/</id>
    <published>2022-04-07T06:52:06.000Z</published>
    <updated>2022-04-07T06:56:26.908Z</updated>
    
    <content type="html"><![CDATA[<p>静态编译：编译时就把所有用到的Java代码全都编译成字节码，是一次性编译。</p><p>动态编译：在Java程序运行时才把需要的Java代码的编译成字节码，是按需编译。</p><p>从JDK1.6开始，引入了Java代码重写过的编译器接口，使得我们可以在运行时编译Java源代码，然后再通过类加载器将编译好的类加载进JVM,这种在运行时编译代码的操作就叫做动态编译。</p><hr><p><a href="https://blog.nowcoder.net/n/d2a7554ea2ec4e4b978cf4a74c3c41b2">【Java动态编译】动态编译的应用_牛客博客</a></p><p><a href="https://segmentfault.com/a/1190000016842546">Java动态性(1) - 动态编译(DynamicCompile)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;静态编译：编译时就把所有用到的Java代码全都编译成字节码，是一次性编译。&lt;/p&gt;
&lt;p&gt;动态编译：在Java程序运行时才把需要的Java代码的编译成字节码，是按需编译。&lt;/p&gt;
&lt;p&gt;从JDK1.6开始，引入了Java代码重写过的编译器接口，使得我们可以在运行时编译Jav</summary>
      
    
    
    
    <category term="java" scheme="http://yoursite.com/categories/java/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>hbase point</title>
    <link href="http://yoursite.com/2022/04/01/hbase%20point/"/>
    <id>http://yoursite.com/2022/04/01/hbase%20point/</id>
    <published>2022-04-01T02:01:30.000Z</published>
    <updated>2022-04-20T02:50:55.101Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>节点</th><th>端口号</th><th>协议</th><th>使用</th><th>说明</th></tr></thead><tbody><tr><td>zookeeper</td><td>2181</td><td></td><td>zkCli.sh -server zookeeper1:2181</td><td>客户端接入</td></tr><tr><td>2888,3888</td><td></td><td>N/A</td><td>集群内部通讯</td><td></td></tr><tr><td>HDFS Namenode</td><td>9000</td><td>HDFS</td><td>hdfs dfs -ls hdfs://namenode1:9000/</td><td>客户端接入</td></tr><tr><td>50070</td><td>HTTP</td><td><a href="http://namenode1:50070/">http://namenode1:50070/</a></td><td>集群监控</td><td></td></tr><tr><td>HDFS SecondaryNamenode</td><td>50090</td><td>HTTP</td><td><a href="http://namenode1:50090/">http://namenode1:50090/</a></td><td>secondary监控</td></tr><tr><td>HDFS Datanode</td><td>50010</td><td></td><td>N/A</td><td>客户端接入/其他节点接入</td></tr><tr><td>50020</td><td></td><td>N/A</td><td></td><td></td></tr><tr><td>50075</td><td>HTTP</td><td><a href="http://datanode1:50075/">http://datanode1:50075/</a></td><td>节点监控</td><td></td></tr><tr><td>HBase Master</td><td>16000</td><td></td><td>hbase-client-1.x.x.jar</td><td>RegionServer接入</td></tr><tr><td>16010</td><td>HTTP</td><td><a href="http://namenode1:16010/">http://namenode1:16010/</a></td><td>集群监控</td><td></td></tr><tr><td>HBase RegionServer</td><td>16020</td><td></td><td>N/A</td><td>客户端接入</td></tr><tr><td>16030</td><td>HTTP</td><td><a href="http://datanode1:16030/">http://datanode1:16030/</a></td><td>节点监控</td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点&lt;/th&gt;
&lt;th&gt;端口号&lt;/th&gt;
&lt;th&gt;协议&lt;/th&gt;
&lt;th&gt;使用&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;zookeeper&lt;/td&gt;
&lt;td&gt;2181&lt;/td</summary>
      
    
    
    
    <category term="hbase" scheme="http://yoursite.com/categories/hbase/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>flink module</title>
    <link href="http://yoursite.com/2022/02/16/flink%20module/"/>
    <id>http://yoursite.com/2022/02/16/flink%20module/</id>
    <published>2022-02-15T16:59:01.425Z</published>
    <updated>2022-02-16T07:11:46.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><ul><li>flink-annotations: Flink自定义的一些注解，用于配置、控制编译等功能。</li><li>flink-clients: Flink客户端，用于向Flink集群提交任务、查询状态等。其中org.apache.flink.client.cli.CliFrontend就是执行./flink run的入口。</li><li>flink-connectors: Flink连接器，相当于Flink读写外部系统的客户端。这些连接器指定了外部存储如何作为Flink的source或sink。例如对于kafka来说，flink-connector-kafka-xx定义了FlinkKafkaConsumer和FlinkKafkaProducer类分别作为Flink的source和sink，实现了对kafka消费和生产的功能。从图二可以看出，flink 1.9目前支持的外部存储有Cassandra、ES、Kafka、Hive等一些开源外部存储。</li><li>flink-container: Flink对docker和kubernetes的支持。</li><li>flink-contrib: 社区开发者提供的一些新特性。</li><li>flink-core: Flink核心的API、类型的定义，包括底层的算子、状态、时间的实现，是Flink最重要的部分。Flink内部的各种参数配置也都定义在这个模块的configuration中。（这部分代码还没怎么看过，就不细讲了）。</li><li>flink-dist: Flink编译好之后的jar包会放在这个文件夹下，也就是网上下载的可执行的版本。其中也包括集群启动、终止的脚本，集群的配置文件等。</li><li>flink-docs: 这个模块并不是Flink的文档，而是Flink文档生成的代码。其中org.apache.flink.docs.configuration.ConfigOptionsDocGenerator是配置文档的生成器，修改相关配置的key或者默认值，重新运行这个类就会更新doc文件夹下的html文件。同样org.apache.flink.docs.rest.RestAPIDocGenerator是Flink RestAPI文档的生成器。</li><li>flink-fliesystems: Flink对各种文件系统的支持，包括HDFS、Azure、AWS S3、阿里云OSS等分布式文件系统。</li><li>flink-formats: Flink对各种格式的数据输入输出的支持。包括Json、CSV、Avro等常用的格式。</li><li>flink-java: Flink java的API，就是写flink应用时用到的map、window、keyBy、State等类或函数的实现。</li><li>flink-jepsen: 对Flink分布式系统正确性的测试，主要验证Flink的容错机制。</li><li>flink-libraries: Flink的高级API，包括CEP（复杂事件处理）、Gelly图处理库等。</li><li>flink-mesos: Flink对mesos集群管理的支持。</li><li>flink-metrics: Flink监控上报。支持上报到influxdb、prometheus等监控系统。具体的使用配置可以在flink-core模块的org.apache.flink.configuration.MetricOptions中找到。</li><li>flink-python: Flink对python的支持，目前还比较弱。</li><li>flink-queryable-state: Flink对可查询状态的支持，其中flink-queryable-state-runtime子模块实现了StateClientProxy和StateServer。这两部分都运行在TaskManager上，StateClientProxy负责接收外部请求，StateServe负责管理内部的queryable state。flink-queryable-state-client-java子模块实现了QueryableStateClient，作为外部系统访问queryable state的客户端。</li><li>flink-runtime: flink运行时核心代码，在第二节细说。</li><li>flink-runtime-web: Flink Web Dashboard的实现。默认启动standalone集群后，访问<a href="http://localhost:8081/">http://localhost:8081</a> 出现的界面。</li><li>flink-scala: Flink scala的API。</li><li>flink-scala-shell: Flink提供的scala命令行交互接口。</li><li>flink-state-backends: flink状态存储的方式，目前这个模块中只有RocksDBStateBackend，未来可能会支持更多种的状态存储，以适应不同的业务场景。MemoryStateBackend和FsStateBackend的实现并不在这个目录下，而是在flink-runtime目录下。</li><li>flink-streaming-java: Flink Streaming的java API。</li><li>flink-streaming-scala: Flink Streaming的scala API。</li><li>flink-table: Flink Table API，在第三小节中细说。</li><li>flink-yarn: Flink对yarn集群管理的支持。</li></ul><hr><ul><li><p>flink-runtime模块是Flink最核心的模块之一，实现了Flink的运行时框架，如JobManager、TaskManager、ResourceManager、Scheduler、Checkpoint Coordinator</p></li><li><p>flink-table模块属于Flink的上层API，包括java和scala版本的table-api，以及SQL的解析和SQL的执行。</p><blockquote><p>随着Flink SQL越来越受重视，flink-table从flink-libraries中移了出来，成为了独立的一级目录。Flink 1.9中，阿里把blink-planner开源了出来，这样整个flink-table中就有了2个planner。从长期来看，流批的统一是一个趋势，因此blink-planner只使用了StreamTableEnvironment中相关的API，而没有使用BatchTableEnvironment，将批当做一个有限的流来处理，希望通过这种方式实现流和批的统一。由于blink-table-planner更好的支持流批统一，且性能更好，在未来的版本中，很有可能完全替代flink-table-planner的功能，而flink-table-planner可能将会被移除。</p></blockquote></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;目录结构&quot;&gt;&lt;a href=&quot;#目录结构&quot; class=&quot;headerlink&quot; title=&quot;目录结构&quot;&gt;&lt;/a&gt;目录结构&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;flink-annotations: Flink自定义的一些注解，用于配置、控制编译等功能。&lt;/li&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>mysql HA &amp; keepalived</title>
    <link href="http://yoursite.com/2022/01/19/mysql%20HA/"/>
    <id>http://yoursite.com/2022/01/19/mysql%20HA/</id>
    <published>2022-01-19T07:54:47.000Z</published>
    <updated>2022-04-20T02:54:47.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql数据备份"><a href="#mysql数据备份" class="headerlink" title="mysql数据备份"></a>mysql数据备份</h1><h2 id="方案二：双主机HA部署"><a href="#方案二：双主机HA部署" class="headerlink" title="方案二：双主机HA部署"></a>方案二：双主机HA部署</h2><p><strong>前提</strong>：准备两个机器master1（172.20.3.113）和master2（172.20.3.114），且分别安装了mysql，其中IP地址根据生产具体ip进行替换</p><h3 id="一、配置my-cnf信息"><a href="#一、配置my-cnf信息" class="headerlink" title="一、配置my.cnf信息"></a>一、配置my.cnf信息</h3><ul><li><p>配置/etc/my.cnf文件（从mysql5.7开始不会自动生成my.cnf文件，所以需要手动创建）my.cnf文件内容大致如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8         #设置mysql客户端默认字符集</span><br><span class="line">[mysqld]</span><br><span class="line">port = 3306  #可自行更改端口</span><br><span class="line">basedir=/usr/local/mysql</span><br><span class="line">datadir=/usr/local/mysql/data</span><br><span class="line">max_connections = 500              #最大连接数</span><br><span class="line">log_bin=mysql-bin</span><br><span class="line">server_id = 1                            #机器1设置为1，机器2设置为2</span><br><span class="line">binlog_format=ROW</span><br><span class="line">auto-increment-increment = 2            #字段变化增量值</span><br><span class="line">auto-increment-offset = 1               #机器1设置为1，机器2设置为2</span><br><span class="line">slave-skip-errors = all                 #忽略所有复制产生的错误</span><br><span class="line">gtid_mode=ON</span><br><span class="line">enforce-gtid-consistency=ON</span><br><span class="line"></span><br><span class="line">character-set-server = utf8</span><br><span class="line">default-storage-engine = INNODB</span><br><span class="line">lower_case_table_names = 1</span><br></pre></td></tr></table></figure><ul><li><p>[mysql]代表我们使用mysql命令登录mysql数据库时的默认设置 </p></li><li><p>[mysqld]代表数据库自身的默认设置</p><blockquote><p>注意：机器1和机器2只有server-id不同和auto-increment-offset不同,其他必须相同。</p><p>部分配置项解释如下：</p><p>binlog_format= ROW：指定mysql的binlog日志的格式，日志中会记录成每一行数据被修改的形式，然后在 slave 端再对相同的数据进行修改。</p><p>auto-increment-increment= 2：表示自增长字段每次递增的量，其默认值是1。它的值应设为整个结构中服务器的总数，本案例用到两台服务器，所以值设为2。</p><p>auto-increment-offset= 2：用来设定数据库中自动增长的起点(即初始值)，因为这两能服务器都设定了一次自动增长值2，所以它们的起点必须得不同，这样才能避免两台服务器数据同步时出现主键冲突。</p><p>注：另外还可以在my.cnf配置文件中，添加“binlog_do_db=数据库名”配置项（可以添加多个）来指定要同步的数据库。如果配置了这个配置项，如果没添加在该配置项后面的数据库，则binlog不记录它的事件。</p></blockquote></li></ul></li><li><p>切换到datacanvas用户进行mysql启动服务 （建议）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/mysql/support-files/mysql.server start</span><br></pre></td></tr></table></figure><p>或者在已经创建软连接的前提下，切换到root用户，并启动mysql服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure></li><li><p>客户端登录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/mysql/bin/mysql -uroot -p</span><br></pre></td></tr></table></figure><p>  设置可远程登录root用户</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> PRIVILEGES <span class="keyword">ON</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;123456&#x27;</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><blockquote><p>注意：上面的密码’123456’修改成真实的root密码</p></blockquote></li></ul><h4 id="开始设置双主备份"><a href="#开始设置双主备份" class="headerlink" title="开始设置双主备份"></a>开始设置双主备份</h4><ul><li><p>在master1上操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">先在master2上执行，</span><br><span class="line"><span class="keyword">show</span> master status;（获取master_log_file和master_log_pos信息）</span><br><span class="line"></span><br><span class="line">在master1上执行</span><br><span class="line">change master <span class="keyword">to</span> master_host<span class="operator">=</span><span class="string">&#x27;172.20.3.114&#x27;</span>,master_port<span class="operator">=</span><span class="number">3306</span>,master_user<span class="operator">=</span><span class="string">&#x27;rt&#x27;</span>,master_password<span class="operator">=</span><span class="string">&#x27;rt123&#x27;</span>,master_log_file<span class="operator">=</span><span class="string">&#x27;mysql-bin.000003&#x27;</span>,master_log_pos<span class="operator">=</span><span class="number">194</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">start</span> slave;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> slave status\G</span><br></pre></td></tr></table></figure></li><li><p>在master2上操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先在master1上执行，</span><br><span class="line"><span class="keyword">show</span> master status;（获取master_log_file和master_log_pos信息）</span><br><span class="line">在master2上执行</span><br><span class="line">change master <span class="keyword">to</span> master_host<span class="operator">=</span><span class="string">&#x27;172.20.3.113&#x27;</span>,master_port<span class="operator">=</span><span class="number">3306</span>,master_user<span class="operator">=</span><span class="string">&#x27;rt&#x27;</span>,master_password<span class="operator">=</span><span class="string">&#x27;rt123&#x27;</span>,master_log_file<span class="operator">=</span><span class="string">&#x27;mysql-bin.000004&#x27;</span>,master_log_pos<span class="operator">=</span><span class="number">194</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">start</span> slave;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> slave status\G</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="二、keepalived安装配置"><a href="#二、keepalived安装配置" class="headerlink" title="二、keepalived安装配置"></a>二、keepalived安装配置</h3><p>需要在master1和master2的机器上安装keepalived服务，安装过程大致如下：</p><ul><li><p>通过地址<a href="https://pkgs.org/download/keepalived%E4%B8%8B%E8%BD%BD%E7%9B%B8%E5%BA%94%E7%9A%84%E5%AE%89%E8%A3%85%E7%89%88%E6%9C%AC%EF%BC%8C%E7%84%B6%E5%90%8E%E8%A7%A3%E5%8E%8B%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9B%AE%E5%BD%95%E3%80%82">https://pkgs.org/download/keepalived下载相应的安装版本，然后解压的相关目录。</a></p></li><li><p>源码的安装一般由3个步骤组成：配置（configure）、编译（make）、安装( make install）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/local/keepalived</span><br></pre></td></tr></table></figure><p> 如果提示错误信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">configure: error: </span><br><span class="line">  !!! OpenSSL is not properly installed on your system. !!!</span><br><span class="line">  !!! Can not include OpenSSL headers files.            !!!</span><br></pre></td></tr></table></figure><p>需要安装yum install openssl openssl-devel（RedHat系统），<br>再次执行./configure –prefix=/usr/local/keepalived</p></li><li><p>在安装目录执行<code>make &amp;&amp; make install</code>进行编译安装</p></li><li><p>keepalived配置文件，默认情况下keepalived启动时会去/etc/keepalived目录下加载配置文件keepalived.conf</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">! Configuration File forkeepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">notification_email &#123;</span><br><span class="line">[email protected]</span><br><span class="line"> &#125;</span><br><span class="line">notification_email_from  [email protected]</span><br><span class="line">smtp_server 127.0.0.1</span><br><span class="line">smtp_connect_timeout 30</span><br><span class="line">router_id MYSQL_HA      #标识，双主相同</span><br><span class="line"> &#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"> state BACKUP           #两台都设置BACKUP</span><br><span class="line"> interface eth0         #网卡名称</span><br><span class="line"> virtual_router_id 51       #主备相同</span><br><span class="line"> priority 100   #优先级，另一台改为90    </span><br><span class="line"> advert_int 1    </span><br><span class="line"> nopreempt  #不抢占，只在优先级高的机器上设置即可，优先级低的机器不设置    </span><br><span class="line"> authentication &#123;</span><br><span class="line"> auth_type PASS    #鉴权，默认通过</span><br><span class="line"> auth_pass 1111    # 鉴权访问密码</span><br><span class="line"> &#125;</span><br><span class="line"> virtual_ipaddress &#123;</span><br><span class="line">  172.20.3.200    #虚拟ip</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">virtual_server 172.20.3.200 3306 &#123;    </span><br><span class="line">     delay_loop 2   #每个2秒检查一次real_server状态    </span><br><span class="line">     lb_algo wrr   #LVS算法    </span><br><span class="line">     lb_kind DR    #LVS模式    </span><br><span class="line">     persistence_timeout 60   #会话保持时间    </span><br><span class="line">     protocol TCP    </span><br><span class="line">     real_server 172.20.3.113 3306 &#123;    </span><br><span class="line">         weight 1    #指定了当前主机的权重    </span><br><span class="line">         notify_down /usr/local/keepalived/kill_keepalived.sh  #检测到服务down后执行的脚本    </span><br><span class="line">         TCP_CHECK &#123;    </span><br><span class="line">             connect_timeout 10    #连接超时时间</span><br><span class="line">             delay_before_retry 3   #重连间隔时间    </span><br><span class="line">             connect_port 3306   #健康检查端口  </span><br><span class="line">         &#125;  </span><br><span class="line">     &#125;</span><br><span class="line">     real_server 172.20.3.114 3306 &#123;</span><br><span class="line">        weight 2</span><br><span class="line">        notify_down /usr/local/keepalived/kill_keepalived.sh  #检测到服务down后执行的脚本</span><br><span class="line">        TCP_CHECK &#123;</span><br><span class="line">            connect_timeout 10</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">            connect_port 3306</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：参数priority两个服务器配置不同，其中virtual_ipaddress是虚拟ip，之后项目可通过访问 172.20.3.200:3306进行访问双主mysql机群。</p><p>上述配置中会涉及/usr/local/keepalived/kill_keepalived.sh，分别在两台服务器上编写kill_keepalived.sh脚本内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">pkill keepalived</span><br></pre></td></tr></table></figure><p>   然后给脚本加权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x /usr/local/keepalived/kill_keepalived.sh</span><br></pre></td></tr></table></figure><ul><li>启动keepalived服务<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service keepalived start</span><br></pre></td></tr></table></figure>如果启动失败，尝试输入<code>pkill -9 keepalived</code>，然后再尝试重启</li></ul><hr><h3 id="三、访问双主mysql集群"><a href="#三、访问双主mysql集群" class="headerlink" title="三、访问双主mysql集群"></a>三、访问双主mysql集群</h3><p>两台机器的mysql和keepalived配置完成之后，即可在项目中，通过访问虚拟ip地址（172.20.3.200:3306）进行mysql集群的访问。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql数据备份&quot;&gt;&lt;a href=&quot;#mysql数据备份&quot; class=&quot;headerlink&quot; title=&quot;mysql数据备份&quot;&gt;&lt;/a&gt;mysql数据备份&lt;/h1&gt;&lt;h2 id=&quot;方案二：双主机HA部署&quot;&gt;&lt;a href=&quot;#方案二：双主机HA部署&quot; c</summary>
      
    
    
    
    <category term="mysql" scheme="http://yoursite.com/categories/mysql/"/>
    
    
    <category term="HA" scheme="http://yoursite.com/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>mysql backup plan</title>
    <link href="http://yoursite.com/2022/01/19/mysql%20backup%20A/"/>
    <id>http://yoursite.com/2022/01/19/mysql%20backup%20A/</id>
    <published>2022-01-19T06:54:47.000Z</published>
    <updated>2022-04-20T02:54:42.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql数据备份"><a href="#mysql数据备份" class="headerlink" title="mysql数据备份"></a>mysql数据备份</h1><h2 id="方案一：定期备份数据库数据文件"><a href="#方案一：定期备份数据库数据文件" class="headerlink" title="方案一：定期备份数据库数据文件"></a>方案一：定期备份数据库数据文件</h2><h3 id="一、编写shell脚本"><a href="#一、编写shell脚本" class="headerlink" title="一、编写shell脚本"></a>一、编写shell脚本</h3><p>脚本文件<strong>backup_mysql.sh</strong>信息如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">用户名</span></span><br><span class="line">username=root</span><br><span class="line"><span class="meta">#</span><span class="bash">密码</span></span><br><span class="line">password=填写密码</span><br><span class="line"><span class="meta">#</span><span class="bash">将要备份的数据库</span></span><br><span class="line">database_name=填写需要备份的数据库</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">保存备份文件最多个数</span></span><br><span class="line">count=30</span><br><span class="line"><span class="meta">#</span><span class="bash">备份保存路径</span></span><br><span class="line">backup_path=/data/mysql_backup</span><br><span class="line"><span class="meta">#</span><span class="bash">日期</span></span><br><span class="line">date_time=`date +%Y-%m-%d-%H-%M`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果文件夹不存在则创建</span></span><br><span class="line">if [ ! -d $backup_path ]; </span><br><span class="line">then     </span><br><span class="line">    mkdir -p $backup_path; </span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">开始备份</span></span><br><span class="line">mysqldump -u $username -p$password $database_name &gt; $backup_path/$database_name-$date_time.sql</span><br><span class="line"><span class="meta">#</span><span class="bash">开始压缩</span></span><br><span class="line">cd $backup_path</span><br><span class="line">tar -zcvf $database_name-$date_time.tar.gz $database_name-$date_time.sql</span><br><span class="line"><span class="meta">#</span><span class="bash">删除源文件</span></span><br><span class="line">rm -rf $backup_path/$database_name-$date_time.sql</span><br><span class="line"><span class="meta">#</span><span class="bash">更新备份日志</span></span><br><span class="line">echo &quot;create $backup_path/$database_name-$date_time.tar.gz&quot; &gt;&gt; $backup_path/dump.log</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">找出需要删除的备份</span></span><br><span class="line">delfile=`ls -l -crt $backup_path/*.tar.gz | awk &#x27;&#123;print $9 &#125;&#x27; | head -1`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">判断现在的备份数量是否大于阈值</span></span><br><span class="line">number=`ls -l -crt  $backup_path/*.tar.gz | awk &#x27;&#123;print $9 &#125;&#x27; | wc -l`</span><br><span class="line"></span><br><span class="line">if [ $number -gt $count ]</span><br><span class="line">then</span><br><span class="line"><span class="meta">  #</span><span class="bash">删除最早生成的备份，只保留count数量的备份</span></span><br><span class="line">  rm $delfile</span><br><span class="line"><span class="meta">  #</span><span class="bash">更新删除文件日志</span></span><br><span class="line">  echo &quot;delete $delfile&quot; &gt;&gt; $backup_path/dump.log</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>该脚本实现的功能：备份指定数据库的数据信息到指定目录，并只保存指定数量的最新文件。</p><p>注意：脚本中需要补全脚本中的<strong>password</strong>和<strong>database_name</strong>信息，可修改备份保存路径<strong>backup_path</strong>，以及最多保存的备份文件数量<strong>count</strong>。</p><p>编写完脚本信息之后，需要给脚本赋予可执行权限 <code>chmod +x backup_mysql.sh</code></p><h3 id="二、设定定时任务crontab"><a href="#二、设定定时任务crontab" class="headerlink" title="二、设定定时任务crontab"></a>二、设定定时任务crontab</h3><p>运行crontab -e命令，打开一个可编辑的文本，输入<code>0 1 * * * /path/to/backup_mysql.sh</code>  保本并退出即添加完成。</p><p>注意：其中<code>0 1 * * *</code>，表示每天凌晨1点进行备份操作，可自行修改1的值（范围0～23）</p><p>其中路径信息<code>/path/to/backup_mysql.sh</code>需要修改为实际的脚本路径。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql数据备份&quot;&gt;&lt;a href=&quot;#mysql数据备份&quot; class=&quot;headerlink&quot; title=&quot;mysql数据备份&quot;&gt;&lt;/a&gt;mysql数据备份&lt;/h1&gt;&lt;h2 id=&quot;方案一：定期备份数据库数据文件&quot;&gt;&lt;a href=&quot;#方案一：定期备份数据</summary>
      
    
    
    
    <category term="mysql" scheme="http://yoursite.com/categories/mysql/"/>
    
    
    <category term="backup" scheme="http://yoursite.com/tags/backup/"/>
    
  </entry>
  
  <entry>
    <title>flink cdc</title>
    <link href="http://yoursite.com/2022/01/15/flink%20cdc/"/>
    <id>http://yoursite.com/2022/01/15/flink%20cdc/</id>
    <published>2022-01-15T08:24:07.000Z</published>
    <updated>2022-04-20T02:59:22.144Z</updated>
    
    <content type="html"><![CDATA[<p>reference</p><p><a href="https://developer.aliyun.com/article/848448?spm=a2c6h.12873639.0.d102020001.6a5a2de1EwwX6V&utm_content=g_1000316418">Flink Forward Aisa 系列专刊｜Flink CDC 新一代数据集成框架 - 技术原理、入门与生产实践-阿里云开发者社区</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;reference&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/article/848448?spm=a2c6h.12873639.0.d102020001.6a5a2de1EwwX6V&amp;utm_content=g_100031</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="cdc" scheme="http://yoursite.com/tags/cdc/"/>
    
  </entry>
  
</feed>
