<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>source is the essence</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2022-01-20T01:36:49.474Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>brook</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2022/01/20/encrypt-decrpt/"/>
    <id>http://yoursite.com/2022/01/20/encrypt-decrpt/</id>
    <published>2022-01-20T01:35:55.658Z</published>
    <updated>2022-01-20T01:36:49.474Z</updated>
    
    <content type="html"><![CDATA[<hr><hr><hr><p>reference</p><p><a href="https://opensource.com/article/21/7/linux-age">https://opensource.com/article/21/7/linux-age</a></p><p><a href="https://github.com/FiloSottile/age">https://github.com/FiloSottile/age</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt;reference&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://opensource.com/article/21/7/linux-age&quot;&gt;https://opensource.com/article/21/7/linux-age&lt;/a&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>mysql HA &amp; keepalived</title>
    <link href="http://yoursite.com/2022/01/19/mysql%20HA/"/>
    <id>http://yoursite.com/2022/01/19/mysql%20HA/</id>
    <published>2022-01-19T07:54:47.000Z</published>
    <updated>2022-01-20T07:54:57.761Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql数据备份"><a href="#mysql数据备份" class="headerlink" title="mysql数据备份"></a>mysql数据备份</h1><h2 id="方案二：双主机HA部署"><a href="#方案二：双主机HA部署" class="headerlink" title="方案二：双主机HA部署"></a>方案二：双主机HA部署</h2><p><strong>前提</strong>：准备两个机器master1（172.20.3.113）和master2（172.20.3.114），且分别安装了mysql，其中IP地址根据生产具体ip进行替换</p><h3 id="一、配置my-cnf信息"><a href="#一、配置my-cnf信息" class="headerlink" title="一、配置my.cnf信息"></a>一、配置my.cnf信息</h3><ul><li><p>配置/etc/my.cnf文件（从mysql5.7开始不会自动生成my.cnf文件，所以需要手动创建）my.cnf文件内容大致如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8         #设置mysql客户端默认字符集</span><br><span class="line">[mysqld]</span><br><span class="line">port = 3306  #可自行更改端口</span><br><span class="line">basedir=/usr/local/mysql</span><br><span class="line">datadir=/usr/local/mysql/data</span><br><span class="line">max_connections = 500              #最大连接数</span><br><span class="line">log_bin=mysql-bin</span><br><span class="line">server_id = 1                            #机器1设置为1，机器2设置为2</span><br><span class="line">binlog_format=ROW</span><br><span class="line">auto-increment-increment = 2            #字段变化增量值</span><br><span class="line">auto-increment-offset = 1               #机器1设置为1，机器2设置为2</span><br><span class="line">slave-skip-errors = all                 #忽略所有复制产生的错误</span><br><span class="line">gtid_mode=ON</span><br><span class="line">enforce-gtid-consistency=ON</span><br><span class="line"></span><br><span class="line">character-set-server = utf8</span><br><span class="line">default-storage-engine = INNODB</span><br><span class="line">lower_case_table_names = 1</span><br></pre></td></tr></table></figure><ul><li><p>[mysql]代表我们使用mysql命令登录mysql数据库时的默认设置 </p></li><li><p>[mysqld]代表数据库自身的默认设置</p><blockquote><p>注意：机器1和机器2只有server-id不同和auto-increment-offset不同,其他必须相同。</p><p>部分配置项解释如下：</p><p>binlog_format= ROW：指定mysql的binlog日志的格式，日志中会记录成每一行数据被修改的形式，然后在 slave 端再对相同的数据进行修改。</p><p>auto-increment-increment= 2：表示自增长字段每次递增的量，其默认值是1。它的值应设为整个结构中服务器的总数，本案例用到两台服务器，所以值设为2。</p><p>auto-increment-offset= 2：用来设定数据库中自动增长的起点(即初始值)，因为这两能服务器都设定了一次自动增长值2，所以它们的起点必须得不同，这样才能避免两台服务器数据同步时出现主键冲突。</p><p>注：另外还可以在my.cnf配置文件中，添加“binlog_do_db=数据库名”配置项（可以添加多个）来指定要同步的数据库。如果配置了这个配置项，如果没添加在该配置项后面的数据库，则binlog不记录它的事件。</p></blockquote></li></ul></li><li><p>切换到datacanvas用户进行mysql启动服务 （建议）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/mysql/support-files/mysql.server start</span><br></pre></td></tr></table></figure><p>或者在已经创建软连接的前提下，切换到root用户，并启动mysql服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure></li><li><p>客户端登录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/mysql/bin/mysql -uroot -p</span><br></pre></td></tr></table></figure><p>  设置可远程登录root用户</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> PRIVILEGES <span class="keyword">ON</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;123456&#x27;</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><blockquote><p>注意：上面的密码’123456’修改成真实的root密码</p></blockquote></li></ul><h4 id="开始设置双主备份"><a href="#开始设置双主备份" class="headerlink" title="开始设置双主备份"></a>开始设置双主备份</h4><ul><li><p>在master1上操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">先在master2上执行，</span><br><span class="line"><span class="keyword">show</span> master status;（获取master_log_file和master_log_pos信息）</span><br><span class="line"></span><br><span class="line">在master1上执行</span><br><span class="line">change master <span class="keyword">to</span> master_host<span class="operator">=</span><span class="string">&#x27;172.20.3.114&#x27;</span>,master_port<span class="operator">=</span><span class="number">3306</span>,master_user<span class="operator">=</span><span class="string">&#x27;rt&#x27;</span>,master_password<span class="operator">=</span><span class="string">&#x27;rt123&#x27;</span>,master_log_file<span class="operator">=</span><span class="string">&#x27;mysql-bin.000003&#x27;</span>,master_log_pos<span class="operator">=</span><span class="number">194</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">start</span> slave;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> slave status\G</span><br></pre></td></tr></table></figure></li><li><p>在master2上操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先在master1上执行，</span><br><span class="line"><span class="keyword">show</span> master status;（获取master_log_file和master_log_pos信息）</span><br><span class="line">在master2上执行</span><br><span class="line">change master <span class="keyword">to</span> master_host<span class="operator">=</span><span class="string">&#x27;172.20.3.113&#x27;</span>,master_port<span class="operator">=</span><span class="number">3306</span>,master_user<span class="operator">=</span><span class="string">&#x27;rt&#x27;</span>,master_password<span class="operator">=</span><span class="string">&#x27;rt123&#x27;</span>,master_log_file<span class="operator">=</span><span class="string">&#x27;mysql-bin.000004&#x27;</span>,master_log_pos<span class="operator">=</span><span class="number">194</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">start</span> slave;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> slave status\G</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="二、keepalived安装配置"><a href="#二、keepalived安装配置" class="headerlink" title="二、keepalived安装配置"></a>二、keepalived安装配置</h3><p>需要在master1和master2的机器上安装keepalived服务，安装过程大致如下：</p><ul><li><p>通过地址<a href="https://pkgs.org/download/keepalived%E4%B8%8B%E8%BD%BD%E7%9B%B8%E5%BA%94%E7%9A%84%E5%AE%89%E8%A3%85%E7%89%88%E6%9C%AC%EF%BC%8C%E7%84%B6%E5%90%8E%E8%A7%A3%E5%8E%8B%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9B%AE%E5%BD%95%E3%80%82">https://pkgs.org/download/keepalived下载相应的安装版本，然后解压的相关目录。</a></p></li><li><p>源码的安装一般由3个步骤组成：配置（configure）、编译（make）、安装( make install）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/local/keepalived</span><br></pre></td></tr></table></figure><p> 如果提示错误信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">configure: error: </span><br><span class="line">  !!! OpenSSL is not properly installed on your system. !!!</span><br><span class="line">  !!! Can not include OpenSSL headers files.            !!!</span><br></pre></td></tr></table></figure><p>需要安装yum install openssl openssl-devel（RedHat系统），<br>再次执行./configure –prefix=/usr/local/keepalived</p></li><li><p>在安装目录执行<code>make &amp;&amp; make install</code>进行编译安装</p></li><li><p>keepalived配置文件，默认情况下keepalived启动时会去/etc/keepalived目录下加载配置文件keepalived.conf</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">! Configuration File forkeepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">notification_email &#123;</span><br><span class="line">[email protected]</span><br><span class="line"> &#125;</span><br><span class="line">notification_email_from  [email protected]</span><br><span class="line">smtp_server 127.0.0.1</span><br><span class="line">smtp_connect_timeout 30</span><br><span class="line">router_id MYSQL_HA      #标识，双主相同</span><br><span class="line"> &#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"> state BACKUP           #两台都设置BACKUP</span><br><span class="line"> interface eth0         #网卡名称</span><br><span class="line"> virtual_router_id 51       #主备相同</span><br><span class="line"> priority 100   #优先级，另一台改为90    </span><br><span class="line"> advert_int 1    </span><br><span class="line"> nopreempt  #不抢占，只在优先级高的机器上设置即可，优先级低的机器不设置    </span><br><span class="line"> authentication &#123;</span><br><span class="line"> auth_type PASS    #鉴权，默认通过</span><br><span class="line"> auth_pass 1111    # 鉴权访问密码</span><br><span class="line"> &#125;</span><br><span class="line"> virtual_ipaddress &#123;</span><br><span class="line">  172.20.3.200    #虚拟ip</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">virtual_server 172.20.3.200 3306 &#123;    </span><br><span class="line">     delay_loop 2   #每个2秒检查一次real_server状态    </span><br><span class="line">     lb_algo wrr   #LVS算法    </span><br><span class="line">     lb_kind DR    #LVS模式    </span><br><span class="line">     persistence_timeout 60   #会话保持时间    </span><br><span class="line">     protocol TCP    </span><br><span class="line">     real_server 172.20.3.113 3306 &#123;    </span><br><span class="line">         weight 1    #指定了当前主机的权重    </span><br><span class="line">         notify_down /usr/local/keepalived/kill_keepalived.sh  #检测到服务down后执行的脚本    </span><br><span class="line">         TCP_CHECK &#123;    </span><br><span class="line">             connect_timeout 10    #连接超时时间</span><br><span class="line">             delay_before_retry 3   #重连间隔时间    </span><br><span class="line">             connect_port 3306   #健康检查端口  </span><br><span class="line">         &#125;  </span><br><span class="line">     &#125;</span><br><span class="line">     real_server 172.20.3.114 3306 &#123;</span><br><span class="line">        weight 2</span><br><span class="line">        notify_down /usr/local/keepalived/kill_keepalived.sh  #检测到服务down后执行的脚本</span><br><span class="line">        TCP_CHECK &#123;</span><br><span class="line">            connect_timeout 10</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">            connect_port 3306</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：参数priority两个服务器配置不同，其中virtual_ipaddress是虚拟ip，之后项目可通过访问 172.20.3.200:3306进行访问双主mysql机群。</p><p>上述配置中会涉及/usr/local/keepalived/kill_keepalived.sh，分别在两台服务器上编写kill_keepalived.sh脚本内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">pkill keepalived</span><br></pre></td></tr></table></figure><p>   然后给脚本加权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x /usr/local/keepalived/kill_keepalived.sh</span><br></pre></td></tr></table></figure><ul><li>启动keepalived服务<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service keepalived start</span><br></pre></td></tr></table></figure>如果启动失败，尝试输入<code>pkill -9 keepalived</code>，然后再尝试重启</li></ul><hr><h3 id="三、访问双主mysql集群"><a href="#三、访问双主mysql集群" class="headerlink" title="三、访问双主mysql集群"></a>三、访问双主mysql集群</h3><p>两台机器的mysql和keepalived配置完成之后，即可在项目中，通过访问虚拟ip地址（172.20.3.200:3306）进行mysql集群的访问。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql数据备份&quot;&gt;&lt;a href=&quot;#mysql数据备份&quot; class=&quot;headerlink&quot; title=&quot;mysql数据备份&quot;&gt;&lt;/a&gt;mysql数据备份&lt;/h1&gt;&lt;h2 id=&quot;方案二：双主机HA部署&quot;&gt;&lt;a href=&quot;#方案二：双主机HA部署&quot; c</summary>
      
    
    
    
    <category term="mysql" scheme="http://yoursite.com/categories/mysql/"/>
    
    
    <category term="HA" scheme="http://yoursite.com/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>mysql backup</title>
    <link href="http://yoursite.com/2022/01/19/mysql%20backup/"/>
    <id>http://yoursite.com/2022/01/19/mysql%20backup/</id>
    <published>2022-01-19T06:54:47.000Z</published>
    <updated>2022-01-20T07:55:19.873Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql数据备份"><a href="#mysql数据备份" class="headerlink" title="mysql数据备份"></a>mysql数据备份</h1><h2 id="方案一：定期备份数据库数据文件"><a href="#方案一：定期备份数据库数据文件" class="headerlink" title="方案一：定期备份数据库数据文件"></a>方案一：定期备份数据库数据文件</h2><h3 id="一、编写shell脚本"><a href="#一、编写shell脚本" class="headerlink" title="一、编写shell脚本"></a>一、编写shell脚本</h3><p>脚本文件<strong>backup_mysql.sh</strong>信息如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">用户名</span></span><br><span class="line">username=root</span><br><span class="line"><span class="meta">#</span><span class="bash">密码</span></span><br><span class="line">password=填写密码</span><br><span class="line"><span class="meta">#</span><span class="bash">将要备份的数据库</span></span><br><span class="line">database_name=填写需要备份的数据库</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">保存备份文件最多个数</span></span><br><span class="line">count=30</span><br><span class="line"><span class="meta">#</span><span class="bash">备份保存路径</span></span><br><span class="line">backup_path=/data/mysql_backup</span><br><span class="line"><span class="meta">#</span><span class="bash">日期</span></span><br><span class="line">date_time=`date +%Y-%m-%d-%H-%M`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果文件夹不存在则创建</span></span><br><span class="line">if [ ! -d $backup_path ]; </span><br><span class="line">then     </span><br><span class="line">    mkdir -p $backup_path; </span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">开始备份</span></span><br><span class="line">mysqldump -u $username -p$password $database_name &gt; $backup_path/$database_name-$date_time.sql</span><br><span class="line"><span class="meta">#</span><span class="bash">开始压缩</span></span><br><span class="line">cd $backup_path</span><br><span class="line">tar -zcvf $database_name-$date_time.tar.gz $database_name-$date_time.sql</span><br><span class="line"><span class="meta">#</span><span class="bash">删除源文件</span></span><br><span class="line">rm -rf $backup_path/$database_name-$date_time.sql</span><br><span class="line"><span class="meta">#</span><span class="bash">更新备份日志</span></span><br><span class="line">echo &quot;create $backup_path/$database_name-$date_time.tar.gz&quot; &gt;&gt; $backup_path/dump.log</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">找出需要删除的备份</span></span><br><span class="line">delfile=`ls -l -crt $backup_path/*.tar.gz | awk &#x27;&#123;print $9 &#125;&#x27; | head -1`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">判断现在的备份数量是否大于阈值</span></span><br><span class="line">number=`ls -l -crt  $backup_path/*.tar.gz | awk &#x27;&#123;print $9 &#125;&#x27; | wc -l`</span><br><span class="line"></span><br><span class="line">if [ $number -gt $count ]</span><br><span class="line">then</span><br><span class="line"><span class="meta">  #</span><span class="bash">删除最早生成的备份，只保留count数量的备份</span></span><br><span class="line">  rm $delfile</span><br><span class="line"><span class="meta">  #</span><span class="bash">更新删除文件日志</span></span><br><span class="line">  echo &quot;delete $delfile&quot; &gt;&gt; $backup_path/dump.log</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>该脚本实现的功能：备份指定数据库的数据信息到指定目录，并只保存指定数量的最新文件。</p><p>注意：脚本中需要补全脚本中的<strong>password</strong>和<strong>database_name</strong>信息，可修改备份保存路径<strong>backup_path</strong>，以及最多保存的备份文件数量<strong>count</strong>。</p><p>编写完脚本信息之后，需要给脚本赋予可执行权限 <code>chmod +x backup_mysql.sh</code></p><h3 id="二、设定定时任务crontab"><a href="#二、设定定时任务crontab" class="headerlink" title="二、设定定时任务crontab"></a>二、设定定时任务crontab</h3><p>运行crontab -e命令，打开一个可编辑的文本，输入<code>0 1 * * * /path/to/backup_mysql.sh</code>  保本并退出即添加完成。</p><p>注意：其中<code>0 1 * * *</code>，表示每天凌晨1点进行备份操作，可自行修改1的值（范围0～23）</p><p>其中路径信息<code>/path/to/backup_mysql.sh</code>需要修改为实际的脚本路径。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mysql数据备份&quot;&gt;&lt;a href=&quot;#mysql数据备份&quot; class=&quot;headerlink&quot; title=&quot;mysql数据备份&quot;&gt;&lt;/a&gt;mysql数据备份&lt;/h1&gt;&lt;h2 id=&quot;方案一：定期备份数据库数据文件&quot;&gt;&lt;a href=&quot;#方案一：定期备份数据</summary>
      
    
    
    
    <category term="mysql" scheme="http://yoursite.com/categories/mysql/"/>
    
    
    <category term="backup" scheme="http://yoursite.com/tags/backup/"/>
    
  </entry>
  
  <entry>
    <title>flink cep</title>
    <link href="http://yoursite.com/2022/01/14/flink%20cep/"/>
    <id>http://yoursite.com/2022/01/14/flink%20cep/</id>
    <published>2022-01-14T08:24:07.000Z</published>
    <updated>2022-01-14T08:24:15.313Z</updated>
    
    <content type="html"><![CDATA[<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>风险控制<br>对用户异常行为模式进行实时检测，当一个用户发生了不该发生的行为，判定这个用户是不是有违规操作的嫌疑。</p><p>策略营销<br>用预先定义好的规则对用户的行为轨迹进行实时跟踪，对行为轨迹匹配预定义规则的用户实时发送相应策略的推广。</p><p>运维监控<br>灵活配置多指标、多依赖来实现更复杂的监控模式。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;应用场景&quot;&gt;&lt;a href=&quot;#应用场景&quot; class=&quot;headerlink&quot; title=&quot;应用场景&quot;&gt;&lt;/a&gt;应用场景&lt;/h3&gt;&lt;p&gt;风险控制&lt;br&gt;对用户异常行为模式进行实时检测，当一个用户发生了不该发生的行为，判定这个用户是不是有违规操作的嫌疑。&lt;/p&gt;</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="cep" scheme="http://yoursite.com/tags/cep/"/>
    
  </entry>
  
  <entry>
    <title>flink watermark</title>
    <link href="http://yoursite.com/2022/01/11/flink%20watermark/"/>
    <id>http://yoursite.com/2022/01/11/flink%20watermark/</id>
    <published>2022-01-11T10:42:32.000Z</published>
    <updated>2022-01-11T10:44:03.586Z</updated>
    
    <content type="html"><![CDATA[<p>Watermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳。</p><p>由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。</p><p>Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。</p><p>Watermark的产生和Apache Flink内部处理逻辑如下图所示:</p><p><img src="/images/flink/flink_watermark.png"></p><h3 id="产生方式"><a href="#产生方式" class="headerlink" title="产生方式"></a>产生方式</h3><ul><li><p>Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。 在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。</p></li><li><p>Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Watermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳。&lt;/p&gt;
&lt;p&gt;由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>flink state</title>
    <link href="http://yoursite.com/2022/01/11/flink%20state/"/>
    <id>http://yoursite.com/2022/01/11/flink%20state/</id>
    <published>2022-01-11T06:32:39.000Z</published>
    <updated>2022-01-11T06:56:56.985Z</updated>
    
    <content type="html"><![CDATA[<h3 id="what"><a href="#what" class="headerlink" title="what ?"></a>what ?</h3><p>State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。</p><h3 id="why"><a href="#why" class="headerlink" title="why ?"></a>why ?</h3><p>与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。</p><h3 id="how"><a href="#how" class="headerlink" title="how ?"></a>how ?</h3><h4 id="存储实现"><a href="#存储实现" class="headerlink" title="存储实现"></a>存储实现</h4><ul><li>基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用；</li><li>基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；</li><li>基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化；<blockquote><p>Apache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。</p></blockquote></li><li>还有一个是基于Niagara(Alibaba内部实现)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用；</li></ul><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>通过算子和数据层面划分</p><ul><li><p>算子类state</p><p>KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的</p></li><li><p>数据类state</p><p>OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;what&quot;&gt;&lt;a href=&quot;#what&quot; class=&quot;headerlink&quot; title=&quot;what ?&quot;&gt;&lt;/a&gt;what ?&lt;/h3&gt;&lt;p&gt;State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>flink streaming warehouse</title>
    <link href="http://yoursite.com/2022/01/11/flink%20streaming%20warehouse/"/>
    <id>http://yoursite.com/2022/01/11/flink%20streaming%20warehouse/</id>
    <published>2022-01-11T02:57:26.000Z</published>
    <updated>2022-01-11T02:59:01.387Z</updated>
    
    <content type="html"><![CDATA[<p>流式数仓（Streaming Warehouse）更准确地说，其实是“make data warehouse streaming”，就是让整个数仓的数据全实时地流动起来，且是以纯流的方式而不是微批（mini-batch）的方式流动。</p><p>目标是实现一个具备端到端实时性的纯流服务（Streaming Service），用一套 API 分析所有流动中的数据，当源头数据发生变化，比如捕捉到在线服务的 Log 或数据库的 Binlog 以后，就按照提前定义好的 Query 逻辑或数据处理逻辑，对数据进行分析，分析后的数据落到数仓的某一个分层，再从第一个分层向下一个分层流动，然后数仓所有分层会全部流动起来，最终流到一个在线系统里，用户可以看到整个数仓的全实时流动效果。</p><p>在这个过程中，数据是主动的，而查询是被动的，分析由数据的变化来驱动。同时在垂直方向上，对每一个数据明细层，用户都可以执行 Query 进行主动查询，并且能实时获得查询结果。此外，它还能兼容离线分析场景，API 依然是同一套，实现真正的一体化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;流式数仓（Streaming Warehouse）更准确地说，其实是“make data warehouse streaming”，就是让整个数仓的数据全实时地流动起来，且是以纯流的方式而不是微批（mini-batch）的方式流动。&lt;/p&gt;
&lt;p&gt;目标是实现一个具备端到端实</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="warehouse" scheme="http://yoursite.com/tags/warehouse/"/>
    
  </entry>
  
  <entry>
    <title>macos command</title>
    <link href="http://yoursite.com/2022/01/07/mac%20command/"/>
    <id>http://yoursite.com/2022/01/07/mac%20command/</id>
    <published>2022-01-07T06:03:15.000Z</published>
    <updated>2022-01-07T06:23:09.542Z</updated>
    
    <content type="html"><![CDATA[<ul><li>解压带有中文名称的zip包</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ditto -V -x -k --sequesterRsrc filename.zip destination</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;解压带有中文名称的zip包&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/</summary>
      
    
    
    
    <category term="macos" scheme="http://yoursite.com/categories/macos/"/>
    
    
    <category term="command" scheme="http://yoursite.com/tags/command/"/>
    
  </entry>
  
  <entry>
    <title>file system</title>
    <link href="http://yoursite.com/2022/01/07/file%20system/"/>
    <id>http://yoursite.com/2022/01/07/file%20system/</id>
    <published>2022-01-07T05:59:21.000Z</published>
    <updated>2022-01-07T06:23:11.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h1><p><img src="/images/fileSystemType.jpeg" alt="fileSystemType.jpeg"></p><ul><li><p>Mac 默认可以读 Windows 的 NTFS 格式，但不能写。</p></li><li><p>Windows 无法识别 Mac 的 HFS+ 或 APFS 格式。</p></li><li><p>Mac 和 Windows 都能正常读写 FAT32 和 ExFAT 格式</p></li><li><p>linux</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Linux：存在几十个文件系统类型：ext2，ext3，ext4，xfs，brtfs，zfs（man 5 fs可以取得全部文件系统的介绍）</span><br><span class="line"></span><br><span class="line">不同文件系统采用不同的方法来管理磁盘空间，各有优劣；文件系统是具体到分区的，所以格式化针对的是分区，分区格式化是指采用指定的文件系统类型对分区空间进行登记、索引并建立相应的管理表格的过程。</span><br><span class="line"></span><br><span class="line">ext2具有极快的速度和极小的CPU占用率，可用于硬盘和移动存储设备</span><br><span class="line">ext3增加日志功能，可回溯追踪</span><br><span class="line">ext4日志式文件系统，支持1EB（1024*1024TB），最大单文件16TB，支持连续写入可减少文件碎片。rhel6默认文件系统</span><br><span class="line">xfs可以管理500T的硬盘。rhel7默认文件系统</span><br><span class="line">brtfs文件系统针对固态盘做优化，</span><br></pre></td></tr></table></figure></li><li><p>windows</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FAT16：MS—DOS和win95采用的磁盘分区格式，采用16位的文件分配表，只支持2GB的磁盘分区，最大单文件2GB，且磁盘利用率低</span><br><span class="line">FAT32：（即Vfat）采用32位的文件分配表，支持最大分区128GB，最大文件4GB</span><br><span class="line">NTFS：支持最大分区2TB，最大文件2TB，安全性和稳定性非常好，不易出现文件碎片。</span><br></pre></td></tr></table></figure></li></ul><hr><p>reference</p><p><a href="https://www.yinxiang.com/everhub/note/0312ed71-61f5-4c75-9c77-3db0ffdeb613">https://www.yinxiang.com/everhub/note/0312ed71-61f5-4c75-9c77-3db0ffdeb613</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;文件系统&quot;&gt;&lt;a href=&quot;#文件系统&quot; class=&quot;headerlink&quot; title=&quot;文件系统&quot;&gt;&lt;/a&gt;文件系统&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/images/fileSystemType.jpeg&quot; alt=&quot;fileSystemType.jpe</summary>
      
    
    
    
    <category term="file system" scheme="http://yoursite.com/categories/file-system/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2022/01/06/flink%20cdc/"/>
    <id>http://yoursite.com/2022/01/06/flink%20cdc/</id>
    <published>2022-01-06T08:52:25.071Z</published>
    <updated>2022-01-12T17:36:45.477Z</updated>
    
    <content type="html"><![CDATA[<hr><hr><p>reference</p><p><a href="https://developer.aliyun.com/article/848448?spm=a2c6h.12873639.0.d102020001.6a5a2de1EwwX6V&utm_content=g_1000316418">Flink Forward Aisa 系列专刊｜Flink CDC 新一代数据集成框架 - 技术原理、入门与生产实践-阿里云开发者社区</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt;reference&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/article/848448?spm=a2c6h.12873639.0.d102020001.6a5a2de1EwwX6V&amp;utm_conten</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>flink join</title>
    <link href="http://yoursite.com/2022/01/05/flink%20join/"/>
    <id>http://yoursite.com/2022/01/05/flink%20join/</id>
    <published>2022-01-05T08:52:01.000Z</published>
    <updated>2022-01-21T09:50:22.485Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>使用 SQL 进行数据分析的过程中，join 是经常要使用的操作。</p><p>在离线场景中，join 的数据集是有边界的，可以缓存数据有边界的数据集进行查询，有Nested Loop/Hash Join/Sort Merge Join 等多表 join；</p><p>而在实时场景中，join 两侧的数据都是无边界的数据流，所以缓存数据集对长时间 job 来说，存储和查询压力很大，另外双流的到达时间可能不一致，造成 join 计算结果准确度不够；因此，Flink SQL 提供了多种 join 方法，来帮助用户应对各种 join 场景。</p></blockquote><h3 id="regular-join"><a href="#regular-join" class="headerlink" title="regular join"></a>regular join</h3><blockquote><p>regular join 是最通用的 join 类型，不支持时间窗口以及时间属性，任何一侧数据流有更改都是可见的，直接影响整个 join 结果。如果有一侧数据流增加一个新纪录，那么它将会把另一侧的所有的过去和将来的数据合并在一起，因为 regular join 没有剔除策略，这就影响最新输出的结果; 正因为历史数据不会被清理，所以 regular join 支持数据流的任何更新操作。</p><p>对于 regular join 来说，更适合用于离线场景和小数据量场景。</p></blockquote><ul><li>语法<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> columns</span><br><span class="line"><span class="keyword">FROM</span> t1  [<span class="keyword">AS</span> <span class="operator">&lt;</span>alias1<span class="operator">&gt;</span>]</span><br><span class="line">[<span class="keyword">LEFT</span><span class="operator">/</span><span class="keyword">INNER</span><span class="operator">/</span><span class="keyword">FULL</span> <span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> t2</span><br><span class="line"><span class="keyword">ON</span> t1.column1 <span class="operator">=</span> t2.key<span class="operator">-</span>name1</span><br></pre></td></tr></table></figure></li></ul><h3 id="interval-join"><a href="#interval-join" class="headerlink" title="interval join"></a>interval join</h3><blockquote><p>相对于 regular join，interval Join 则利用窗口的给两个输入表设定一个 Join 的时间界限，超出时间范围的数据则对 join 不可见并可以被清理掉，这样就能修正 regular join 因为没有剔除数据策略带来 join 结果的误差以及需要大量的资源。</p><p>但是使用interval join，需要定义好时间属性字段，可以是计算发生的 Processing Time，也可以是根据数据本身提取的 Event Time；如果是定义的是 Processing Time，则Flink 框架本身根据系统划分的时间窗口定时清理数据；如果定义的是 Event Time，Flink 框架分配 Event Time 窗口并根据设置的 watermark 来清理数据。</p></blockquote><ul><li><p>语法1</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> columns</span><br><span class="line"><span class="keyword">FROM</span> t1  [<span class="keyword">AS</span> <span class="operator">&lt;</span>alias1<span class="operator">&gt;</span>]</span><br><span class="line">[<span class="keyword">LEFT</span><span class="operator">/</span><span class="keyword">INNER</span><span class="operator">/</span><span class="keyword">FULL</span> <span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> t2</span><br><span class="line"><span class="keyword">ON</span> t1.column1 <span class="operator">=</span> t2.key<span class="operator">-</span>name1 <span class="keyword">AND</span> t1.timestamp <span class="keyword">BETWEEN</span> t2.timestamp  <span class="keyword">AND</span>  <span class="keyword">BETWEEN</span> t2.timestamp <span class="operator">+</span> <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>;</span><br></pre></td></tr></table></figure></li><li><p>语法2</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> columns</span><br><span class="line"><span class="keyword">FROM</span> t1  [<span class="keyword">AS</span> <span class="operator">&lt;</span>alias1<span class="operator">&gt;</span>]</span><br><span class="line">[<span class="keyword">LEFT</span><span class="operator">/</span><span class="keyword">INNER</span><span class="operator">/</span><span class="keyword">FULL</span> <span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> t2</span><br><span class="line"><span class="keyword">ON</span> t1.column1 <span class="operator">=</span> t2.key<span class="operator">-</span>name1 <span class="keyword">AND</span> t2.timestamp <span class="operator">&lt;=</span> t1.timestamp <span class="keyword">and</span> t1.timestamp <span class="operator">&lt;=</span>  t2.timestamp <span class="operator">+</span> <span class="operator">+</span> <span class="type">INTERVAL</span> ’<span class="number">10</span><span class="string">&#x27; MINUTE ;</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="temproal-table-join"><a href="#temproal-table-join" class="headerlink" title="temproal table join"></a>temproal table join</h3><blockquote><p>interval Join 提供了剔除数据的策略，解决资源问题以及计算更加准确，这是有个前提：join 的两个流需要时间属性，需要明确时间的下界，来方便剔除数据；</p><p>显然，这种场景不适合维度表的 join，因为维度表没有时间界限，对于这种场景，Flink 提供了 temproal table join 来覆盖此类场景。</p><p>在 regular join和interval join中，join 两侧的表是平等的，任意的一个表的更新，都会去和另外的历史纪录进行匹配，temproal table 的更新对另一表在该时间节点以前的记录是不可见的。</p></blockquote><ul><li>语法<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> columns</span><br><span class="line"><span class="keyword">FROM</span> t1  [<span class="keyword">AS</span> <span class="operator">&lt;</span>alias1<span class="operator">&gt;</span>]</span><br><span class="line">[<span class="keyword">LEFT</span>] <span class="keyword">JOIN</span> t2 <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> t1.proctime [<span class="keyword">AS</span> <span class="operator">&lt;</span>alias2<span class="operator">&gt;</span>]</span><br><span class="line"><span class="keyword">ON</span> t1.column1 <span class="operator">=</span> t2.key<span class="operator">-</span>name1</span><br></pre></td></tr></table></figure></li></ul><hr><p>reference:</p><p><a href="https://developer.aliyun.com/article/780048?accounttraceid=dd5fdbf3eed04f6185ed6461d8a33012zihq">Flink SQL 实战：双流 join 场景应用-阿里云开发者社区</a></p><p><a href="https://www.liangzl.com/get-article-detail-114889.html">Flink SQL 功能解密系列 —— 维表 JOIN 与异步优化</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;使用 SQL 进行数据分析的过程中，join 是经常要使用的操作。&lt;/p&gt;
&lt;p&gt;在离线场景中，join 的数据集是有边界的，可以缓存数据有边界的数据集进行查询，有Nested Loop/Hash Join/Sort Merge Join 等多表 </summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="learn" scheme="http://yoursite.com/tags/learn/"/>
    
  </entry>
  
  <entry>
    <title>flink sql</title>
    <link href="http://yoursite.com/2022/01/04/flink%20sql/"/>
    <id>http://yoursite.com/2022/01/04/flink%20sql/</id>
    <published>2022-01-04T09:55:24.000Z</published>
    <updated>2022-01-05T03:02:49.900Z</updated>
    
    <content type="html"><![CDATA[<ul><li>NOT ENFORCED</li></ul><blockquote><p>If you know that the data conforms to these constraints, you can use the NOT ENFORCED capability to help achieve two goals:</p><ul><li>Improve performance, primarily in insert, update, and delete operations on the table</li><li>Reduce space requirements that are associated with enforcing a primary key or unique constraint</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;NOT ENFORCED&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If you know that the data conforms to these constraints, you can use the NOT ENFORCED capab</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>flink on yarn</title>
    <link href="http://yoursite.com/2022/01/04/flink%20on%20yarn/"/>
    <id>http://yoursite.com/2022/01/04/flink%20on%20yarn/</id>
    <published>2022-01-03T16:27:40.000Z</published>
    <updated>2022-01-03T17:46:02.790Z</updated>
    
    <content type="html"><![CDATA[<h2 id="interaction"><a href="#interaction" class="headerlink" title="interaction"></a>interaction</h2><p><img src="/images/flinkOnYarn/flink_on_yarn.png"> </p><blockquote></blockquote><h2 id="two-way-to-submit-job-on-yarn"><a href="#two-way-to-submit-job-on-yarn" class="headerlink" title="two way to submit job on yarn"></a>two way to submit job on yarn</h2><p><img src="/images/flinkOnYarn/submit_job.png"></p><h3 id="first-way：yarn-session"><a href="#first-way：yarn-session" class="headerlink" title="first way：yarn session"></a>first way：yarn session</h3><blockquote><p>(Start a long-running Flink cluster on YARN)这种方式需要先启动集群，然后在提交作业，接着会向yarn申请一块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成后，释放了资源，那下一个作业才会正常提交.</p><p>ps:适用于本地测试或者开发</p></blockquote><h4 id="mode-one-客户端模式"><a href="#mode-one-客户端模式" class="headerlink" title="mode one: 客户端模式"></a>mode one: 客户端模式</h4><blockquote><p>可以启动多个yarn session，一个yarn session模式对应一个JobManager,并按照需求提交作业，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止.</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh -n 2 -jm 1024 -tm 4096 -s 6</span><br></pre></td></tr></table></figure><ul><li><p>YarnSessionClusterEntrypoint进程</p><p>代表本节点可以命令方式提交job，而且可以不用指定-m参数。</p><ul><li><p>本节点提交任务</p><p><code>bin/flink run ~/flink-demo-wordcount.jar</code></p></li><li><p>如果需要在其他主机节点提交任务</p><p><code>bin/flink run -m vmhome10.com:43258 examples/batch/WordCount.jar</code></p></li></ul></li><li><p>FlinkYarnSessionCli进程</p><p>代表yarn-session集群入口，实际就是jobmanager节点，也是yarn的ApplicationMaster节点。</p></li></ul><h4 id="mode-two-分离式模式"><a href="#mode-two-分离式模式" class="headerlink" title="mode two: 分离式模式"></a>mode two: 分离式模式</h4><blockquote><p>JobManager的个数只能是一个，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止。通过-d指定分离模式.</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/yarn-session.sh -nm test3 -d</span><br></pre></td></tr></table></figure><blockquote><p>在所有的节点只会出现一个 YarnSessionClusterEntrypoint进程</p></blockquote><h3 id="second-way-flink-run"><a href="#second-way-flink-run" class="headerlink" title="second way: flink run"></a>second way: flink run</h3><blockquote><p>直接在YARN上提交运行Flink作业(Run a Flink job on YARN)，这种方式的好处是一个任务会对应一个job,即没提交一个作业会根据自身的情况，向yarn申请资源，直到作业执行完成，并不会影响下一个作业的正常运行，除非是yarn上面没有任何资源的情况下。</p><p>ps:适用于生产环境，可启动多个yarn session （bin/yarn-session.sh -nm ipOrHostName）</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -m addressOfJobmanager -yn 1 -yjm 1024 -ytm 1024 ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure><p>注意使用参数-m yarn-cluster提交到yarn集群。</p><ul><li>运行到指定的yarn session可以指定 -yid,–yarnapplicationId <arg> Attach to running YARN session来附加到到特定的yarn session上运行</li></ul><hr><p>reference</p><p><a href="https://www.jianshu.com/p/1b05202c4fb6">Flink on yarn部署模式 - 简书</a></p><p><a href="https://www.cnblogs.com/asker009/p/11327533.html">flink on yarn模式下两种提交job方式 - 我是属车的 - 博客园</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;interaction&quot;&gt;&lt;a href=&quot;#interaction&quot; class=&quot;headerlink&quot; title=&quot;interaction&quot;&gt;&lt;/a&gt;interaction&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/flinkOnYarn/flin</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>hive snippet</title>
    <link href="http://yoursite.com/2022/01/04/hive%20snippet/"/>
    <id>http://yoursite.com/2022/01/04/hive%20snippet/</id>
    <published>2022-01-03T16:10:30.000Z</published>
    <updated>2022-01-03T16:17:06.874Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Hive-site里面的配置！！！</p></blockquote><h3 id="get-started"><a href="#get-started" class="headerlink" title="get started"></a>get started</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore &amp;</span><br><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h3 id="beeline"><a href="#beeline" class="headerlink" title="beeline"></a>beeline</h3><p>1）metadata ：hive元数据，即hive定义的表名，字段名，类型，分区，用户这些数据。一般存储关系型书库mysql中，在测试阶段也可以用hive内置Derby数据库。</p><p>（2）metastore ：hivestore服务端。主要提供将DDL，DML等语句转换为MapReduce，提交到hdfs中。</p><p>（3）hiveserver2：hive服务端。提供hive服务。客户端可以通过beeline，jdbc（即用java代码链接）等多种方式链接到hive。</p><p>（4）beeline：hive客户端链接到hive的一个工具。可以理解成mysql的客户端。如：navite cat 等。</p><p><img src="https://img-blog.csdnimg.cn/20191122115956341.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDQwMDQw,size_16,color_FFFFFF,t_70"></p><p>2 连接hive：<br>（1）./bin/hive<br>通过 ./bin/hive 启动的hive服务，第一步会先启动metastore服务，然后在启动一个客户端连接到metastore。此时metastore服务端和客户端都在一台机器上，别的机器无法连接到metastore，所以也无法连接到hive。这种方式不常用，一直只用于调试环节。</p><p>（2） ./bin/hive  –service metastore<br><strong>通过hive –service metastore 会启动一个 hive metastore服务默认的端口号为：9083。metastore服务里面配置metadata相关的配置。此时可以有多个hive客户端在hive-site.xml配置hive.metastore.uris=thrift://ipxxx:9083  的方式链接到hive。motestore 虽然能使hive服务端和客户端分别部署到不同的节点，客户端不需要关注metadata的相关配置。但是metastore只能通过只能通过配置hive.metastore.uris的方式连接，无法通过jdbc的方式访问。</strong></p><p>（3）./bin/hiveserver2<br>hiveserver2 会启动一个hive服务端默认端口为：10000，可以通过beeline，jdbc，odbc的方式链接到hive。<strong>hiveserver2启动的时候会先检查有没有配置hive.metastore.uris，如果没有会先启动一个metastore服务，然后在启动hiveserver2。如果有配置hive.metastore.uris。会连接到远程的metastore服务。这种方式是最常用的。</strong>部署在图如下：</p><ul><li>登录bin/beeline，可以启动客户端链接到hiveserver2。执行beeline后在控制输入 !connect jdbc:hive2://localhost:10000/default root 123 就可以链接到 hiveserver2了；default表示链接到default database， root 和123 分别为密码。注意这里的密码不是mysql的密码，是hive中的用户</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">连接库</span><br><span class="line">!connect jdbc:hive2://localhost:10000/default root 123</span><br></pre></td></tr></table></figure><h1 id="hive中几种分割符"><a href="#hive中几种分割符" class="headerlink" title="hive中几种分割符"></a>hive中几种分割符</h1><p><strong>分隔符</strong></p><p>\n    每行一条记录<br>^A    分隔列（八进制 \001）<br>^B    分隔ARRAY或者STRUCT中的元素，或者MAP中多个键值对之间分隔（八进制 \002）<br>^C    分隔MAP中键值对的“键”和“值”（八进制 \003）</p><p><strong>用到了系统默认分隔符。通常下面2中情况我们需要需要用到分隔符</strong></p><p>1，制作table的输入文件，有时候我们需要输入一些特殊的分隔符</p><p>2，把hive表格导出到本地时，系统默认的分隔符是^A，这个是特殊字符，直接cat或者vim是看不到的</p><p><strong>分隔符在HIVE中的用途</strong></p><table><thead><tr><th>分隔符</th><th>描述</th></tr></thead><tbody><tr><td>\n</td><td>对于文本文件来说，每行都是一条记录，因此换行符可以分隔记录</td></tr><tr><td>^A(Ctrl+A)</td><td>用于分隔字段(列)。在CREATE TABLE语句中可以使用八进制编码\001表示</td></tr><tr><td>^B(Ctrl+B)</td><td>用于分隔ARRAY或者STRUCT中的元素，或用于MAP中键-值对之间的分隔。在CREATE TABLE语句中可以使用八进制编码\002表示</td></tr><tr><td>^C(Ctrl+C)</td><td>用于MAP中键和值之间的分隔。在CREATE TABLE语句中可以使用八进制编码\003表示</td></tr></tbody></table><blockquote><p>Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。</p></blockquote><p>我们可以在create表格的时候，选择如下，表格加载input的文件的时候就会按照下面格式匹配</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">row format delimited </span><br><span class="line">fields terminated by &#x27;\001&#x27; </span><br><span class="line">collection items terminated by &#x27;\002&#x27; </span><br><span class="line">map keys terminated by &#x27;\003&#x27;</span><br><span class="line">lines terminated by &#x27;\n&#x27; </span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure><h3 id="如何查看和修改分割符，特殊符号"><a href="#如何查看和修改分割符，特殊符号" class="headerlink" title="如何查看和修改分割符，特殊符号"></a>如何查看和修改分割符，特殊符号</h3><ol><li>查看隐藏字符的方法</li></ol><p>1.1，cat -A filename</p><p><img src="http://image.okcode.net/26FFE1BCC5620E19E94B26122C71BA2E.png" alt="img"></p><p>1.2，vim filename后 命令模式下输入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set list显示特殊符号</span><br><span class="line">set nolist 取消显示特殊符号</span><br></pre></td></tr></table></figure><ol start="2"><li>修改隐藏字符的方法</li></ol><p>首先按照1.2打开显示特殊符号。进入INSERT模式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ctrl + V 可以输入 ^符号</span><br><span class="line">ctrl + a 可以输入A---&#x27;\001&#x27;</span><br><span class="line">ctrl + b 可以输入A---&#x27;\002&#x27;</span><br><span class="line">ctrl + c 可以输入A---&#x27;\003&#x27;</span><br></pre></td></tr></table></figure><p> 注意：虽然键盘上你能找到^和A但直接输入时不行的，必须按照上面的方法输入。</p><p>第一行是特殊符号颜色蓝色，第二行直接输入不是特殊符号。</p><p><img src="http://image.okcode.net/DD9ED976ABB6F4313B8F0F7C2DD5C33E.png" alt="img"></p><p>特殊号直接cat是不可以看见的，但是第二行是可见的，所以不是特殊符号。</p><p><img src="http://image.okcode.net/75D96F800A1815F7A84A8CF543BD7063.png" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED BY &#x27;\u0001&#x27; </span><br><span class="line">COLLECTION ITEMS TERMINATED BY &#x27;\u0002&#x27; </span><br><span class="line">MAP KEYS TERMINATED BY &#x27;\u0003&#x27;</span><br><span class="line">\u0001是ASCII编码值，对应java代码中的&quot;\001&quot;</span><br></pre></td></tr></table></figure><p>意义如下：</p><p>（1）FIELDS，字段之间的分隔符是’\u0001’</p><p>（2）COLLECTION ITEMS，多个集合之间的分隔符是’\u0002’，例如（kv1，kv2，kv3）这种多个键值对之间的分隔符就是’\u0002’</p><p>（3）MAP KEYS，单个map的k和v之间的分隔符是\u0003\，例如kv1里，k \u0003 v</p><h3 id="查看orc文件"><a href="#查看orc文件" class="headerlink" title="查看orc文件"></a>查看orc文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --orcfiledump &lt;hdfs-location-of-orc-file&gt;</span><br></pre></td></tr></table></figure><h3 id="修改字段类型"><a href="#修改字段类型" class="headerlink" title="修改字段类型"></a>修改字段类型</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">-</span>name<span class="operator">&gt;</span> CHANGE <span class="operator">&lt;</span><span class="keyword">old</span><span class="operator">-</span>col<span class="operator">-</span>name<span class="operator">&gt;</span> <span class="operator">&lt;</span><span class="keyword">new</span><span class="operator">-</span>col<span class="operator">-</span>name<span class="operator">&gt;</span> <span class="operator">&lt;</span>data<span class="operator">-</span>type<span class="operator">&gt;</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> employee CHANGE e_id e_id <span class="type">INT</span>;</span><br></pre></td></tr></table></figure><h3 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h3><ul><li><p>Create ORC table</p></li><li><p>Login to the web console</p></li><li><p>Launch Hive by typing <code>hive</code> in the web console. Run the below commands in Hive.</p></li><li><p>Use your database by using the below command. <code>$&#123;env:USER&#125;</code> gets replaced by your username automatically:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use $&#123;env:USER&#125;;</span><br></pre></td></tr></table></figure></li><li><p>To create an ORC file format:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orc_table (</span><br><span class="line">    first_name STRING, </span><br><span class="line">    last_name STRING</span><br><span class="line"> ) </span><br><span class="line"> STORED AS ORC;</span><br></pre></td></tr></table></figure></li><li><p>To insert values in the table:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO orc_table VALUES (&#x27;John&#x27;,&#x27;Gill&#x27;);</span><br></pre></td></tr></table></figure></li><li><p>To retrieve all the values in the table:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM orc_table;</span><br></pre></td></tr></table></figure></li></ul><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>查看hive进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps -ml  | grep Hive</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Hive-site里面的配置！！！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;get-started&quot;&gt;&lt;a href=&quot;#get-started&quot; class=&quot;headerlink&quot; title=&quot;get started&quot;&gt;&lt;/a&gt;g</summary>
      
    
    
    
    <category term="hive" scheme="http://yoursite.com/categories/hive/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>TOP command</title>
    <link href="http://yoursite.com/2022/01/03/top%20command/"/>
    <id>http://yoursite.com/2022/01/03/top%20command/</id>
    <published>2022-01-03T09:18:01.000Z</published>
    <updated>2022-01-03T10:28:12.172Z</updated>
    
    <content type="html"><![CDATA[<h3 id="cpu"><a href="#cpu" class="headerlink" title="cpu"></a>cpu</h3><p>us：用户态使用的cpu时间比<br>sy：系统态使用的cpu时间比<br>ni：用做nice加权的进程分配的用户态cpu时间比<br>id：空闲的cpu时间比<br>wa：cpu等待磁盘写入完成时间<br>hi：硬中断消耗时间<br>si：软中断消耗时间<br>st：虚拟机偷取时间</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;cpu&quot;&gt;&lt;a href=&quot;#cpu&quot; class=&quot;headerlink&quot; title=&quot;cpu&quot;&gt;&lt;/a&gt;cpu&lt;/h3&gt;&lt;p&gt;us：用户态使用的cpu时间比&lt;br&gt;sy：系统态使用的cpu时间比&lt;br&gt;ni：用做nice加权的进程分配的用户态cpu时间比&lt;b</summary>
      
    
    
    
    <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
    <category term="command" scheme="http://yoursite.com/tags/command/"/>
    
  </entry>
  
  <entry>
    <title>yarn point</title>
    <link href="http://yoursite.com/2021/12/31/yarn%20snippet/"/>
    <id>http://yoursite.com/2021/12/31/yarn%20snippet/</id>
    <published>2021-12-30T18:01:13.000Z</published>
    <updated>2022-01-03T16:17:18.804Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Yet Another Resource Negotiator</strong></p><p>YARN 看做一个云操作系统，它负责为应用程序启 动 ApplicationMaster（相当于主线程），然后再由 ApplicationMaster 负责数据切分、任务分配、 启动和监控等工作，而由 ApplicationMaster 启动的各个 Task（相当于子线程）仅负责自己的计 算任务。当所有任务计算完成后，ApplicationMaster 认为应用程序运行完成，然后退出。</p><p><img src="/images/yarn/yarn_construct.gif"></p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h4 id="contrainer"><a href="#contrainer" class="headerlink" title="contrainer"></a>contrainer</h4><blockquote><p>容器（Container）这个东西是 Yarn 对资源做的一层抽象。就像我们平时开发过程中，经常需要对底层一些东西进行封装，只提供给上层一个调用接口一样，Yarn 对资源的管理也是用到了这种思想。</p></blockquote><p><img src="/images/yarn/contrainer.jpeg"></p><blockquote><p>Yarn 将CPU核数，内存这些计算资源都封装成为一个个的容器（Container）。    </p></blockquote><ul><li>容器由 NodeManager 启动和管理，并被它所监控。</li><li>容器被 ResourceManager 进行调度。</li></ul><h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><blockquote><p>负责资源管理的，整个系统有且只有一个 RM ，来负责资源的调度。它也包含了两个主要的组件：定时调用器(Scheduler)以及应用管理器(ApplicationManager)。</p></blockquote><ol><li>定时调度器(Scheduler)：从本质上来说，定时调度器就是一种策略，或者说一种算法。当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配。注意，它只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪。</li><li>应用管理器(ApplicationManager)：同样，听名字就能大概知道它是干嘛的。应用管理器就是负责管理 Client 用户提交的应用。上面不是说到定时调度器（Scheduler）不对用户提交的程序监控嘛，其实啊，监控应用的工作正是由应用管理器（ApplicationManager）完成的。</li></ol><h4 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h4><blockquote><p>每当 Client 提交一个 Application 时候，就会新建一个 ApplicationMaster 。由这个 ApplicationMaster 去与 ResourceManager 申请容器资源，获得资源后会将要运行的程序发送到容器上启动，然后进行分布式计算。</p><p>ps: 大数据分布式计算的思想，大数据难以移动（海量数据移动成本太大，时间太长），那就把容易移动的应用程序发布到各个节点进行计算。</p></blockquote><h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><blockquote><p>NodeManager 是 ResourceManager 在每台机器的上代理，负责容器的管理，并监控他们的资源使用情况（cpu，内存，磁盘及网络等），以及向 ResourceManager/Scheduler 提供这些资源使用报告。</p></blockquote><h2 id="submit-application-to-yarn"><a href="#submit-application-to-yarn" class="headerlink" title="submit application to yarn"></a>submit application to yarn</h2><p><img src="/images/yarn/submit_app_flow.jpeg"></p><ol><li>Client 向 Yarn 提交 Application，这里我们假设是一个 MapReduce 作业。</li><li>ResourceManager 向 NodeManager 通信，为该 Application 分配第一个容器。并在这个容器中运行这个应用程序对应的 ApplicationMaster。</li><li>ApplicationMaster 启动以后，对 作业（也就是 Application） 进行拆分，拆分 task 出来，这些 task 可以运行在一个或多个容器中。然后向 ResourceManager 申请要运行程序的容器，并定时向 ResourceManager 发送心跳。</li><li>申请到容器后，ApplicationMaster 会去和容器对应的 NodeManager 通信，而后将作业分发到对应的 NodeManager 中的容器去运行，这里会将拆分后的 MapReduce 进行分发，对应容器中运行的可能是 Map 任务，也可能是 Reduce 任务。</li><li>容器中运行的任务会向 ApplicationMaster 发送心跳，汇报自身情况。当程序运行完成后， ApplicationMaster 再向 ResourceManager 注销并释放容器资源。</li></ol><hr><p>reference:</p><p><a href="https://zhuanlan.zhihu.com/p/54192454">https://zhuanlan.zhihu.com/p/54192454</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Yet Another Resource Negotiator&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;YARN 看做一个云操作系统，它负责为应用程序启 动 ApplicationMaster（相当于主线程），然后再由 ApplicationMaster 负责数据切</summary>
      
    
    
    
    <category term="yarn" scheme="http://yoursite.com/categories/yarn/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>数据湖</title>
    <link href="http://yoursite.com/2021/12/30/%E6%95%B0%E6%8D%AE%E6%B9%96/"/>
    <id>http://yoursite.com/2021/12/30/%E6%95%B0%E6%8D%AE%E6%B9%96/</id>
    <published>2021-12-30T01:52:19.000Z</published>
    <updated>2021-12-30T01:52:32.124Z</updated>
    
    <content type="html"><![CDATA[<p>数据湖，目前关注度比较高的有 Databricks 推出的 Delta Lake、Uber 的 Hudi 以及 Netflix 的 Iceberg</p><p>reference <a href="https://mp.weixin.qq.com/s/m8-iFg-ekykWGrG3gXlLew">https://mp.weixin.qq.com/s/m8-iFg-ekykWGrG3gXlLew</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;数据湖，目前关注度比较高的有 Databricks 推出的 Delta Lake、Uber 的 Hudi 以及 Netflix 的 Iceberg&lt;/p&gt;
&lt;p&gt;reference &lt;a href=&quot;https://mp.weixin.qq.com/s/m8-iFg-eky</summary>
      
    
    
    
    <category term="DSA" scheme="http://yoursite.com/categories/DSA/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>实时数仓</title>
    <link href="http://yoursite.com/2021/12/30/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/"/>
    <id>http://yoursite.com/2021/12/30/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/</id>
    <published>2021-12-30T01:52:19.000Z</published>
    <updated>2022-01-24T02:17:53.841Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>当设计一款产品或者平台的时候，可以划分为两层，即底层实现和上层抽象。</p></blockquote><p>实时数仓和传统数仓的对比主要可以从四个方面考虑：</p><ul><li>第一个是<strong>分层方式</strong>，离线数仓为了考虑到效率问题，一般会采取空间换时间的方式，层级划分会比较多；则实时数仓考虑到实时性问题，一般分层会比较少，另外也减少了中间流程出错的可能性。</li><li>第二个是<strong>事实数据</strong>存储方面，离线数仓会基于 HDFS，实时数仓则会基于消息队列（如 Kafka）。</li><li>第三个是<strong>维度数据</strong>存储，实时数仓会将数据放在 KV 存储上面。</li><li>第四个是<strong>数据加工</strong>过程，离线数仓一般以 Hive、Spark 等批处理为主，而实时数仓则是基于实时计算引擎如 Storm、Flink 等，以流处理为主。</li></ul><p>实时数仓主要有两个要点。首先是分层设计上，一般也是参考离线数仓的设计，通常会分为ODS操作数据层、DWD明细层、DWS汇总层以及ADS应用层，可能还会分出一层DIM维度数据层。另外分层设计上也有不同的思路，比如可以将DWS和ADS归为DM数据集市层</p><hr><p>reference：</p><p><a href="https://cloud.tencent.com/developer/article/1618182">实时数仓 | 你想要的数仓分层设计与技术选型 - 云+社区 - 腾讯云</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;当设计一款产品或者平台的时候，可以划分为两层，即底层实现和上层抽象。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实时数仓和传统数仓的对比主要可以从四个方面考虑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一个是&lt;strong&gt;分层方式&lt;/strong&gt;，离线数仓</summary>
      
    
    
    
    <category term="实时数仓" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse points</title>
    <link href="http://yoursite.com/2021/12/29/clickhouse/"/>
    <id>http://yoursite.com/2021/12/29/clickhouse/</id>
    <published>2021-12-29T11:11:30.000Z</published>
    <updated>2021-12-29T11:16:57.430Z</updated>
    
    <content type="html"><![CDATA[<h2 id="存储架构"><a href="#存储架构" class="headerlink" title="存储架构"></a>存储架构</h2><blockquote><p>Clickhouse 存储中的最小单位是 DataPart，写入链路为了提升吞吐，放弃了部分写入实时可见性，即数据攒批写入，一次批量写入的数据会落盘成一个 DataPart.</p><p>它不像 Druid 那样一条一条实时摄入。但 ClickHouse 把数据延迟攒批写入的工作交给来客户端实现，比如达到 10 条记录或每过 5s 间隔写入，换句话说就是可以在用户侧平衡吞吐量和时延，如果在业务高峰期流量不是太大，可以结合实际场景将参数调小，以达到极致的实时效果。</p></blockquote><h2 id="查询架构"><a href="#查询架构" class="headerlink" title="查询架构"></a>查询架构</h2><h3 id="计算能力方面"><a href="#计算能力方面" class="headerlink" title="计算能力方面"></a>计算能力方面</h3><p>Clickhouse 采用向量化函数和 aggregator 算子极大地提升了聚合计算性能，配合完备的 SQL 能力使得数据分析变得更加简单、灵活。</p><h3 id="数据扫描方面"><a href="#数据扫描方面" class="headerlink" title="数据扫描方面"></a>数据扫描方面</h3><p>ClickHouse 是完全列式的存储计算引擎，而且是以有序存储为核心，在查询扫描数据的过程中，首先会根据存储的有序性、列存块统计信息、分区键等信息推断出需要扫描的列存块，然后进行并行的数据扫描，像表达式计算、聚合算子都是在正规的计算引擎中处理。从计算引擎到数据扫描，数据流转都是以列存块为单位，高度向量化的。</p><h3 id="高并发服务方面"><a href="#高并发服务方面" class="headerlink" title="高并发服务方面"></a>高并发服务方面</h3><p>Clickhouse 的并发能力其实是与并行计算量和机器资源决定的。如果查询需要扫描的数据量和计算复杂度很大，并发度就会降低，但是如果保证单个 query 的 latency 足够低（增加内存和 cpu 资源），部分场景下用户可以通过设置合适的系统参数来提升并发能力，比如 max_threads 等。其他分析型系统（例如 Elasticsearch）的并发能力为什么很好，从 Cache 设计层面来看，ES 的 Cache 包括 Query Cache, Request Cache，Data Cache，Index Cache，从查询结果到索引扫描结果层层的 Cache 加速，因为 Elasticsearch 认为它的场景下存在热点数据，可能被反复查询。反观 ClickHouse，只有一个面向 IO 的 UnCompressedBlockCache 和系统的 PageCache，为了实现更优秀的并发，我们很容易想到在 Clickhouse 外面加一层 Cache，比如 redis，但是分析场景下的数据和查询都是多变的，查询结果等 Cache 都不容易命中，而且在广投业务中实时查询的数据是基于 T 之后不断更新的数据，如果外挂缓存将降低数据查询的时效性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;存储架构&quot;&gt;&lt;a href=&quot;#存储架构&quot; class=&quot;headerlink&quot; title=&quot;存储架构&quot;&gt;&lt;/a&gt;存储架构&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Clickhouse 存储中的最小单位是 DataPart，写入链路为了提升吞吐，放弃了部分写入实</summary>
      
    
    
    
    <category term="clickhouse" scheme="http://yoursite.com/categories/clickhouse/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>cdc &amp; 实时数仓 points</title>
    <link href="http://yoursite.com/2021/12/29/cdc/"/>
    <id>http://yoursite.com/2021/12/29/cdc/</id>
    <published>2021-12-29T09:42:30.000Z</published>
    <updated>2022-01-05T03:02:31.125Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Change Data Capture（变更数据获取）</strong></p><p>核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li><strong>数据同步</strong>，用于备份，容灾；</li><li><strong>数据分发</strong>，一个数据源分发给多个下游；</li><li><strong>数据采集</strong>(E)，面向数据仓库/数据湖的 ETL 数据集成。</li></ul><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>主要分为<strong>基于查询</strong>和<strong>基于 Binlog</strong> 两种方式</p><h3 id="传统-CDC-ETL"><a href="#传统-CDC-ETL" class="headerlink" title="传统 CDC ETL"></a>传统 CDC ETL</h3><p><img src="/images/cdc/cdc_etl.png"></p><h3 id="基于-Flink-CDC-的-ETL-分析"><a href="#基于-Flink-CDC-的-ETL-分析" class="headerlink" title="基于 Flink CDC 的 ETL 分析"></a>基于 Flink CDC 的 ETL 分析</h3><p><img src="/images/cdc/flink_cdc_etl.png"></p><h3 id="基于-Flink-CDC-的聚合分析"><a href="#基于-Flink-CDC-的聚合分析" class="headerlink" title="基于 Flink CDC 的聚合分析"></a>基于 Flink CDC 的聚合分析</h3><p><img src="/images/cdc/flink_cdc_aggregate.png"></p><h3 id="基于-Flink-CDC-的数据打宽"><a href="#基于-Flink-CDC-的数据打宽" class="headerlink" title="基于 Flink CDC 的数据打宽"></a>基于 Flink CDC 的数据打宽</h3><p><img src="/images/cdc/flink_cdc_merge.png"></p><h2 id="性能点"><a href="#性能点" class="headerlink" title="性能点"></a>性能点</h2><p>大数据领域的 4 类场景：</p><p><strong>B</strong>    batch    离线计算</p><p><strong>A</strong>    Analytical    交互式分析</p><p><strong>S</strong>    Servering    高并发的在线服务</p><p><strong>T</strong>    Transaction    事务隔离机制</p><blockquote><p>离线计算通常在计算层，所以应该重点考虑 A、S 和 T</p></blockquote><h2 id="考虑点"><a href="#考虑点" class="headerlink" title="考虑点"></a>考虑点</h2><ul><li><p>保证端到端的数据一致性，包括维度一致性以及全流程数据一致性;</p></li><li><p>实时流处理过程中数据到达顺序无法预知时，如何保证双流 join 时数据能及时关联同时不造成数据堵塞；</p></li><li><p>Oracle</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.Oracle 是第三方厂商维护的，不允许对线上系统有过多的侵入，容易造成监听故障甚至系统瘫痪，</span><br><span class="line">2.归档日志是在开启那一刻起才开始生成的，之前的存量数据难以进入 kafka，但是后来实时数据又必须依赖前面的计算结果</span><br></pre></td></tr></table></figure></li></ul><h2 id="实时数仓方案"><a href="#实时数仓方案" class="headerlink" title="实时数仓方案"></a>实时数仓方案</h2><h3 id="Lambda-架构"><a href="#Lambda-架构" class="headerlink" title="Lambda 架构"></a>Lambda 架构</h3><blockquote><p>目前主流的一套实时数仓架构，存在离线和实时两条链路。实时部分以消息队列的方式实时增量消费，一般以 Flink+Kafka 的组合实现，维度表存在关系型数据库或者 HBase；离线部分一般采用 T+1 周期调度分析历史存量数据，每天凌晨产出，更新覆盖前一天的结果数据，计算引擎通常会选择 Hive 或者 Spark。</p></blockquote><p><img src="/images/cdc/structure_lambda.png"></p><h3 id="Kappa-架构"><a href="#Kappa-架构" class="headerlink" title="Kappa 架构"></a>Kappa 架构</h3><blockquote><p>相较于 Lambda 架构，它移除了离线生产链路，思路是通过传递任意想要的 offset(偏移量)来达到重新消费处理历史数据的目的。优点是架构相对简化，数据来源单一，共用一套代码，开发效率高；缺点是必须要求消息队列中保存了存量数据，而且主要业务逻辑在计算层，比较消耗内存资源。</p></blockquote><p><img src="/images/cdc/structure_kappa.png"></p><h3 id="OLAP-变体架构"><a href="#OLAP-变体架构" class="headerlink" title="OLAP 变体架构"></a>OLAP 变体架构</h3><blockquote><p>是 Kappa 架构的进一步演化，它的思路是将聚合分析计算由 OLAP 引擎承担，减轻实时计算部分的聚合处理压力。优点是自由度高，可以满足数据分析师的实时自助分析需求，减轻了计算引擎的处理压力；缺点是必须要求消息队列中保存存量数据，且因为是将计算部分的压力转移到了查询层，对查询引擎的吞吐和实时摄入性能要求较高。</p></blockquote><p><img src="/images/cdc/structure_olap.png"></p><h3 id="数据湖架构"><a href="#数据湖架构" class="headerlink" title="数据湖架构"></a>数据湖架构</h3><blockquote><p>存储、计算和查询，分别由三个独立产品负责，分别是数据湖、Flink 和 Clickhouse。数仓分层存储和维度表管理均由数据湖承担，Flink SQL 负责批流任务的 SQL 化协同开发，Clickhouse 实现变体的事务机制，为用户提供离线分析和交互查询。CDC 到消息队列这一链路将来是完全可以去掉的，只需要 Flink CDC 家族中再添加 Oracle CDC 一员。未来，实时数仓架构将得到极致的简化并且性能有质的提升。</p></blockquote><p><img src="/images/cdc/structure_rtdb.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Change Data Capture（变更数据获取）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="cdc" scheme="http://yoursite.com/categories/cdc/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
</feed>
