<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Discipline &amp; Reflect</title>
  
  
  <link href="https://www.willshirley.top/atom.xml" rel="self"/>
  
  <link href="https://www.willshirley.top/"/>
  <updated>2025-03-28T06:11:37.715Z</updated>
  <id>https://www.willshirley.top/</id>
  
  <author>
    <name>brook</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>copilot</title>
    <link href="https://www.willshirley.top/2025/03/28/containerd/"/>
    <id>https://www.willshirley.top/2025/03/28/containerd/</id>
    <published>2025-03-28T06:10:57.000Z</published>
    <updated>2025-03-28T06:11:37.715Z</updated>
    
    <content type="html"><![CDATA[<h2 id="best-practise"><a href="#best-practise" class="headerlink" title="best practise"></a>best practise</h2><p><strong>change containerd’s default data path</strong></p><ol><li><p>Identify the Current Data Path</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">containerd config default | grep &quot;root&quot; # Expected output: root = &quot;/var/lib/containerd&quot;</span><br></pre></td></tr></table></figure></li><li><p>Modify the following lines in /etc/containerd/config.toml:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root = &quot;/path/to/new/data/path&quot; # the location where container data (images, volumes) is stored</span><br></pre></td></tr></table></figure></li><li><p>Move Existing Data (if required)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop containerd  # optional</span><br><span class="line">sudo mv /var/lib/containerd /data/containerd</span><br><span class="line">sudo systemctl start containerd</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;best-practise&quot;&gt;&lt;a href=&quot;#best-practise&quot; class=&quot;headerlink&quot; title=&quot;best practise&quot;&gt;&lt;/a&gt;best practise&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;change containerd’</summary>
      
    
    
    
    <category term="containerd" scheme="https://www.willshirley.top/categories/containerd/"/>
    
    
    <category term="container" scheme="https://www.willshirley.top/tags/container/"/>
    
  </entry>
  
  <entry>
    <title>ceph fs</title>
    <link href="https://www.willshirley.top/2025/03/26/ceph%20FS/"/>
    <id>https://www.willshirley.top/2025/03/26/ceph%20FS/</id>
    <published>2025-03-26T08:45:16.000Z</published>
    <updated>2025-03-26T09:53:14.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h1><p>Before mounting CephFS, ensure that</p><ol><li>the client host (where CephFS has to be mounted and used) has a copy of the Ceph configuration file (i.e. ceph.conf)</li><li>a keyring of the CephX user that has permission to access the MDS.<br>both of these files must already be present on the host where the Ceph MON resides.</li></ol><ul><li><p>Generate a minimal conf file for the client host and place it at a standard location:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> on client host</span></span><br><span class="line">mkdir -p -m 755 /etc/ceph</span><br><span class="line">ssh &#123;user&#125;@&#123;mon-host&#125; &quot;sudo ceph config generate-minimal-conf&quot; | sudo tee /etc/ceph/ceph.conf</span><br><span class="line">chmod 644 /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure></li><li><p>Create a CephX user and get its secret key:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;user&#125;@&#123;mon-host&#125; &quot;sudo ceph fs authorize cephfs-1 client.dongwei / rw&quot; | sudo tee /etc/ceph/ceph.client.dongwei.keyring</span><br></pre></td></tr></table></figure><blockquote><p>In above command, replace cephfs with the name of your CephFS, foo by the name you want for your CephX user and / by the path within your CephFS for which you want to allow access to the client host and rw stands for both read and write permissions.</p></blockquote></li><li><p>Ensure that the keyring has appropriate permissions:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 /etc/ceph/ceph.client.dongwei.keyring</span><br></pre></td></tr></table></figure></li></ul><h1 id="Create-Pool-FS"><a href="#Create-Pool-FS" class="headerlink" title="Create Pool/FS"></a>Create Pool/FS</h1><ul><li><p>Creating pools</p><blockquote><p>A Ceph file system requires at least two RADOS pools, one for data and one for metadata.</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create cephfs_data</span><br><span class="line">ceph osd pool create cephfs_metadata</span><br></pre></td></tr></table></figure></li><li><p>Creating the file system</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph fs new cephfs-1 cephfs_metadata cephfs_data</span><br></pre></td></tr></table></figure></li><li><p>Check the status of the file system</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph fs status</span><br></pre></td></tr></table></figure></li><li><p>check mds status</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mds stat</span><br></pre></td></tr></table></figure></li><li><p>Using Erasure Coded pools with CephFS（optional）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set my_ec_pool allow_ec_overwrites true</span><br></pre></td></tr></table></figure></li></ul><h1 id="Mount-CephFS-using-Kernel-Driver"><a href="#Mount-CephFS-using-Kernel-Driver" class="headerlink" title="Mount CephFS using Kernel Driver"></a>Mount CephFS using Kernel Driver</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mount -t ceph &#123;device-string&#125;=&#123;path-to-mounted&#125; &#123;mount-point&#125; -o &#123;key-value-args&#125; &#123;other-args&#125;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mount -t ceph &lt;name&gt;@&lt;fsid&gt;.&lt;fs_name&gt;=/ /mnt/mycephfs</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> fsid is the FSID of the Ceph cluster, <span class="built_in">which</span> can be found using the `ceph fsid` <span class="built_in">command</span>.</span></span><br><span class="line"></span><br><span class="line">mkdir /mnt/mycephfs</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> not use mount.helper</span></span><br><span class="line">mount -t ceph dongwei@b3acfc0d-575f-41d3-9c91-0e7ed3dbb3fa.cephfs-1=/ -o mon_addr=192.168.0.1:6789,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use mount.helper</span></span><br><span class="line">mount -t ceph dongwei@.cephfs-1=/ /mnt/cephfs -o secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use secret file and Multiple monitor hosts</span></span><br><span class="line">mount -t ceph cephuser@.cephfs=/ /mnt/mycephfs -o</span><br><span class="line">mon_addr=192.168.0.1:6789/192.168.0.2:6789,secretfile=/etc/ceph/cephuser.secret</span><br></pre></td></tr></table></figure><ul><li><p>check</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dongwei@6a65c746-e532-11ef-8ac2-fa7c097efb00.cephfs-1=/  190G     0  190G   0% /mnt/cephfs</span><br></pre></td></tr></table></figure></li></ul><h1 id="Mount-CephFS-using-FUSE"><a href="#Mount-CephFS-using-FUSE" class="headerlink" title="Mount CephFS using FUSE"></a>Mount CephFS using FUSE</h1><ul><li><p>install ceph-fuse</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Install the Ceph release RPM (adjust <span class="string">&#x27;reef&#x27;</span> as needed)</span></span><br><span class="line">sudo rpm -Uvh https://download.ceph.com/rpm-reef/el9/noarch/ceph-release-1-1.el9.noarch.rpm</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Import the Ceph GPG key</span></span><br><span class="line">sudo rpm --import &#x27;https://download.ceph.com/keys/release.asc&#x27;</span><br><span class="line"></span><br><span class="line">sudo dnf install -y ceph-fuse</span><br></pre></td></tr></table></figure></li><li><p>mount</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph-fuse &#123;mount point&#125; &#123;options&#125;</span></span><br><span class="line">mkdir /mnt/mycephfs</span><br><span class="line">ceph-fuse --id dongwei /mnt/mycephfs  # mount default fs</span><br><span class="line"></span><br><span class="line">ceph-fuse --id dongwei --client_fs cephfs-1 /mnt/mycephfs # mount specify fs</span><br></pre></td></tr></table></figure></li><li><p>check</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-fuse190G     0  190G   0% /mnt/cephfs2</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Prerequisites&quot;&gt;&lt;a href=&quot;#Prerequisites&quot; class=&quot;headerlink&quot; title=&quot;Prerequisites&quot;&gt;&lt;/a&gt;Prerequisites&lt;/h1&gt;&lt;p&gt;Before mounting CephFS, en</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="fs" scheme="https://www.willshirley.top/tags/fs/"/>
    
  </entry>
  
  <entry>
    <title>ceph csi</title>
    <link href="https://www.willshirley.top/2025/03/26/ceph%20csi/"/>
    <id>https://www.willshirley.top/2025/03/26/ceph%20csi/</id>
    <published>2025-03-26T06:14:17.000Z</published>
    <updated>2025-03-27T08:34:02.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="image"><a href="#image" class="headerlink" title="image"></a>image</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> v3.13</span></span><br><span class="line">quay.io/cephcsi/cephcsi:canary</span><br><span class="line"></span><br><span class="line">registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.13.0</span><br><span class="line">registry.k8s.io/sig-storage/csi-provisioner:v5.1.0</span><br><span class="line">registry.k8s.io/sig-storage/csi-resizer:v1.13.1</span><br><span class="line">registry.k8s.io/sig-storage/csi-snapshotter:v8.2.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> v3.10</span></span><br><span class="line">quay.io/cephcsi/cephcsi:v3.10-canary</span><br><span class="line"></span><br><span class="line">registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.1</span><br><span class="line">registry.k8s.io/sig-storage/csi-provisioner:v3.6.2</span><br><span class="line">registry.k8s.io/sig-storage/csi-resizer:v1.9.2</span><br><span class="line">registry.k8s.io/sig-storage/csi-snapshotter:v6.3.2</span><br></pre></td></tr></table></figure><h1 id="resource"><a href="#resource" class="headerlink" title="resource"></a>resource</h1><h2 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h2><ul><li>step1</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f csidriver.yaml</span><br><span class="line">kubectl create -f csi-provisioner-rbac.yaml</span><br><span class="line">kubectl create -f csi-nodeplugin-rbac.yaml</span><br></pre></td></tr></table></figure><ul><li><p>step2</p><p><code>kubectl create -f csi-config-map.yaml</code></p><blockquote><p>check by <code>ceph mon dump</code></p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-csi-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ceph</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        &quot;clusterID&quot;: &quot;xxx&quot;, </span></span><br><span class="line"><span class="string">        &quot;monitors&quot;: [</span></span><br><span class="line"><span class="string">          &quot;172.20.7.xxx:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;172.20.7.xxx:6789&quot;,</span></span><br><span class="line"><span class="string">          &quot;172.20.7.xxx:6789&quot;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br></pre></td></tr></table></figure></li><li><p>step3</p><p><code>kubectl create -f ceph-conf.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">ceph.conf:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    [global]</span></span><br><span class="line"><span class="string">    auth_cluster_required = cephx</span></span><br><span class="line"><span class="string">    auth_service_required = cephx</span></span><br><span class="line"><span class="string">    auth_client_required = cephx</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">  <span class="comment"># keyring is a required key and its value should be empty</span></span><br><span class="line">  <span class="attr">keyring:</span> <span class="string">|</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ceph</span></span><br></pre></td></tr></table></figure></li><li><p>step4</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f kms-config.yaml</span><br><span class="line">kubectl create -f csi-cephfsplugin-provisioner.yaml</span><br><span class="line">kubectl create -f csi-cephfsplugin.yaml</span><br></pre></td></tr></table></figure></li></ul><h2 id="provision"><a href="#provision" class="headerlink" title="provision"></a>provision</h2><h3 id="dynamic-provision"><a href="#dynamic-provision" class="headerlink" title="dynamic provision"></a>dynamic provision</h3><ul><li><p>step1 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph fs subvolumegroup create &lt;fsName&gt; csi</span><br></pre></td></tr></table></figure></li><li><p>step2</p><p><code>kubectl create -f secret.yaml</code></p><blockquote><p>check by <code>ceph auth get client.dongwei</code> and <code>ceph auth get client.admin</code></p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-cephfs-secret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ceph</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="comment"># Required for statically provisioned volumes, 🚨 this use user</span></span><br><span class="line">  <span class="attr">user:</span> <span class="string">dongwei</span></span><br><span class="line">  <span class="attr">userKey:</span> <span class="string">&lt;&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Required for dynamically provisioned volumes, 🚨 this use admin</span></span><br><span class="line">  <span class="attr">admin:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">adminKey:</span> <span class="string">&lt;&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Encryption passphrase</span></span><br><span class="line">  <span class="attr">encryptionPassphrase:</span> <span class="string">test_passphrase</span></span><br></pre></td></tr></table></figure></li><li><p>step3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f storageclass.yaml</span><br><span class="line">kubectl create -f pvc.yaml</span><br><span class="line">kubectl create -f pod.yaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="static-provision"><a href="#static-provision" class="headerlink" title="static provision"></a>static provision</h3><ul><li><p>step1 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph fs subvolumegroup create &lt;fsName&gt; testGroup</span><br><span class="line">ceph fs subvolume create &lt;fsName&gt; testSubVolume testGroup --size=1073741824  # byte 1073741824/1024/1024=1024MB</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> check</span></span><br><span class="line">ceph fs subvolume getpath &lt;fsName&gt; testSubVolume testGroup</span><br></pre></td></tr></table></figure></li><li><p>step2</p><p><code>kubectl create -f secret.yaml</code></p><blockquote><p>check by <code>ceph auth get client.dongwei</code> </p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-cephfs-secret-static</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ceph</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="comment"># Required for statically provisioned volumes, 🚨 this use userID</span></span><br><span class="line">  <span class="attr">userID:</span> <span class="string">dongwei</span></span><br><span class="line">  <span class="attr">userKey:</span> <span class="string">&lt;&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Encryption passphrase</span></span><br><span class="line">  <span class="attr">encryptionPassphrase:</span> <span class="string">test_passphrase</span></span><br></pre></td></tr></table></figure></li><li><p>step3</p><p><code>kubectl create -f pv.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cephfs-static-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">csi:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">cephfs.csi.ceph.com</span></span><br><span class="line">    <span class="attr">nodeStageSecretRef:</span></span><br><span class="line">      <span class="comment"># node stage secret name</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">csi-cephfs-secret</span></span><br><span class="line">      <span class="comment"># node stage secret namespace where above secret is created</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">ceph</span></span><br><span class="line">    <span class="attr">volumeAttributes:</span></span><br><span class="line">      <span class="comment"># optional file system to be mounted</span></span><br><span class="line">      <span class="attr">&quot;fsName&quot;:</span> <span class="string">&quot;cephfs-1&quot;</span></span><br><span class="line">      <span class="comment"># Required options from storageclass parameters need to be added in volumeAttributes</span></span><br><span class="line">      <span class="attr">&quot;clusterID&quot;:</span> <span class="string">&quot;ba68226a-672f-4ba5-97bc-22840318b2ec&quot;</span></span><br><span class="line">      <span class="attr">&quot;staticVolume&quot;:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">&quot;rootPath&quot;:</span> <span class="string">/volumes/testGroup/testSubVolume</span></span><br><span class="line">    <span class="comment"># volumeHandle can be anything, need not to be same</span></span><br><span class="line">    <span class="comment"># as PV name or volume name. keeping same for brevity</span></span><br><span class="line">    <span class="attr">volumeHandle:</span> <span class="string">cephfs-static-pv</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure><blockquote><p>rootPath 也可以指定为  /volumes/csi</p></blockquote></li><li><p>step4</p><p><code>kubectl create -f pvc.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cephfs-static-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="comment"># volumeName should be same as PV name</span></span><br><span class="line">  <span class="attr">volumeName:</span> <span class="string">cephfs-static-pv</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;image&quot;&gt;&lt;a href=&quot;#image&quot; class=&quot;headerlink&quot; title=&quot;image&quot;&gt;&lt;/a&gt;image&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="csi" scheme="https://www.willshirley.top/tags/csi/"/>
    
  </entry>
  
  <entry>
    <title>kubelet snippet</title>
    <link href="https://www.willshirley.top/2025/03/25/kubelet%20snippet/"/>
    <id>https://www.willshirley.top/2025/03/25/kubelet%20snippet/</id>
    <published>2025-03-25T06:58:19.000Z</published>
    <updated>2025-03-25T06:59:59.187Z</updated>
    
    <content type="html"><![CDATA[<h1 id="troubleshooting"><a href="#troubleshooting" class="headerlink" title="troubleshooting"></a>troubleshooting</h1><h2 id="Version-from-runtime-service-failed"><a href="#Version-from-runtime-service-failed" class="headerlink" title="Version from runtime service failed"></a>Version from runtime service failed</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">E0325 12:15:10.889620 3185179 remote_runtime.go:189] &quot;Version from runtime service failed&quot; err=&quot;rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService&quot;</span><br><span class="line">E0325 12:15:10.889663 3185179 kuberuntime_manager.go:226] &quot;Get runtime version failed&quot; err=&quot;get remote runtime typed version failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService&quot;</span><br><span class="line">E0325 12:15:10.889683 3185179 run.go:74] &quot;command failed&quot; err=&quot;failed to run Kubelet: failed to create kubelet: get remote runtime typed version failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService&quot;</span><br></pre></td></tr></table></figure><ul><li><p>resolve</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">复制正常节点 /etc/containerd/config.toml 配置文件，然后重启 containerd</span><br><span class="line">sudo systemctl restart containerd</span><br><span class="line"></span><br><span class="line">kubelet服务会自动拉起</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;troubleshooting&quot;&gt;&lt;a href=&quot;#troubleshooting&quot; class=&quot;headerlink&quot; title=&quot;troubleshooting&quot;&gt;&lt;/a&gt;troubleshooting&lt;/h1&gt;&lt;h2 id=&quot;Version-from-</summary>
      
    
    
    
    <category term="kubernetes" scheme="https://www.willshirley.top/categories/kubernetes/"/>
    
    
    <category term="kubelet" scheme="https://www.willshirley.top/tags/kubelet/"/>
    
  </entry>
  
  <entry>
    <title>clang-uml</title>
    <link href="https://www.willshirley.top/2025/03/20/clang-uml/"/>
    <id>https://www.willshirley.top/2025/03/20/clang-uml/</id>
    <published>2025-03-20T11:27:14.000Z</published>
    <updated>2025-03-24T07:46:54.780Z</updated>
    
    <content type="html"><![CDATA[<h1 id="clang-uml"><a href="#clang-uml" class="headerlink" title="clang-uml"></a>clang-uml</h1><h2 id="init"><a href="#init" class="headerlink" title="init"></a>init</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> init .clang-uml file</span></span><br><span class="line">clang-uml --init</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use specify config file</span></span><br><span class="line">clang-uml -c /path/to/configFile</span><br></pre></td></tr></table></figure><h2 id="generate-PlantUML"><a href="#generate-PlantUML" class="headerlink" title="generate PlantUML"></a>generate PlantUML</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> generate xxx.puml base on default config</span></span><br><span class="line">clang-uml # --progress</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> draw svg picture base on above puml file</span></span><br><span class="line">plantuml -tsvg xxx.puml</span><br></pre></td></tr></table></figure><h2 id="generate-mermaid"><a href="#generate-mermaid" class="headerlink" title="generate  mermaid"></a>generate  mermaid</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> genrate xxx.mmd</span></span><br><span class="line">clang-uml -g mermaid</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> draw svg</span></span><br><span class="line">mmdc -i xxx.mmd # -o &lt;name&gt;.svg</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> draw big svg</span></span><br><span class="line">vim config.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;maxTextSize&quot;: 1000000</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mmdc -i diagram.mmd -c config.json</span><br></pre></td></tr></table></figure><h2 id="other"><a href="#other" class="headerlink" title="other"></a>other</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To find the exact <span class="keyword">function</span> signature</span></span><br><span class="line"> clang-uml --print-from -n client_class_diagram -c .clang-uml-sequence | grep main</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;clang-uml&quot;&gt;&lt;a href=&quot;#clang-uml&quot; class=&quot;headerlink&quot; title=&quot;clang-uml&quot;&gt;&lt;/a&gt;clang-uml&lt;/h1&gt;&lt;h2 id=&quot;init&quot;&gt;&lt;a href=&quot;#init&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="clang-uml" scheme="https://www.willshirley.top/categories/clang-uml/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>conda</title>
    <link href="https://www.willshirley.top/2025/03/14/conda/"/>
    <id>https://www.willshirley.top/2025/03/14/conda/</id>
    <published>2025-03-14T04:22:53.000Z</published>
    <updated>2025-03-16T04:45:40.949Z</updated>
    
    <content type="html"><![CDATA[<h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> download binary</span></span><br><span class="line">mkdir -p ~/miniconda3</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh</span><br><span class="line">bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3</span><br><span class="line">rm ~/miniconda3/miniconda.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> active</span></span><br><span class="line">source ~/miniconda3/bin/activate</span><br><span class="line">conda init --all</span><br></pre></td></tr></table></figure><h1 id="command"><a href="#command" class="headerlink" title="command"></a>command</h1><ul><li><p>查看环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure></li><li><p>新建环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name your_env_name</span><br></pre></td></tr></table></figure><p>指定python版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name xxx python=3.10</span><br></pre></td></tr></table></figure></li><li><p>激活环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate xxx</span><br></pre></td></tr></table></figure></li><li><p>删除环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove --name xxx --all</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;install&quot;&gt;&lt;a href=&quot;#install&quot; class=&quot;headerlink&quot; title=&quot;install&quot;&gt;&lt;/a&gt;install&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    <category term="conda" scheme="https://www.willshirley.top/categories/conda/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>nvidia cuda</title>
    <link href="https://www.willshirley.top/2025/03/12/cuda/"/>
    <id>https://www.willshirley.top/2025/03/12/cuda/</id>
    <published>2025-03-12T07:59:26.000Z</published>
    <updated>2025-03-17T03:14:03.109Z</updated>
    
    <content type="html"><![CDATA[<h1 id="version"><a href="#version" class="headerlink" title="version"></a>version</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GPU  drivercudavllm torch</span><br><span class="line">h10055012.4</span><br><span class="line">h20056512.70.7.32.5.1+cu124</span><br><span class="line">h20057012.80.7.32.5.1+cu124</span><br></pre></td></tr></table></figure><h1 id="install"><a href="#install" class="headerlink" title="install"></a>install</h1><h2 id="CUDA-Toolkit"><a href="#CUDA-Toolkit" class="headerlink" title="CUDA Toolkit"></a>CUDA Toolkit</h2><blockquote><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></p></blockquote><ul><li><p>local</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</span><br><span class="line"></span><br><span class="line">sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600</span><br><span class="line"></span><br><span class="line">wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda-repo-ubuntu2204-12-8-local_12.8.1-570.124.06-1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu2204-12-8-local_12.8.1-570.124.06-1_amd64.deb</span><br><span class="line"></span><br><span class="line">sudo cp /var/cuda-repo-ubuntu2204-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get -y install cuda-toolkit-12-8</span><br></pre></td></tr></table></figure></li><li><p>online</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb</span><br><span class="line">sudo dpkg -i cuda-keyring_1.1-1_all.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install cuda-toolkit-12-8</span><br></pre></td></tr></table></figure></li></ul><h3 id="post-operate"><a href="#post-operate" class="headerlink" title="post operate"></a>post operate</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> config env</span></span><br><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NVIDIA persistence daemon</span></span><br><span class="line">sudo systemctl start nvidia-persistenced</span><br></pre></td></tr></table></figure><h2 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y cuda-drivers</span><br></pre></td></tr></table></figure><h2 id="nvidia-fabricmanager"><a href="#nvidia-fabricmanager" class="headerlink" title="nvidia-fabricmanager"></a>nvidia-fabricmanager</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y nvidia-fabricmanager-570</span><br><span class="line"></span><br><span class="line">sudo systemctl start nvidia-fabricmanager</span><br></pre></td></tr></table></figure><h1 id="ENV"><a href="#ENV" class="headerlink" title="ENV"></a>ENV</h1><blockquote><p>resolve issues: Error 802: system not yet initialized</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sort GPUs, by ordering their IDs with IDs on the PCIe bus.</span></span><br><span class="line">export CUDA_DEVICE_ORDER=&quot;PCI_BUS_ID&quot; </span><br><span class="line"><span class="meta">#</span><span class="bash"> perform an availability check using NVML (NVIDIA Management Library). NVML is an API layer <span class="keyword">for</span> obtaining data directly from the NVIDIA-smi utility.</span></span><br><span class="line">export PYTORCH_NVML_BASED_CUDA_CHECK=1 </span><br><span class="line"><span class="meta">#</span><span class="bash"> force show the system the IDs of available GPUs.</span></span><br><span class="line">export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7</span><br></pre></td></tr></table></figure><h1 id="cuda-kernel-model"><a href="#cuda-kernel-model" class="headerlink" title="cuda kernel model"></a>cuda kernel model</h1><ul><li>check by <code>lsmod | grep nvidia</code></li></ul><table><thead><tr><th><strong>Module</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>nvidia_uvm</td><td>NVIDIA’s Unified Memory driver</td></tr><tr><td>nvidia_drm</td><td>Direct Rendering Manager support</td></tr><tr><td>nvidia_modeset</td><td>Kernel mode-setting support</td></tr><tr><td>nvidia</td><td>Main NVIDIA driver module</td></tr></tbody></table><h1 id="command"><a href="#command" class="headerlink" title="command"></a>command</h1><h2 id="nvidia-smi"><a href="#nvidia-smi" class="headerlink" title="nvidia-smi"></a>nvidia-smi</h2><ul><li><p>Enable Persistence Mode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-smi -pm 1</span><br></pre></td></tr></table></figure></li><li><p>check state</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi conf-compute -grs</span><br><span class="line"><span class="meta">#</span><span class="bash"> Confidential Compute GPUs Ready state: not-ready</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Confidential Compute GPUs Ready state: ready</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> above state is not-ready, execute below cmd</span></span><br><span class="line">nvidia-smi conf-compute -srs 1</span><br></pre></td></tr></table></figure></li><li><p>cuda 12.8</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |</span><br><span class="line">| N/A   23C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H200                    On  |   00000000:3B:00.0 Off |                    0 |</span><br><span class="line">| N/A   21C    P0             75W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H200                    On  |   00000000:4C:00.0 Off |                    0 |</span><br><span class="line">| N/A   23C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H200                    On  |   00000000:5D:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H200                    On  |   00000000:9B:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             75W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H200                    On  |   00000000:BB:00.0 Off |                    0 |</span><br><span class="line">| N/A   23C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H200                    On  |   00000000:CB:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H200                    On  |   00000000:DB:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                              |</span><br><span class="line">|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |</span><br><span class="line">|        ID   ID                                                               Usage      |</span><br><span class="line">|=========================================================================================|</span><br><span class="line">|  No running processes found                                                             |</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></li><li><p>cuda 12.7</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Fri Mar 14 10:23:56 2025       </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            111W /  700W |  134402MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H200                    On  |   00000000:3B:00.0 Off |                    0 |</span><br><span class="line">| N/A   25C    P0            116W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H200                    On  |   00000000:4C:00.0 Off |                    0 |</span><br><span class="line">| N/A   25C    P0            112W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H200                    On  |   00000000:5D:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            115W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H200                    On  |   00000000:9B:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            115W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H200                    On  |   00000000:BB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            114W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H200                    On  |   00000000:CB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            113W /  700W |  132320MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H200                    On  |   00000000:DB:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0            114W /  700W |  131840MiB / 143771MiB |      0%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                              |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |</span><br><span class="line">|        ID   ID                                                               Usage      |</span><br><span class="line">|=========================================================================================|</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><h1 id="vllm"><a href="#vllm" class="headerlink" title="vllm"></a>vllm</h1><h2 id="install-offline"><a href="#install-offline" class="headerlink" title="install offline"></a>install offline</h2><p>On your local machine, create a virtual environment:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv vllm_env</span><br><span class="line">source vllm_env/bin/activate</span><br></pre></td></tr></table></figure><p>1️⃣ <strong>On your local machine:</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip download --dest=./vllm_deps vllm</span><br></pre></td></tr></table></figure><p>2️⃣ <strong>Transfer dependencies to the remote server:</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r vllm_deps user@remote_server:/path/to/destination/</span><br></pre></td></tr></table></figure><p>3️⃣ <strong>On the remote server:</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /path/to/destination/vllm_deps</span><br><span class="line">pip install --no-index --find-links=. vllm*</span><br><span class="line">•--no-index tells pip not to use the internet.</span><br><span class="line">•--find-links=./vllm_deps tells pip to look for packages in this directory.</span><br><span class="line">•vllm* ensures pip finds the correct package in that folder.</span><br></pre></td></tr></table></figure><h2 id="running-time"><a href="#running-time" class="headerlink" title="running time"></a>running time</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vllm serve /mnt/dingofs-test/DeepSeek-R1 --host 0.0.0.0 --port 8000 --served-model-name deepseek-r1 --tensor-parallel-size 8 --gpu-memory-utilization 0.85 --max-model-len 128000 --max-num-batched-tokens 32000 --max-num-seqs 1024 --trust-remote-code --enable-reasoning --reasoning-parser deepseek_r1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">Sat Mar 15 20:33:04 2025       </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |</span><br><span class="line">|-----------------------------------------+------------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                        |               MIG M. |</span><br><span class="line">|=========================================+========================+======================|</span><br><span class="line">|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            115W /  700W |   84474MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H200                    On  |   00000000:3B:00.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0            113W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H200                    On  |   00000000:4C:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            114W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H200                    On  |   00000000:5D:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            117W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H200                    On  |   00000000:9B:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            114W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H200                    On  |   00000000:BB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            116W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H200                    On  |   00000000:CB:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0            115W /  700W |   84522MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H200                    On  |   00000000:DB:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0            114W /  700W |   84282MiB / 143771MiB |      1%      Default |</span><br><span class="line">|                                         |                        |             Disabled |</span><br><span class="line">+-----------------------------------------+------------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                              |</span><br><span class="line">|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |</span><br><span class="line">|        ID   ID                                                               Usage      |</span><br><span class="line">|=========================================================================================|</span><br><span class="line">|    0   N/A  N/A          915920      C   ...niconda3/envs/vllm/bin/python      84464MiB |</span><br><span class="line">|    1   N/A  N/A          916338      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    2   N/A  N/A          916339      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    3   N/A  N/A          916340      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    4   N/A  N/A          916341      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    5   N/A  N/A          916342      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    6   N/A  N/A          916343      C   ...niconda3/envs/vllm/bin/python      84512MiB |</span><br><span class="line">|    7   N/A  N/A          916344      C   ...niconda3/envs/vllm/bin/python      84272MiB |</span><br><span class="line">+-----------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><ul><li><p>log</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INFO 03-15 20:36:48 worker.py:267] Memory profiling takes 7.63 seconds</span><br><span class="line">INFO 03-15 20:36:48 worker.py:267] the current vLLM instance can use total_gpu_memory (139.81GiB) x gpu_memory_utilization (0.85) = 118.84GiB</span><br><span class="line">INFO 03-15 20:36:48 worker.py:267] model weights take 83.88GiB; non_torch_memory takes 7.16GiB; PyTorch activation peak memory takes 6.37GiB; the rest of the memory reserved </span><br><span class="line">for KV Cache is 21.43GiB.</span><br><span class="line">INFO 03-15 20:36:48 executor_base.py:111] # cuda blocks: 18418, # CPU blocks: 3437</span><br><span class="line">INFO 03-15 20:36:48 executor_base.py:116] Maximum concurrency for 128000 tokens per request: 2.30x</span><br></pre></td></tr></table></figure></li><li><p>chat</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions \</span><br><span class="line">    -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;deepseek-r1&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;introduce yourself&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure></li></ul><h1 id="sglang"><a href="#sglang" class="headerlink" title="sglang"></a>sglang</h1><h2 id="install-offline-1"><a href="#install-offline-1" class="headerlink" title="install offline"></a>install offline</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> prepare env</span></span><br><span class="line">python3 -m venv sglang_env</span><br><span class="line">source sglang_env/bin/activate</span><br><span class="line"><span class="meta">#</span><span class="bash"> optional use uv</span></span><br><span class="line">pip install --upgrade pip</span><br><span class="line"><span class="meta">#</span><span class="bash">pip install uv</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> download deps</span></span><br><span class="line">mkdir -p ./sglang_deps</span><br><span class="line">pip download &quot;sglang[all]&gt;=0.4.4.post1&quot; --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python -d ./sglang_deps</span><br><span class="line"><span class="meta">#</span><span class="bash"> scp deps to remote</span></span><br><span class="line">scp -r sglang_deps user@remote_server:/path/to/destination/</span><br><span class="line"><span class="meta">#</span><span class="bash"> install sglang on remote</span></span><br><span class="line">cd /path/to/remote/sglang_deps</span><br><span class="line">pip install --no-index --find-links=. &quot;sglang[all]&gt;=0.4.4.post1&quot;</span><br></pre></td></tr></table></figure><h2 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m sglang.launch_server --model /mnt/3fs/DeepSeek-R1 --tp 8 --trust-remote-code --port 30000</span><br></pre></td></tr></table></figure><ul><li><p>chat</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:30000/v1/chat/completions \</span><br><span class="line">    -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;deepseek-r1&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;introduce yourself&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure></li></ul><h1 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h1><h2 id="install-offline-2"><a href="#install-offline-2" class="headerlink" title="install offline"></a>install offline</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/torch_deps</span><br><span class="line">pip download --dest=~/torch_deps torch==2.5.1 --extra-index-url https://download.pytorch.org/whl/nightly/cu128</span><br><span class="line"></span><br><span class="line">scp -r ~/torch_deps user@remote_server:/path/to/remote/directory</span><br><span class="line">cd /path/to/remote/directory</span><br><span class="line">pip install --no-index --find-links=./ torch</span><br></pre></td></tr></table></figure><ul><li><p>check</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c &quot;import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())&quot;</span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())  <span class="comment"># print false</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.device_count())  <span class="comment"># print 8</span></span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># print 2.5.1+cu124</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># print 12.4</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;version&quot;&gt;&lt;a href=&quot;#version&quot; class=&quot;headerlink&quot; title=&quot;version&quot;&gt;&lt;/a&gt;version&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    <category term="cuda" scheme="https://www.willshirley.top/categories/cuda/"/>
    
    
    <category term="nvidia" scheme="https://www.willshirley.top/tags/nvidia/"/>
    
  </entry>
  
  <entry>
    <title>k8s ctr</title>
    <link href="https://www.willshirley.top/2025/03/12/k8s%20ctr/"/>
    <id>https://www.willshirley.top/2025/03/12/k8s%20ctr/</id>
    <published>2025-03-12T07:59:26.000Z</published>
    <updated>2025-03-20T16:22:21.010Z</updated>
    
    <content type="html"><![CDATA[<h1 id="command"><a href="#command" class="headerlink" title="command"></a>command</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> check k8s image</span></span><br><span class="line">ctr --namespace k8s.io images list</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;command&quot;&gt;&lt;a href=&quot;#command&quot; class=&quot;headerlink&quot; title=&quot;command&quot;&gt;&lt;/a&gt;command&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    <category term="k8s" scheme="https://www.willshirley.top/categories/k8s/"/>
    
    
    <category term="ctr" scheme="https://www.willshirley.top/tags/ctr/"/>
    
  </entry>
  
  <entry>
    <title>network solution</title>
    <link href="https://www.willshirley.top/2025/03/10/network%20solution/"/>
    <id>https://www.willshirley.top/2025/03/10/network%20solution/</id>
    <published>2025-03-10T02:04:37.000Z</published>
    <updated>2025-03-31T06:15:25.550Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h1><h2 id="add-new-dns"><a href="#add-new-dns" class="headerlink" title="add new dns"></a>add new dns</h2><ul><li><p>method 1:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step1: vim /etc/systemd/resolved.conf</span></span><br><span class="line">[<span class="string">Resolve</span>]</span><br><span class="line"><span class="string">DNS=10.201.44.51</span> <span class="number">223.5</span><span class="number">.5</span><span class="number">.5</span>  <span class="comment"># new DNS</span></span><br><span class="line"><span class="string">FallbackDNS=127.0.0.53</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step2: restart service</span></span><br><span class="line"><span class="string">sudo</span> <span class="string">systemctl</span> <span class="string">restart</span> <span class="string">systemd-resolved</span></span><br></pre></td></tr></table></figure></li><li><p>method2: (有时候不起效)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">vim</span> <span class="string">/etc/resolv.conf</span></span><br><span class="line"><span class="comment"># 注意：添加的dns解析地址，需要放在前面，会有顺序影响</span></span><br><span class="line"></span><br><span class="line"><span class="string">nameserver</span> <span class="number">10.201</span><span class="number">.44</span><span class="number">.51</span>  <span class="comment"># new DNS</span></span><br><span class="line"><span class="string">nameserver</span> <span class="number">223.5</span><span class="number">.5</span><span class="number">.5</span><span class="comment"># new DNS</span></span><br><span class="line"><span class="string">nameserver</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.53</span></span><br></pre></td></tr></table></figure></li></ul><h1 id="network"><a href="#network" class="headerlink" title="network"></a>network</h1><h2 id="create-new-network"><a href="#create-new-network" class="headerlink" title="create new network"></a>create new network</h2><p>step1: vim <code>/etc/netplan/ib.yaml</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">network:</span><br><span class="line">  version: 2</span><br><span class="line">  renderer: networkd</span><br><span class="line">  ethernets:</span><br><span class="line">    ibp27s0:</span><br><span class="line">      dhcp4: no</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.2.52/21</span><br></pre></td></tr></table></figure><p>step2: <code>netplan apply</code></p><h1 id="bond"><a href="#bond" class="headerlink" title="bond"></a>bond</h1><h2 id="create-bond"><a href="#create-bond" class="headerlink" title="create bond"></a>create bond</h2><p>Bonding Mode</p><table><thead><tr><th>Mode</th><th>Max Speed (Single Flow)</th><th>Max Speed (Multiple Flows)</th><th>Key Feature</th></tr></thead><tbody><tr><td>mode=0 (round-robin)</td><td>Sum of all slaves</td><td>Sum of all slaves</td><td>Best for maximizing bandwidth, may cause out-of-order packets.</td></tr><tr><td>mode=1 (active-backup)</td><td>One interface’s speed</td><td>One interface’s speed</td><td>Best for redundancy, no speed gain.</td></tr><tr><td>mode=2 (balance-xor)</td><td>One interface’s speed</td><td>Sum of all slaves</td><td>Good for performance; switch support required.</td></tr><tr><td>mode=4 (802.3ad - LACP)</td><td>One interface’s speed</td><td>Sum of all slaves</td><td>Efficient load balancing for multiple flows; requires switch support.</td></tr><tr><td>mode=5 (balance-tlb)</td><td>One interface’s speed</td><td>Sum of all slaves (outgoing only)</td><td>Adaptive transmit load balancing.</td></tr><tr><td>mode=6 (balance-alb)</td><td>One interface’s speed</td><td>Sum of all slaves</td><td>Adaptive load balancing without switch support.</td></tr></tbody></table><blockquote><p> assume make ib7s400p0 to bond1, which have assigned  ip 172.30.12.46 with ib7s400p0</p></blockquote><p><strong>optional</strong>: Load the Bonding Kernel Module</p><p>Ensure the bonding driver is loaded:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe bonding</span><br></pre></td></tr></table></figure><p>To persist this across reboots:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;bonding&quot; | sudo tee /etc/modules-load.d/bonding.conf</span><br></pre></td></tr></table></figure><p><strong>optional</strong>: Remove the IP Address from ib7s400p0</p><p>Since the bond interface will carry the IP, you must remove the IP from ib7s400p0 first:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr del 172.30.12.61/24 dev ib7s400p0</span><br></pre></td></tr></table></figure><p><strong>Step 2: Create the Bond Interface (bond1)</strong></p><p>Create the bond interface:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link add bond1 type bond</span><br></pre></td></tr></table></figure><p><strong>Step 3: Configure Bonding Mode</strong></p><p>For your use case, you can set mode=active-backup (best for redundancy with one NIC now) or mode=802.3ad (if planning for LACP in the future).</p><p><strong>Set Active-Backup Mode (Recommended for 1 NIC Now)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;active-backup&quot; | sudo tee /sys/class/net/bond1/bonding/mode</span><br></pre></td></tr></table></figure><p><strong>OR Set 802.3ad Mode (If Future Expansion is Planned)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;802.3ad&quot; | sudo tee /sys/class/net/bond1/bonding/mode</span><br></pre></td></tr></table></figure><p><strong>Step 4: Add ib7s400p0 as a Slave</strong></p><ol><li>Remove any IP address from ib7s400p0 (if it has one):</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev ib7s400p0</span><br></pre></td></tr></table></figure><ol start="2"><li>Add ib7s400p0 to bond1:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set ib7s400p0 down</span><br><span class="line">sudo ip link set ib7s400p0 master bond1</span><br><span class="line">sudo ip link set ib7s400p0 up</span><br></pre></td></tr></table></figure><blockquote><p>check: ethtool ib7s400p0</p></blockquote><ol start="3"><li>Bring bond1 up:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set bond1 up</span><br></pre></td></tr></table></figure><blockquote><p>check: cat /proc/net/bonding/bond1</p></blockquote><p><strong>Step 5: Assign an IP Address</strong></p><p>If using a <strong>static IP</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr add 172.30.12.61/24 dev bond1</span><br></pre></td></tr></table></figure><p>Or if using <strong>DHCP</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dhclient bond1</span><br></pre></td></tr></table></figure><p>Try to force traffic through bond1 by removing the direct route through ib7s400p0</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip route del 172.30.12.0/24 dev ib7s400p0</span><br></pre></td></tr></table></figure><p><strong>Step 6: Persistent Configuration (Rocky 9 / RHEL 9) (optional)</strong></p><p>To ensure the bond configuration persists after reboot:</p><ol><li>Create <code>/etc/sysconfig/network-scripts/ifcfg-bond1</code></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=bond1</span><br><span class="line">TYPE=Bond</span><br><span class="line">BONDING_MASTER=yes</span><br><span class="line">BOOTPROTO=dhcp      # Or use &#x27;static&#x27; if assigning a static IP</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BONDING_OPTS=&quot;mode=active-backup miimon=100&quot;</span><br></pre></td></tr></table></figure><ol start="2"><li>Create <code>/etc/sysconfig/network-scripts/ifcfg-ib7s400p0</code></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=ib7s400p0</span><br><span class="line">MASTER=bond1</span><br><span class="line">SLAVE=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">ONBOOT=yes</span><br></pre></td></tr></table></figure><ol start="3"><li>Restart NetworkManager to apply changes:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart NetworkManager</span><br></pre></td></tr></table></figure><p><strong>Step 7: Verify Configuration</strong></p><ol><li>Check bond details:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/net/bonding/bond1</span><br></pre></td></tr></table></figure><p>ib7s400p0 should now appear as a <strong>Slave Interface</strong>.</p><ol start="2"><li>Confirm IP address and route:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip a</span><br><span class="line">ip route</span><br></pre></td></tr></table></figure><ol start="3"><li>Test connectivity:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping 172.30.12.47 # other same bond addr</span><br></pre></td></tr></table></figure><p><strong>Step 8: Optional - Test Failover (For active-backup Mode)</strong></p><p>​    1.    Temporarily bring down ib7s400p0:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set ib7s400p0 down</span><br></pre></td></tr></table></figure><p>​    2.    Check bond1 status:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/net/bonding/bond1</span><br></pre></td></tr></table></figure><p>In active-backup mode, bond1 should remain active (even though ib7s400p0 is down).</p><p>In 802.3ad mode, bond1 would go down since no alternate NIC exists yet.</p><p><strong>Summary</strong></p><p>Use <strong>active-backup</strong> mode if ib7s400p0 is the only slave (recommended now).</p><p>Use <strong>802.3ad</strong> if planning to add more NICs for higher throughput in the future.</p><p>Ensure <code>/etc/sysconfig/network-scripts/</code> configs are properly set for persistence.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DNS&quot;&gt;&lt;a href=&quot;#DNS&quot; class=&quot;headerlink&quot; title=&quot;DNS&quot;&gt;&lt;/a&gt;DNS&lt;/h1&gt;&lt;h2 id=&quot;add-new-dns&quot;&gt;&lt;a href=&quot;#add-new-dns&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    <category term="network" scheme="https://www.willshirley.top/categories/network/"/>
    
    
    <category term="solution" scheme="https://www.willshirley.top/tags/solution/"/>
    
  </entry>
  
  <entry>
    <title>llm vllm</title>
    <link href="https://www.willshirley.top/2025/03/06/llm%20vllm/"/>
    <id>https://www.willshirley.top/2025/03/06/llm%20vllm/</id>
    <published>2025-03-06T07:25:11.000Z</published>
    <updated>2025-03-06T07:28:51.215Z</updated>
    
    <content type="html"><![CDATA[<h1 id="route"><a href="#route" class="headerlink" title="route"></a>route</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">INFO llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 135.23 seconds</span><br><span class="line">INFO api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000</span><br><span class="line">Available routes are:</span><br><span class="line"> Route: /openapi.json, Methods: GET, HEAD</span><br><span class="line"> Route: /docs, Methods: GET, HEAD</span><br><span class="line"> Route: /docs/oauth2-redirect, Methods: GET, HEAD</span><br><span class="line"> Route: /redoc, Methods: GET, HEAD</span><br><span class="line"> Route: /health, Methods: GET</span><br><span class="line"> Route: /ping, Methods: GET, POST</span><br><span class="line"> Route: /tokenize, Methods: POST</span><br><span class="line"> Route: /detokenize, Methods: POST</span><br><span class="line"> Route: /v1/models, Methods: GET</span><br><span class="line"> Route: /version, Methods: GET</span><br><span class="line"> Route: /v1/chat/completions, Methods: POST</span><br><span class="line"> Route: /v1/completions, Methods: POST</span><br><span class="line"> Route: /v1/embeddings, Methods: POST</span><br><span class="line"> Route: /pooling, Methods: POST</span><br><span class="line"> Route: /score, Methods: POST</span><br><span class="line"> Route: /v1/score, Methods: POST</span><br><span class="line"> Route: /v1/audio/transcriptions, Methods: POST</span><br><span class="line"> Route: /rerank, Methods: POST</span><br><span class="line"> Route: /v1/rerank, Methods: POST</span><br><span class="line"> Route: /v2/rerank, Methods: POST</span><br><span class="line"> Route: /invocations, Methods: POST</span><br><span class="line">INFO:     Started server process [1]</span><br><span class="line">INFO:     Waiting for application startup.</span><br><span class="line">INFO:     Application startup complete.</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;route&quot;&gt;&lt;a href=&quot;#route&quot; class=&quot;headerlink&quot; title=&quot;route&quot;&gt;&lt;/a&gt;route&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;</summary>
      
    
    
    
    <category term="llm" scheme="https://www.willshirley.top/categories/llm/"/>
    
    
    <category term="sinppet" scheme="https://www.willshirley.top/tags/sinppet/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes RBAC</title>
    <link href="https://www.willshirley.top/2025/03/01/kubernetes%20RBAC/"/>
    <id>https://www.willshirley.top/2025/03/01/kubernetes%20RBAC/</id>
    <published>2025-03-01T11:48:15.000Z</published>
    <updated>2025-03-01T12:20:55.733Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>RBAC (Role-Based Access Control) in Kubernetes is a security model that controls who can access and perform actions on resources (like Pods, Deployments, Services, etc.) within a cluster.</p></blockquote><h1 id="Component"><a href="#Component" class="headerlink" title="Component"></a>Component</h1><p>Kubernetes RBAC consists of four main components:</p><table><thead><tr><th>Component</th><th>Description</th></tr></thead><tbody><tr><td>Role / ClusterRole</td><td>Defines what actions (verbs) can be performed on which resources.</td></tr><tr><td>RoleBinding / ClusterRoleBinding</td><td>Grants a Role or ClusterRole to a User, Group, or ServiceAccount.</td></tr><tr><td>Subjects (Users, Groups, ServiceAccounts)</td><td>Who is allowed to perform actions (User, Group, or ServiceAccount).</td></tr><tr><td>Resources &amp; API Groups</td><td>The objects that can be controlled (e.g., pods, deployments, services, etc.).</td></tr></tbody></table><h2 id="Role-amp-ClusterRole"><a href="#Role-amp-ClusterRole" class="headerlink" title="Role &amp; ClusterRole"></a>Role &amp; ClusterRole</h2><blockquote><p>RBAC defines what actions can be performed on which resources using Roles and ClusterRoles.</p></blockquote><h3 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h3><blockquote><p>A Role is used for namespace-scoped access control.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">my-namespace</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><p>✅ This Role allows a user to view (get, list, watch) Pods only in the namespace my-namespace.</p><p>❌ This does not grant access to other namespaces.</p><h3 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h3><blockquote><p> A ClusterRole is used for cluster-wide permissions or permissions across all namespaces.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><p>✅ This allows viewing (get, list, watch) Pods in all namespaces.</p><h2 id="RoleBinding-amp-ClusterRoleBinding"><a href="#RoleBinding-amp-ClusterRoleBinding" class="headerlink" title="RoleBinding &amp; ClusterRoleBinding"></a>RoleBinding &amp; ClusterRoleBinding</h2><blockquote><p>Who Gets These Permissions?</p></blockquote><h3 id="RoleBinding"><a href="#RoleBinding" class="headerlink" title="RoleBinding"></a>RoleBinding</h3><blockquote><p>A RoleBinding assigns a Role to a specific user, group, or ServiceAccount in a namespace.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">my-namespace</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><p>✅ This binds the pod-reader Role to my-user in the my-namespace namespace.</p><p>❌ It does not grant permissions outside my-namespace.</p><h3 id="ClusterRoleBinding"><a href="#ClusterRoleBinding" class="headerlink" title="ClusterRoleBinding"></a>ClusterRoleBinding</h3><blockquote><p>A ClusterRoleBinding assigns a ClusterRole to a User, Group, or ServiceAccount across all namespaces.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-pod-reader-binding</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><p>✅ This grants my-user access to Pods across all namespaces.</p><h2 id="Subjects"><a href="#Subjects" class="headerlink" title="Subjects"></a>Subjects</h2><blockquote><p>A RoleBinding or ClusterRoleBinding grants permissions to a subject, which can be:</p></blockquote><table><thead><tr><th>Subject Type</th><th>Description</th></tr></thead><tbody><tr><td>User</td><td>A real human user (external identity).</td></tr><tr><td>Group</td><td>A group of users (e.g., dev-team).</td></tr><tr><td>ServiceAccount</td><td>An internal Kubernetes identity for a Pod or Controller.</td></tr></tbody></table><p>Example: Binding a ServiceAccount (my-sa) to a Role</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">my-namespace</span></span><br></pre></td></tr></table></figure><h2 id="API-Groups-amp-Resources"><a href="#API-Groups-amp-Resources" class="headerlink" title="API Groups &amp; Resources"></a>API Groups &amp; Resources</h2><p>Each RBAC rule specifies which resources in which API groups can be accessed.</p><table><thead><tr><th>API Group</th><th>Resources Example</th></tr></thead><tbody><tr><td>“” (core)</td><td>pods, services, nodes</td></tr><tr><td>apps</td><td>deployments, daemonsets</td></tr><tr><td>batch</td><td>jobs, cronjobs</td></tr><tr><td>rbac.authorization.k8s.io</td><td>roles, rolebindings, clusterroles, clusterrolebindings</td></tr></tbody></table><p>Example: Allow Access to Pods, Deployments, and Nodes</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>, <span class="string">&quot;services&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;apps&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;deployments&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>]</span><br></pre></td></tr></table></figure><p>✅ Allows reading Pods and Services (core API group).</p><p>✅ Allows reading Deployments (apps API group).</p><p>✅ Allows reading Nodes (core API group).</p><h1 id="Works-Scenario"><a href="#Works-Scenario" class="headerlink" title="Works Scenario"></a>Works Scenario</h1><h2 id="Scenario-1"><a href="#Scenario-1" class="headerlink" title="Scenario 1"></a>Scenario 1</h2><p>Use Case:</p><ul><li><p>A user (dev-user) needs read-only access to Pods in the dev namespace.</p></li><li><p>The ServiceAccount (cicd-runner-sa) needs to create Deployments in the cicd namespace.</p></li></ul><p>Solution: Define the RBAC Rules</p><ol><li>Create a Role for Read-Only Pod Access in dev Namespace</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><ol start="2"><li>Bind the Role to dev-user in dev Namespace</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dev-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><ol start="3"><li>Create a Role for cicd-runner-sa to Deploy Apps in cicd Namespace</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-creator</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cicd</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;apps&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;deployments&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br></pre></td></tr></table></figure><ol start="4"><li>Bind the Role to the cicd-runner-sa ServiceAccount</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cicd-runner-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cicd</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cicd-runner-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cicd</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-creator</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><h2 id="Scenario-2"><a href="#Scenario-2" class="headerlink" title="Scenario 2"></a>Scenario 2</h2><p>Scenario: Grant Read-Only Access to Pods Across Multiple Namespaces</p><p>Use Case:</p><ul><li><p>A developer team needs read-only access to Pods in multiple namespaces (dev, staging, prod).</p></li><li><p>Instead of creating separate Roles in each namespace, we use one ClusterRole.</p></li><li><p>We then create RoleBindings in each namespace to assign the ClusterRole.</p></li></ul><ol><li>Create a ClusterRole to Read Pods Across Namespaces</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">multi-namespace-pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><ol start="2"><li>Bind the ClusterRole to a User for Multiple Namespaces</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">multi-namespace-pod-reader-binding</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">developer-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">multi-namespace-pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><h1 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h1><p>Key Takeaways from Kubernetes RBAC Architecture</p><ol><li><p>Role / ClusterRole → Defines permissions (what actions are allowed?).</p></li><li><p>RoleBinding / ClusterRoleBinding → Assigns permissions (who gets access?).</p></li><li><p>Subjects → Users, Groups, ServiceAccounts (who is authorized?).</p></li><li><p>API Groups → Determines resources that can be controlled.</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;RBAC (Role-Based Access Control) in Kubernetes is a security model that controls who can access and perform actions on resou</summary>
      
    
    
    
    <category term="Kubernetes" scheme="https://www.willshirley.top/categories/Kubernetes/"/>
    
    
    <category term="RBAC" scheme="https://www.willshirley.top/tags/RBAC/"/>
    
  </entry>
  
  <entry>
    <title>podman snippet</title>
    <link href="https://www.willshirley.top/2025/02/10/podman%20snippet/"/>
    <id>https://www.willshirley.top/2025/02/10/podman%20snippet/</id>
    <published>2025-02-10T08:06:03.000Z</published>
    <updated>2025-03-13T08:16:52.173Z</updated>
    
    <content type="html"><![CDATA[<ul><li>不同用户下执行 <code>podman ps</code>，只能查看当前用户的运行容器（即使是root用户，也不能查看其他普通用户启用的容器信息）</li></ul><h1 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h1><h2 id="change-default-data-dir"><a href="#change-default-data-dir" class="headerlink" title="change default data dir"></a>change default data dir</h2><ul><li><p><strong>rootful mode</strong> </p><blockquote><p>Default graphroot: /var/lib/containers/storage.</p></blockquote></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> check</span></span><br><span class="line">podman info | grep &#x27;GraphRoot&#x27;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Edit Podman’s Storage Configuration</span></span><br><span class="line">sudo mkdir -p /etc/containers</span><br><span class="line">sudo vim /etc/containers/storage.conf </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> edit storage.conf</span></span><br><span class="line">[storage]</span><br><span class="line">driver = &quot;overlay&quot;</span><br><span class="line">graphroot = &quot;//mnt/podman-data&quot;</span><br></pre></td></tr></table></figure><ul><li><p><strong>rootless mode</strong></p><blockquote><p>Default graphroot: ~/.local/share/containers/storage.</p></blockquote></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> check</span></span><br><span class="line">podman info | grep &#x27;GraphRoot&#x27;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Edit Podman’s Storage Configuration</span></span><br><span class="line">sudo mkdir -p ~/.config/containers</span><br><span class="line">sudo vim ~/.config/containers/storage.conf  </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> edit storge.conf</span></span><br><span class="line">[storage]</span><br><span class="line">driver = &quot;overlay&quot;</span><br><span class="line">graphroot = &quot;/mnt/podman-data&quot;</span><br></pre></td></tr></table></figure><h2 id="potentially-insufficient-UIDs-or-GIDs-available-in-user-namespace"><a href="#potentially-insufficient-UIDs-or-GIDs-available-in-user-namespace" class="headerlink" title="potentially insufficient UIDs or GIDs available in user namespace"></a>potentially insufficient UIDs or GIDs available in user namespace</h2><blockquote><p> If the requested UID/GID still falls outside or Podman needs more mappings, you can edit /etc/subuid and /etc/subgid (as root) to increase the range</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Increase UID/GID Range (Optional):</span></span><br><span class="line">sudo vim /etc/subuid  # dingofs:60000:131072</span><br><span class="line">sudo vim /etc/subgid  # dingofs:60000:131072</span><br><span class="line"></span><br><span class="line">podman system migrate</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;不同用户下执行 &lt;code&gt;podman ps&lt;/code&gt;，只能查看当前用户的运行容器（即使是root用户，也不能查看其他普通用户启用的容器信息）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;solution&quot;&gt;&lt;a href=&quot;#solution&quot; class=&quot;</summary>
      
    
    
    
    <category term="podman" scheme="https://www.willshirley.top/categories/podman/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>ceph rados</title>
    <link href="https://www.willshirley.top/2025/02/08/ceph%20rados/"/>
    <id>https://www.willshirley.top/2025/02/08/ceph%20rados/</id>
    <published>2025-02-08T03:40:20.000Z</published>
    <updated>2025-02-08T10:12:34.786Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Storage-Cluster">Ceph Storage Cluster</a> provides the basic storage service that allows <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph">Ceph</a> to uniquely deliver <strong>object, block, and file storage</strong> in one unified system. However, you are not limited to using the RESTful, block, or POSIX interfaces. Based upon RADOS, the <code>librados</code> API enables you to create your own interface to the Ceph Storage Cluster.</p><h1 id="command"><a href="#command" class="headerlink" title="command"></a>command</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> list objects by specify pool</span></span><br><span class="line">rados -p &lt;pool-name&gt; ls</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">read</span> object</span></span><br><span class="line">rados -p &lt;pool-name&gt; get &lt;object-name&gt; output.txt</span><br><span class="line">cat output.txt</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check xattr value</span></span><br><span class="line">rados -p &lt;pool-name&gt; getxattr &lt;object-name&gt; &lt;xattr-key&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The &lt;a href=&quot;https://docs.ceph.com/en/reef/glossary/#term-Ceph-Storage-Cluster&quot;&gt;Ceph Storage Cluster&lt;/a&gt; provides the basic storage servi</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="storage" scheme="https://www.willshirley.top/tags/storage/"/>
    
  </entry>
  
  <entry>
    <title>ceph deploy</title>
    <link href="https://www.willshirley.top/2025/02/07/ceph%20deploy/"/>
    <id>https://www.willshirley.top/2025/02/07/ceph%20deploy/</id>
    <published>2025-02-07T09:40:41.000Z</published>
    <updated>2025-02-08T08:09:19.197Z</updated>
    
    <content type="html"><![CDATA[<h1 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h1><h2 id="install-cephadm"><a href="#install-cephadm" class="headerlink" title="install cephadm"></a>install cephadm</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dnf search release-ceph</span><br><span class="line">dnf install --assumeyes centos-release-ceph-reef</span><br><span class="line">dnf install --assumeyes cephadm</span><br></pre></td></tr></table></figure><ul><li><p>enable ceph cli</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cephadm add-repo --release reef</span><br><span class="line">cephadm install ceph-common</span><br></pre></td></tr></table></figure></li></ul><h2 id="booststrap"><a href="#booststrap" class="headerlink" title="booststrap"></a>booststrap</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cephadm bootstrap --mon-ip 172.20.7.232</span><br></pre></td></tr></table></figure><ul><li>log<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Ceph Dashboard is now available at:</span><br><span class="line">        URL: https://dingo7232.com:8443/</span><br><span class="line">        User: admin</span><br><span class="line">    Password: &lt;password&gt;</span><br><span class="line">Enabling client.admin keyring and conf on hosts with &quot;admin&quot; label</span><br><span class="line">Saving cluster configuration to /var/lib/ceph/6a65c746-e532-11ef-8ac2-fa7c097efb00/config directory</span><br><span class="line">Enabling autotune for osd_memory_target</span><br><span class="line">You can access the Ceph CLI as following in case of multi-cluster or non-default config:</span><br><span class="line"></span><br><span class="line">        sudo /sbin/cephadm shell --fsid 6a65c746-e532-11ef-8ac2-fa7c097efb00 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line">Or, if you are only running a single cluster on this host:</span><br><span class="line"></span><br><span class="line">        sudo /sbin/cephadm shell</span><br><span class="line"></span><br><span class="line">Please consider enabling telemetry to help improve Ceph:</span><br><span class="line"></span><br><span class="line">        ceph telemetry on</span><br><span class="line"></span><br><span class="line">For more information see:</span><br><span class="line"></span><br><span class="line">        https://docs.ceph.com/en/latest/mgr/telemetry/</span><br><span class="line"></span><br><span class="line">Bootstrap complete.</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h2 id="add-hosts"><a href="#add-hosts" class="headerlink" title="add hosts"></a>add hosts</h2><ul><li><p>Install the cluster’s public SSH key in the new host’s root user’s authorized_keys file:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -f -i /etc/ceph/ceph.pub root@dingo7233</span><br><span class="line">ssh-copy-id -f -i /etc/ceph/ceph.pub root@dingo7234</span><br></pre></td></tr></table></figure></li><li><p>Tell Ceph that the new node is part of the cluster</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph orch host add *&lt;newhost&gt;* [*&lt;ip&gt;*] [*&lt;label1&gt; ...*]</span></span><br><span class="line">ceph orch host add dingo7233 172.20.7.233</span><br><span class="line">ceph orch host add dingo7234 172.20.7.234</span><br><span class="line">or</span><br><span class="line">ceph orch host add dingo7233 172.20.7.233 --labels _admin</span><br><span class="line">ceph orch host add dingo7234 172.20.7.234 --labels _admin</span><br></pre></td></tr></table></figure></li><li><p>add label (optional)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph orch host label add dingo7233 _admin</span><br><span class="line">ceph orch host label add dingo7234 _admin</span><br></pre></td></tr></table></figure></li><li><p>list hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch host ls --detail</span><br></pre></td></tr></table></figure></li></ul><h2 id="add-storage"><a href="#add-storage" class="headerlink" title="add storage"></a>add storage</h2><ul><li><p>check available devices</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch device ls</span><br></pre></td></tr></table></figure></li><li><p>apply osd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch apply osd --all-available-devices</span><br></pre></td></tr></table></figure></li></ul><h1 id="check"><a href="#check" class="headerlink" title="check"></a>check</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph status</span></span><br><span class="line">cluster:</span><br><span class="line">    id:     6a65c746-e532-11ef-8ac2-fa7c097efb00</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            1 mgr modules have recently crashed</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum dingo7232,dingo7234,dingo7233 (age 16m)</span><br><span class="line">    mgr: dingo7232.znvodw(active, since 24m), standbys: dingo7234.yenqrt</span><br><span class="line">    osd: 3 osds: 3 up (since 15m), 3 in (since 15m)</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 2 objects, 449 KiB</span><br><span class="line">    usage:   81 MiB used, 600 GiB / 600 GiB avail</span><br><span class="line">    pgs:     1 active+clean</span><br></pre></td></tr></table></figure><h1 id="thouble-shooting"><a href="#thouble-shooting" class="headerlink" title="thouble shooting"></a>thouble shooting</h1><h2 id="redeploy-cluster"><a href="#redeploy-cluster" class="headerlink" title="redeploy cluster"></a>redeploy cluster</h2><p>To remove an existing Ceph cluster deployed using <code>cephadm</code> and redeploy a new one, follow these steps:</p><ul><li>Step 1: Stop All Ceph Services</li></ul><p>First, stop all Ceph services on each host in the cluster.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop ceph.target</span><br></pre></td></tr></table></figure><ul><li>Step 2: Remove Ceph Configuration and Data</li></ul><p>Remove the Ceph configuration and data directories.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /etc/ceph</span><br><span class="line">sudo rm -rf /var/lib/ceph</span><br><span class="line">sudo rm -rf /var/<span class="built_in">log</span>/ceph</span><br></pre></td></tr></table></figure><ul><li><p>Step 3: deploy as below words</p></li><li><p>Step 4: Verify Cluster Health</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph -s</span><br></pre></td></tr></table></figure></li></ul><p>If you encounter any issues during the redeployment, check the logs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo journalctl -u ceph-* -f</span><br></pre></td></tr></table></figure><p>Or check the Ceph logs directly:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo less /var/<span class="built_in">log</span>/ceph/ceph.log</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;deploy&quot;&gt;&lt;a href=&quot;#deploy&quot; class=&quot;headerlink&quot; title=&quot;deploy&quot;&gt;&lt;/a&gt;deploy&lt;/h1&gt;&lt;h2 id=&quot;install-cephadm&quot;&gt;&lt;a href=&quot;#install-cephadm&quot; class</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="deploy" scheme="https://www.willshirley.top/tags/deploy/"/>
    
  </entry>
  
  <entry>
    <title>linux A vs B</title>
    <link href="https://www.willshirley.top/2025/01/22/linux%20A%20vs%20B/"/>
    <id>https://www.willshirley.top/2025/01/22/linux%20A%20vs%20B/</id>
    <published>2025-01-22T06:11:11.000Z</published>
    <updated>2025-01-22T06:20:04.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Systemd-limits-vs-PAM-limits"><a href="#Systemd-limits-vs-PAM-limits" class="headerlink" title="Systemd limits vs PAM limits"></a>Systemd limits vs PAM limits</h1><p><strong>Systemd limits</strong> and <strong>PAM limits</strong> are two different ways of configuring resource limits in Linux. They control aspects like <strong>open file limits (<strong>nofile</strong>)</strong>, <strong>memory</strong>, and <strong>CPU usage</strong>, but they work at different levels of the system.</p><h2 id="PAM-Limits"><a href="#PAM-Limits" class="headerlink" title="PAM Limits"></a>PAM Limits</h2><p>PAM (Pluggable Authentication Module) sets limits when <strong>a user logs in</strong> via <strong>SSH, TTY, or GUI login</strong>. It does <strong>not</strong> apply to system services. </p><p><strong>Configuration:</strong></p><ul><li><p>/etc/security/limits.conf</p></li><li><p>/etc/security/limits.d/*.conf</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Example (/etc/security/limits.conf or /etc/security/limits.d/custom.conf):**</span></span><br><span class="line">dongw soft nofile 65535</span><br><span class="line">dongw hard nofile 65535</span><br></pre></td></tr></table></figure><p><strong>Limitations of PAM:</strong></p><ul><li><p>Only affects interactive logins (TTY, SSH, GUI).</p></li><li><p>Does <strong>not</strong> apply to systemd-managed services.</p></li></ul><h2 id="Systemd-Limits"><a href="#Systemd-Limits" class="headerlink" title="Systemd Limits"></a>Systemd Limits</h2><p>Systemd sets limits <strong>for system services and user sessions</strong>. It applies to <strong>both login sessions and system services</strong>, making it more powerful than PAM.</p><p><strong>Configuration:</strong></p><ul><li><p><strong>Global (for all services and users)</strong>:</p></li><li><p>/etc/systemd/system.conf (system-wide limits)</p></li><li><p>/etc/systemd/user.conf (per-user session limits)</p></li><li><p><strong>Per-service limits</strong> (for specific services):</p></li><li><p>/etc/systemd/system/<service>.service</p></li></ul><h3 id="Global-Limits"><a href="#Global-Limits" class="headerlink" title="Global Limits"></a>Global Limits</h3><p>Applies to <strong>all</strong> processes managed by systemd. (/etc/systemd/system.conf or /etc/systemd/user.conf)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DefaultLimitNOFILE=65535</span><br></pre></td></tr></table></figure><h3 id="Per-Service-Limits"><a href="#Per-Service-Limits" class="headerlink" title="Per-Service Limits"></a>Per-Service Limits</h3><p> This applies to only on specify service. (e.g. /etc/systemd/system/myservice.service)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">LimitNOFILE=65535</span><br></pre></td></tr></table></figure><p><strong>Advantages of Systemd Limits:</strong></p><ul><li><p>Affects <strong>both user logins and system services</strong>.</p></li><li><p>Ensures limits persist across <strong>system reboots</strong>.</p></li><li><p>More reliable for applications like Ceph, Docker, and databases.</p></li></ul><h2 id="Comparison-Table"><a href="#Comparison-Table" class="headerlink" title="Comparison Table"></a><strong>Comparison Table</strong></h2><table><thead><tr><th align="left"><strong>Feature</strong></th><th><strong>PAM Limits (<code>limits.conf</code>)</strong></th><th><strong>Systemd Limits (<code>system.conf</code>/<code>.service</code>)</strong></th></tr></thead><tbody><tr><td align="left">Applies to interactive users</td><td>✅ Yes (SSH, TTY, GUI)</td><td>✅ Yes</td></tr><tr><td align="left">Applies to system services</td><td>❌ No</td><td>✅ Yes</td></tr><tr><td align="left">Persists across reboots</td><td>✅ Yes</td><td>✅ Yes</td></tr><tr><td align="left">Easy to configure</td><td>✅ Yes</td><td>✅ Yes</td></tr><tr><td align="left">Best for tuning servers</td><td>⚠️  Partial</td><td>✅ Yes</td></tr></tbody></table><ul><li><p><strong>For interactive user sessions</strong> → Use <strong>PAM (<strong>limits.conf</strong>)</strong>.</p></li><li><p><strong>For system services and all users</strong> → Use <strong>Systemd (<strong>system.conf</strong>,</strong> .service**)**.</p></li><li><p><strong>For the best persistence and reliability</strong>, configure <strong>both</strong>.</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Systemd-limits-vs-PAM-limits&quot;&gt;&lt;a href=&quot;#Systemd-limits-vs-PAM-limits&quot; class=&quot;headerlink&quot; title=&quot;Systemd limits vs PAM limits&quot;&gt;&lt;/a&gt;Sy</summary>
      
    
    
    
    <category term="linux" scheme="https://www.willshirley.top/categories/linux/"/>
    
    
    <category term="A vs B" scheme="https://www.willshirley.top/tags/A-vs-B/"/>
    
  </entry>
  
  <entry>
    <title>linux tune</title>
    <link href="https://www.willshirley.top/2025/01/21/linux%20tune/"/>
    <id>https://www.willshirley.top/2025/01/21/linux%20tune/</id>
    <published>2025-01-21T02:43:04.000Z</published>
    <updated>2025-02-24T06:50:54.511Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cpu"><a href="#cpu" class="headerlink" title="cpu"></a>cpu</h1><h2 id="governor-strategy"><a href="#governor-strategy" class="headerlink" title="governor strategy"></a>governor strategy</h2><ul><li><p>Check Available CPU Governors</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run the following <span class="built_in">command</span> to check available CPU governors:</span></span><br><span class="line">cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You should see something like below info:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> conservative ondemand userspace powersave performance schedutil</span></span><br></pre></td></tr></table></figure></li><li><p>Check Current CPU Governor</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</span><br><span class="line">or</span><br><span class="line">cpupower frequency-info --policy # should install cpupower command by &#x27;sudo dnf install kernel-tools -y&#x27;</span><br></pre></td></tr></table></figure></li><li><p>Set CPU Governor to Performance temporarily</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To temporarily <span class="built_in">set</span> the CPU governor to performance (until reboot):</span></span><br><span class="line">for cpu in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do</span><br><span class="line">  echo performance | sudo tee $cpu</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>Set CPU Governor Persistent</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> First, install cpupower:</span></span><br><span class="line">sudo dnf install kernel-tools -y</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Then, <span class="built_in">set</span> the governor:</span></span><br><span class="line">sudo cpupower frequency-set -g performance</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> To apply this setting at boot, <span class="built_in">enable</span> the service:</span></span><br><span class="line">sudo systemctl enable --now cpupower.service</span><br></pre></td></tr></table></figure></li></ul><h1 id="nvme"><a href="#nvme" class="headerlink" title="nvme"></a>nvme</h1><h2 id="I-O-scheduler"><a href="#I-O-scheduler" class="headerlink" title="I/O scheduler"></a>I/O scheduler</h2><ul><li><p>check current scheduler pattern</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/block/nvme[01]\*/queue/scheduler</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> console <span class="built_in">print</span> below info</span></span><br><span class="line">none [mq-deadline] kyber bfq</span><br></pre></td></tr></table></figure><p>The <strong>scheduler currently in use</strong> is indicated by the square brackets [ ].</p><p><strong>Common NVMe I/O Schedulers</strong></p><ol><li><p><strong>none</strong> – Default for NVMe, provides minimal latency.</p></li><li><p><strong>mq-deadline</strong> – Multi-queue deadline scheduler, balances fairness and performance.</p></li><li><p><strong>kyber</strong> – Optimized for high-performance SSDs.</p></li><li><p><strong>bfq</strong> – Budget Fair Queueing, useful for low-latency workloads.</p></li></ol></li><li><p>Change the I/O Scheduler</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To <span class="built_in">set</span> the none scheduler <span class="keyword">for</span> nvme0n1, use:</span></span><br><span class="line">echo none | sudo tee /sys/block/nvme0n1/queue/scheduler</span><br></pre></td></tr></table></figure><p><strong>When to Change the Scheduler?</strong></p><ul><li><p>Use none <strong>(default)</strong> if latency is the priority (most NVMe drives handle queuing internally).</p></li><li><p>Use mq-deadline if fairness in I/O operations is needed.</p></li><li><p>Use kyber for workloads requiring fast response time.</p></li><li><p>Use bfq for interactive desktop workloads.</p></li></ul></li></ul><h1 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h1><h2 id="open-file-size"><a href="#open-file-size" class="headerlink" title="open file size"></a>open file size</h2><blockquote><p>To configure the <strong>maximum open file size</strong> (ulimit) for a specific user </p></blockquote><ul><li><p>Check Current Limits</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To see the current limits <span class="keyword">for</span> a user:</span></span><br><span class="line">ulimit -n # Check open file limit</span><br><span class="line">ulimit -a # Check all limits</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> To check system-wide limits:</span></span><br><span class="line">cat /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure></li><li><p>Set Open File Limits for a Specific User</p><blockquote><p>The changes in limits.conf are only applied to new login sessions</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/security/limits.conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the following lines (replace username with the actual user):</span></span><br><span class="line">username soft nofile 65535</span><br><span class="line">username hard nofile 65535</span><br></pre></td></tr></table></figure></li></ul><p>​    <strong>soft</strong>: The default limit the user gets.</p><p>​    <strong>hard</strong>: The maximum limit a user can set.</p><table><thead><tr><th>Feature</th><th>/etc/security/limits.conf</th><th>/etc/security/limits.d/*.conf</th></tr></thead><tbody><tr><td><strong>Default File</strong></td><td>Yes</td><td>No (Custom files in limits.d directory)</td></tr><tr><td><strong>Priority</strong></td><td>Read first</td><td>Read after limits.conf</td></tr><tr><td><strong>Organization</strong></td><td>All rules in one file</td><td>Modular approach, one file per service or user</td></tr><tr><td><strong>Example</strong></td><td><code>* soft nofile 65536</code></td><td><code>dongw soft nofile 65536</code> (in <code>/etc/security/limits.d/dongw.conf</code>)</td></tr><tr><td><strong>Syntax</strong></td><td>Same syntax for both files</td><td>Same syntax as limits.conf</td></tr></tbody></table><h1 id="HP-and-THP"><a href="#HP-and-THP" class="headerlink" title="HP and THP"></a>HP and THP</h1><blockquote><p>Huge Pages (HP) and Transparent Huge Pages (THP)</p></blockquote><p>Understanding Huge Pages (HP) and Transparent Huge Pages (THP)</p><p><strong>Huge Pages (HP)</strong>: A manually configured fixed-size memory allocation system, beneficial for workloads that require large contiguous memory allocations.</p><p><strong>Transparent Huge Pages (THP)</strong>: An automated memory management feature that dynamically allocates large memory pages based on usage patterns.</p><p>for performance-sensitive applications, THP can sometimes cause performance issues due to fragmentation and unexpected latency spikes. Thus, it’s often recommended to <strong>disable THP</strong> and manually configure HP.</p><ul><li><p>Checking Current Huge Pages Configuration</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/meminfo | grep HugePages</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Example output:</span></span><br><span class="line">AnonHugePages:  16850944 kB # Memory allocated via THP.</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">HugePages_Total:       0 # Number of configured huge pages.</span><br><span class="line">HugePages_Free:        0 # Available huge pages.</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br></pre></td></tr></table></figure></li><li><p>Configuring Static Huge Pages</p><p><strong>Step 1: Set the Number of Huge Pages</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To allocate a specific number of 2MB huge pages, calculate based on your memory requirements. Example:</span></span><br><span class="line">echo 1024 | sudo tee /proc/sys/vm/nr_hugepages</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Make it persistent:</span></span><br><span class="line">echo &quot;vm.nr_hugepages=1024&quot; | sudo tee -a /etc/sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p><strong>Step 2: Allocate Huge Pages at Boot (Recommended)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/default/grub</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add hugepages kernel parameter:</span></span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;default_hugepagesz=2M hugepagesz=2M hugepages=1024&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Update GRUB:</span></span><br><span class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Reboot the system:</span></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p><strong>Step 3: Mount Huge Pages File System (Optional)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To <span class="built_in">enable</span> shared access to huge pages:</span></span><br><span class="line">mkdir -p /mnt/huge</span><br><span class="line">mount -t hugetlbfs nodev /mnt/huge</span><br><span class="line"><span class="meta">#</span><span class="bash"> To make this persistent, add to /etc/fstab:</span></span><br><span class="line">nodev /mnt/huge hugetlbfs defaults 0 0</span><br></pre></td></tr></table></figure></li><li><p>Disabling Transparent Huge Pages (THP)</p><p>THP should be <strong>disabled</strong> for performance-sensitive applications</p><p><strong>Step 1: Disable THP at Runtime</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">echo never | sudo tee /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure><p><strong>Step 2: Make THP Disabled at Boot</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/default/grub</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add:</span></span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;transparent_hugepage=never&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Update GRUB:</span></span><br><span class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Reboot:</span></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></li><li><p>Verification After Reboot</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Check Huge Pages Allocation</span></span><br><span class="line">cat /proc/meminfo | grep HugePages</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check THP Status</span></span><br><span class="line">cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> Expected output:</span></span><br><span class="line">[always] madvise never</span><br></pre></td></tr></table></figure></li></ul><p>Summary: Should You Configure HP &amp; THP Together?</p><p>For performance-sensitive applications, it’s recommended to <strong>disable THP</strong> and <strong>manually configure Huge Pages (HP)</strong> for better performance.</p><p>THP can cause latency spikes and memory fragmentation, making it less ideal for high-performance file systems.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;cpu&quot;&gt;&lt;a href=&quot;#cpu&quot; class=&quot;headerlink&quot; title=&quot;cpu&quot;&gt;&lt;/a&gt;cpu&lt;/h1&gt;&lt;h2 id=&quot;governor-strategy&quot;&gt;&lt;a href=&quot;#governor-strategy&quot; class=&quot;header</summary>
      
    
    
    
    <category term="linux" scheme="https://www.willshirley.top/categories/linux/"/>
    
    
    <category term="tune" scheme="https://www.willshirley.top/tags/tune/"/>
    
  </entry>
  
  <entry>
    <title>nvme</title>
    <link href="https://www.willshirley.top/2025/01/17/fs:%20nvme/"/>
    <id>https://www.willshirley.top/2025/01/17/fs:%20nvme/</id>
    <published>2025-01-17T04:21:01.000Z</published>
    <updated>2025-01-17T04:34:14.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NVMe"><a href="#NVMe" class="headerlink" title="NVMe"></a>NVMe</h1><p><strong>NVMe (Non-Volatile Memory Express)</strong> is a high-performance storage protocol designed to take full advantage of the speed and efficiency offered by modern solid-state drives (SSDs), particularly those using <strong>NAND flash</strong> or <strong>3D NAND</strong> memory. It was developed to overcome the limitations of older storage protocols (such as SATA and SAS) by optimizing the interaction between the storage device and the CPU over faster interfaces like <strong>PCIe (Peripheral Component Interconnect Express)</strong>.</p><h2 id="1-What-NVMe-is"><a href="#1-What-NVMe-is" class="headerlink" title="1. What NVMe is"></a>1. What NVMe is</h2><p>NVMe is both a <strong>protocol</strong> and a <strong>standard</strong> for connecting <strong>non-volatile memory</strong> (such as NAND flash memory) directly to a computer system via high-speed interfaces, most commonly <strong>PCIe</strong>. It allows data to be transferred at much faster speeds compared to older storage protocols, enabling devices to reach their full potential and significantly reducing latency.</p><h2 id="2-Key-Advantages-of-NVMe"><a href="#2-Key-Advantages-of-NVMe" class="headerlink" title="2. Key Advantages of NVMe"></a>2. Key Advantages of NVMe</h2><ul><li><p><strong>Faster Speeds</strong>: NVMe takes full advantage of the high throughput and low latency of <strong>PCIe lanes</strong>. For example, the PCIe Gen 3.0 standard provides a maximum theoretical speed of about <strong>32 GB/s</strong> (using 16 lanes), while PCIe Gen 4.0 can offer speeds up to <strong>64 GB/s</strong>. NVMe drives can use multiple PCIe lanes (usually 4 or more) to transfer data simultaneously, making them vastly faster than older SATA SSDs or hard disk drives (HDDs).</p></li><li><p><strong>Low Latency</strong>: NVMe’s protocol is optimized to handle thousands of input/output operations per second (IOPS), resulting in very low latency (often measured in microseconds). This is crucial for applications requiring rapid access to large datasets, such as databases or high-performance computing tasks.</p></li><li><p><strong>Parallelism</strong>: NVMe is designed to support <strong>multi-core processors</strong> and can handle many more <strong>queues</strong> and <strong>commands</strong> simultaneously than previous protocols. It can support <strong>64K queues</strong>, each with up to <strong>64K commands</strong>, which is a huge improvement over the older <strong>SATA</strong> interface, which could handle only one queue with a limited number of commands.</p></li><li><p><strong>Efficiency</strong>: NVMe uses a streamlined protocol that minimizes overhead. SATA and SAS, by comparison, were originally designed for spinning hard drives and include more layers of abstraction that slow down communication.</p></li></ul><h2 id="3-NVMe-vs-SATA-SAS"><a href="#3-NVMe-vs-SATA-SAS" class="headerlink" title="3. NVMe vs. SATA/SAS"></a>3. NVMe vs. SATA/SAS</h2><ul><li><p><strong>SATA</strong> (Serial Advanced Technology Attachment) is an older protocol used mainly for connecting hard drives and SSDs. It’s limited by the speed of the interface itself (roughly <strong>600 MB/s</strong> max in the case of SATA III) and was designed when mechanical HDDs were the primary storage medium. Even with SSDs, the SATA interface is a bottleneck.</p></li><li><p><strong>SAS</strong> (Serial Attached SCSI) is often used in enterprise environments for high-reliability storage solutions, but it, too, has limitations in speed and efficiency compared to NVMe.</p></li></ul><h2 id="4-How-NVMe-Works"><a href="#4-How-NVMe-Works" class="headerlink" title="4. How NVMe Works"></a>4. How NVMe Works</h2><p>NVMe devices communicate directly with the CPU via PCIe, bypassing older storage controllers. The <strong>PCIe interface</strong> connects storage devices such as <strong>NVMe SSDs</strong> directly to the motherboard or via expansion cards, allowing faster and more direct data transfer.</p><p>NVMe itself is built around a <strong>command set</strong> that is designed for <strong>flash memory</strong>, as opposed to the traditional command sets used in older storage systems. This reduces unnecessary overhead and allows NVMe devices to process requests more quickly.</p><h2 id="5-NVMe-Form-Factors"><a href="#5-NVMe-Form-Factors" class="headerlink" title="5. NVMe Form Factors"></a>5. NVMe Form Factors</h2><p>There are several physical form factors for NVMe drives, including:</p><ul><li><p><strong>M.2</strong>: A small form factor typically used in laptops and desktops. It plugs directly into an M.2 slot on the motherboard, and modern M.2 NVMe drives can reach speeds of over <strong>7,000 MB/s</strong> with PCIe Gen 3.0 or Gen 4.0 support.</p></li><li><p><strong>U.2</strong>: A connector used for enterprise-class SSDs. These drives are often used in servers and data centers.</p></li><li><p><strong>Add-in Card (AIC)</strong>: These are full-sized PCIe cards that can be inserted into a motherboard’s PCIe slot. They offer high storage capacity and performance, making them ideal for high-end workstations or servers.</p></li></ul><h2 id="6-Applications-of-NVMe"><a href="#6-Applications-of-NVMe" class="headerlink" title="6. Applications of NVMe"></a>6. Applications of NVMe</h2><ul><li><p><strong>Gaming</strong>: NVMe SSDs dramatically reduce game load times, texture rendering, and video streaming, providing a smoother gaming experience.</p></li><li><p><strong>Content Creation</strong>: Video editing, 3D rendering, and other high-bandwidth tasks benefit from the rapid access to large files.</p></li><li><p><strong>Data Centers</strong>: NVMe drives are ideal for enterprise storage solutions, especially when handling high-volume data throughput, real-time analytics, or AI workloads.</p></li><li><p><strong>Databases</strong>: NVMe’s low latency and high throughput make it an excellent choice for applications involving large databases and data warehousing.</p></li></ul><h2 id="7-NVMe-and-PCIe-Versions"><a href="#7-NVMe-and-PCIe-Versions" class="headerlink" title="7. NVMe and PCIe Versions"></a>7. NVMe and PCIe Versions</h2><p>NVMe’s performance scales with <strong>PCIe</strong> versions:</p><ul><li><p><strong>PCIe 3.0</strong> supports a maximum theoretical bandwidth of <strong>1 GB/s per lane</strong> (total of <strong>4 GB/s</strong> for a 4-lane SSD).</p></li><li><p><strong>PCIe 4.0</strong> doubles this bandwidth to <strong>2 GB/s per lane</strong> (total of <strong>8 GB/s</strong> for a 4-lane SSD).</p></li><li><p><strong>PCIe 5.0</strong> further increases the bandwidth to <strong>4 GB/s per lane</strong> (total of <strong>16 GB/s</strong> for a 4-lane SSD).</p></li></ul><h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8. Conclusion"></a>8. Conclusion</h2><p>NVMe is revolutionizing storage by offering higher speeds, lower latency, and better scalability compared to older protocols. Whether for consumer laptops, high-performance desktops, or enterprise-level data centers, NVMe is fast becoming the go-to solution for anyone who demands top-tier storage performance.</p><h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><p>The relationship between <strong>NVMe</strong> and <strong>SSD</strong> is one of complementary technologies. Here’s a breakdown of how the two are related and how they differ:</p><h2 id="1-What-is-an-SSD"><a href="#1-What-is-an-SSD" class="headerlink" title="1. What is an SSD?"></a>1. What is an SSD?</h2><p><strong>SSD (Solid-State Drive)</strong> refers to a type of storage device that uses <strong>non-volatile flash memory</strong> (usually NAND flash) to store data, rather than the spinning platters and moving heads of traditional <strong>HDDs (Hard Disk Drives)</strong>. SSDs are much faster, more reliable, and use less power than HDDs, making them a popular choice for both consumer and enterprise storage solutions.</p><p><strong>SSDs can use various interfaces and protocols</strong> to communicate with the computer system. These include:</p><ul><li><p><strong>SATA (Serial ATA)</strong>: An older protocol, often used for <strong>SATA-based SSDs</strong>. While fast compared to HDDs, it’s limited by the bandwidth of the SATA interface (roughly <strong>600 MB/s</strong>).</p></li><li><p><strong>SAS (Serial Attached SCSI)</strong>: A more advanced, enterprise-grade protocol used for connecting SSDs in servers.</p></li><li><p><strong>PCIe (Peripheral Component Interconnect Express)</strong>: A high-speed interface used by <strong>NVMe SSDs</strong>.</p></li></ul><h2 id="2-How-NVMe-and-SSDs-are-Related"><a href="#2-How-NVMe-and-SSDs-are-Related" class="headerlink" title="2. How NVMe and SSDs are Related"></a>2. How NVMe and SSDs are Related</h2><p>NVMe and SSDs are connected in the following way:</p><ul><li><p><strong>NVMe is a protocol for accessing and transferring data from an SSD</strong>. It is the language or set of instructions that defines how data moves between the SSD and the computer’s CPU.</p></li><li><p><strong>SSD is the storage device</strong>, and it can use different interfaces and protocols (SATA, SAS, or PCIe). <strong>NVMe SSDs</strong> are SSDs that use the <strong>NVMe protocol over the PCIe interface</strong> to provide high-speed data transfer.</p></li></ul><h2 id="3-Key-Differences-Between-NVMe-SSDs-and-Other-SSDs"><a href="#3-Key-Differences-Between-NVMe-SSDs-and-Other-SSDs" class="headerlink" title="3. Key Differences Between NVMe SSDs and Other SSDs"></a>3. Key Differences Between NVMe SSDs and Other SSDs</h2><p>The main difference between <strong>NVMe SSDs</strong> and <strong>non-NVMe SSDs</strong> (like <strong>SATA SSDs</strong>) lies in the <strong>interface</strong> and <strong>protocol</strong> they use for data transfer:</p><ul><li><p><strong>NVMe SSDs</strong>:</p><ul><li><p><strong>Interface</strong>: Use <strong>PCIe</strong> lanes to communicate with the system.</p></li><li><p><strong>Protocol</strong>: Use the <strong>NVMe protocol</strong> to provide faster, more efficient data transfers.</p></li><li><p><strong>Speed</strong>: Much faster than SATA-based SSDs, as PCIe supports higher throughput.</p></li><li><p><strong>Latency</strong>: Lower latency compared to SATA SSDs, as NVMe is optimized for flash memory and can handle many more I/O operations in parallel.</p></li><li><p><strong>Form Factors</strong>: NVMe SSDs typically come in <strong>M.2</strong>, <strong>U.2</strong>, or <strong>Add-in Card</strong> (AIC) form factors.</p></li></ul></li><li><p><strong>Non-NVMe SSDs</strong> (e.g., <strong>SATA SSDs</strong>):</p><ul><li><p><strong>Interface</strong>: Use the <strong>SATA</strong> interface, which was originally designed for spinning hard drives.</p></li><li><p><strong>Protocol</strong>: Use the older <strong>SATA protocol</strong>, which is slower than NVMe.</p></li><li><p><strong>Speed</strong>: Limited to <strong>around 600 MB/s</strong> (the maximum bandwidth of SATA III).</p></li><li><p><strong>Latency</strong>: Higher latency compared to NVMe SSDs because of the older protocol and interface.</p></li><li><p><strong>Form Factor</strong>: Commonly come in <strong>2.5-inch</strong> or <strong>mSATA</strong> form factors, similar to the size of traditional HDDs.</p></li></ul></li></ul><h2 id="4-Performance-Comparison"><a href="#4-Performance-Comparison" class="headerlink" title="4. Performance Comparison"></a><strong>4. Performance Comparison</strong></h2><ul><li><p><strong>SATA SSDs</strong>:</p><ul><li><p>Maximum read/write speeds are capped at around <strong>550-600 MB/s</strong> due to the limitations of the <strong>SATA III interface</strong>.</p></li><li><p>Suitable for everyday consumer use, including boot drives, gaming, and regular workloads, but not optimal for high-performance or data-intensive tasks.</p></li></ul></li><li><p><strong>NVMe SSDs</strong>:</p><ul><li><p>The <strong>PCIe 3.0</strong> interface provides <strong>up to 4 GB/s</strong> of bandwidth for a 4-lane SSD, while <strong>PCIe 4.0</strong> can go up to <strong>8 GB/s</strong> or higher for consumer drives.</p></li><li><p><strong>Latency</strong> is much lower compared to SATA, with NVMe SSDs often providing sub-millisecond access times.</p></li><li><p>Ideal for <strong>high-end gaming</strong>, <strong>video editing</strong>, <strong>3D rendering</strong>, and other professional or enterprise-level applications that require quick access to large datasets.</p></li></ul></li></ul><h2 id="5-Form-Factor-and-Compatibility"><a href="#5-Form-Factor-and-Compatibility" class="headerlink" title="5. Form Factor and Compatibility"></a>5. Form Factor and Compatibility</h2><ul><li><p><strong>SATA SSDs</strong>: Often found in the <strong>2.5-inch form factor</strong> (similar to HDDs) and can be used in most computers and laptops with a SATA interface.</p></li><li><p><strong>NVMe SSDs</strong>: Use <strong>M.2</strong> or <strong>U.2</strong> form factors (for consumer or enterprise-level SSDs) and require a motherboard with an <strong>M.2 PCIe slot</strong> or a <strong>U.2 connector</strong> for compatibility. Some systems may require an adapter to support NVMe drives.</p></li></ul><h2 id="6-Summary-of-the-Relationship-Between-NVMe-and-SSD"><a href="#6-Summary-of-the-Relationship-Between-NVMe-and-SSD" class="headerlink" title="6. Summary of the Relationship Between NVMe and SSD"></a>6. Summary of the Relationship Between NVMe and SSD</h2><ul><li><p><strong>NVMe</strong> is a <strong>protocol</strong> designed to maximize the speed and efficiency of <strong>PCIe-based SSDs</strong>.</p></li><li><p><strong>SSD</strong> is a type of storage device that can use different interfaces like SATA, SAS, or PCIe, with <strong>NVMe SSDs</strong> specifically referring to <strong>SSDs that use the NVMe protocol</strong> over a <strong>PCIe interface</strong>.</p></li><li><p><strong>NVMe SSDs</strong> offer significant performance improvements (speed, latency, and scalability) over <strong>SATA SSDs</strong>, making them ideal for users with high-performance storage needs, such as gamers, content creators, and data centers.</p></li></ul><p>In short: <strong>NVMe SSD</strong> refers to a <strong>high-speed SSD</strong> that uses the <strong>NVMe protocol</strong> and <strong>PCIe interface</strong> for faster data transfer, while <strong>SATA SSDs</strong> use the older <strong>SATA interface</strong> and protocol, which are slower in comparison.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NVMe&quot;&gt;&lt;a href=&quot;#NVMe&quot; class=&quot;headerlink&quot; title=&quot;NVMe&quot;&gt;&lt;/a&gt;NVMe&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;NVMe (Non-Volatile Memory Express)&lt;/strong&gt; is a high-</summary>
      
    
    
    
    <category term="fs" scheme="https://www.willshirley.top/categories/fs/"/>
    
    
    <category term="nvme" scheme="https://www.willshirley.top/tags/nvme/"/>
    
  </entry>
  
  <entry>
    <title>ceph storage cluster</title>
    <link href="https://www.willshirley.top/2025/01/16/ceph%20storage/"/>
    <id>https://www.willshirley.top/2025/01/16/ceph%20storage/</id>
    <published>2025-01-16T06:57:29.000Z</published>
    <updated>2025-02-08T03:40:08.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><p>A Ceph Storage Cluster requires the following: at least one Ceph Monitor and at least one Ceph Manager, and at least as many <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-OSD">Ceph Object Storage Daemon</a>s (OSDs) as there are copies of a given object stored in the Ceph cluster (for example, if three copies of a given object are stored in the Ceph cluster, then at least three OSDs must exist in that Ceph cluster).</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    %% Define Ceph Components</span><br><span class="line">    subgraph &quot;Ceph Cluster&quot;</span><br><span class="line">        direction TB</span><br><span class="line">        OSDs[&quot;OSDs - Object Storage Daemons\n(Store and replicate data)&quot;]</span><br><span class="line">        Monitors[&quot;Monitors (MON) - Cluster Coordination\n(Maintain cluster map &amp; quorum)&quot;]</span><br><span class="line">        Managers[&quot;Managers (MGR) - Metrics &amp; Control\n(Provide monitoring, orchestration, and services)&quot;]</span><br><span class="line">        MDSs[&quot;MDSs - Metadata Servers\n(Manage CephFS metadata)&quot;]</span><br><span class="line">        RADOS[&quot;RADOS - Reliable Autonomic Distributed Object Store\n(Core distributed storage layer)&quot;]</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    %% Relationships</span><br><span class="line">    OSDs --&gt;|Store &amp; Replicate Data| RADOS</span><br><span class="line">    Monitors --&gt;|Maintain Cluster State &amp; Quorum| RADOS</span><br><span class="line">    Managers --&gt;|Monitor, Orchestrate &amp; Provide Services| RADOS</span><br><span class="line">    MDSs --&gt;|Manage CephFS Metadata| RADOS</span><br><span class="line">    RADOS --&gt;|Foundation of Ceph Storage| Ceph-Cluster</span><br><span class="line"></span><br><span class="line">    %% Descriptions</span><br><span class="line">    subgraph &quot;Component Descriptions&quot;</span><br><span class="line">        direction TB</span><br><span class="line">        MDSs_Description[&quot;MDSs: Handle metadata for CephFS, enabling file system operations like directory listing and file creation.&quot;]</span><br><span class="line">        RADOS_Description[&quot;RADOS: Core storage layer ensuring data durability, fault tolerance, and scalability.&quot;]</span><br><span class="line">        OSDs_Description[&quot;OSDs: Store, replicate, and recover object data across the cluster.&quot;]</span><br><span class="line">        Monitors_Description[&quot;Monitors: Maintain cluster state, handle authentication, and ensure consensus.&quot;]</span><br><span class="line">        Managers_Description[&quot;Managers: Provide additional cluster management features, including dashboards, orchestration, and monitoring.&quot;]</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    %% Link Descriptions to Components</span><br><span class="line">    MDSs --&gt; MDSs_Description</span><br><span class="line">    RADOS --&gt; RADOS_Description</span><br><span class="line">    OSDs --&gt; OSDs_Description</span><br><span class="line">    Monitors --&gt; Monitors_Description</span><br><span class="line">    Managers --&gt; Managers_Description</span><br></pre></td></tr></table></figure><ul><li><p><strong>Monitors</strong>: A <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Monitor">Ceph Monitor</a> (<code>ceph-mon</code>) maintains maps of the cluster state, including the <a href="https://docs.ceph.com/en/reef/rados/operations/monitoring/#display-mon-map">monitor map</a>, manager map, the OSD map, the MDS map, and the CRUSH map. These maps are critical cluster state required for Ceph daemons to coordinate with each other. Monitors are also responsible for managing authentication between daemons and clients. At least three monitors are normally required for redundancy and high availability.</p></li><li><p><strong>Managers</strong>: A <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Manager">Ceph Manager</a> daemon (<code>ceph-mgr</code>) is responsible for keeping track of runtime metrics and the current state of the Ceph cluster, including storage utilization, current performance metrics, and system load. The Ceph Manager daemons also host python-based modules to manage and expose Ceph cluster information, including a web-based <a href="https://docs.ceph.com/en/reef/mgr/dashboard/#mgr-dashboard">Ceph Dashboard</a> and <a href="https://docs.ceph.com/en/mgr/restful">REST API</a>. At least two managers are normally required for high availability.</p></li><li><p><strong>Ceph OSDs</strong>: An Object Storage Daemon (<a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-OSD">Ceph OSD</a>, <code>ceph-osd</code>) stores data, handles data replication, recovery, rebalancing, and provides some monitoring information to Ceph Monitors and Managers by checking other Ceph OSD Daemons for a heartbeat. At least three Ceph OSDs are normally required for redundancy and high availability.</p></li><li><p><strong>MDSs</strong>: A <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Metadata-Server">Ceph Metadata Server</a> (MDS, <code>ceph-mds</code>) stores metadata for the <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-File-System">Ceph File System</a>. Ceph Metadata Servers allow CephFS users to run basic commands (like <code>ls</code>, <code>find</code>, etc.) without placing a burden on the Ceph Storage Cluster.</p></li><li><p><strong>RADOS</strong>: (Reliable Autonomic Distributed Object Store) Any Cluster that supports <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Object-Storage">Ceph Object Storage</a> runs Ceph RADOS Gateway daemons (<code>radosgw</code>).</p></li></ul><h1 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h1><img src="/images/ceph/stack.webp" style="zoom: 60%"><h2 id="ceph-block-device"><a href="#ceph-block-device" class="headerlink" title="ceph block device"></a>ceph block device</h2><blockquote><p>RBD (Ceph Block Devices) </p></blockquote><img src="/images/ceph/block-device.webp" style="zoom: 60%"><p>Ceph block devices are thin-provisioned, resizable, and store data striped over multiple OSDs. Ceph block devices leverage RADOS capabilities including snapshotting, replication and strong consistency. Ceph block storage clients communicate with Ceph clusters through kernel modules or the <code>librbd</code> library.</p><h2 id="ceph-object-storage"><a href="#ceph-object-storage" class="headerlink" title="ceph object storage"></a>ceph object storage</h2><blockquote><p>RGW (Ceph Object Storage Gateway)</p></blockquote><img src="/images/ceph/object-storage.webp" style="zoom: 60%"><p>Ceph Object Storage uses the Ceph Object Gateway daemon (<code>radosgw</code>), an HTTP server designed to interact with a Ceph Storage Cluster. The Ceph Object Gateway provides interfaces that are compatible with both Amazon S3 and OpenStack Swift, and it has its own user management. Ceph Object Gateway can use a single Ceph Storage cluster to store data from Ceph File System and from Ceph Block device clients. The S3 API and the Swift API share a common namespace, which means that it is possible to write data to a Ceph Storage Cluster with one API and then retrieve that data with the other API.</p><h2 id="ceph-file-system"><a href="#ceph-file-system" class="headerlink" title="ceph file system"></a>ceph file system</h2><blockquote><p>cephFS</p></blockquote><img src="/images/ceph/cephfs.svg" style="zoom: 40%"><p>The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, <strong>RADOS</strong>. CephFS endeavors to provide a state-of-the-art, multi-use, highly available, and performant file store for a variety of applications, including traditional use-cases like shared home directories, HPC scratch space, and distributed workflow shared storage.</p><h1 id="Storage-Interfaces"><a href="#Storage-Interfaces" class="headerlink" title="Storage Interfaces"></a>Storage Interfaces</h1><p>Ceph offers several “storage interfaces”, which is another way of saying “ways of storing data”. These storage interfaces include: - <strong>CephFS</strong>(a file system) - <strong>RBD</strong> (block devices) - <strong>RADOS</strong> (an object store).</p><p>Deep down, though, all three of these are really RADOS object stores. CephFS and RBD are just presenting themselves as file systems and block devices.</p><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><h2 id="pools-vs-placement-groups-vs-object"><a href="#pools-vs-placement-groups-vs-object" class="headerlink" title="pools vs placement groups vs object"></a>pools vs placement groups vs object</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">graph TD;</span><br><span class="line">    </span><br><span class="line">    subgraph Ceph Cluster</span><br><span class="line">        A[Pool: mypool] --&gt;|Contains| B1[PG 1]</span><br><span class="line">        A[Pool: mypool] --&gt;|Contains| B2[PG 2]</span><br><span class="line">        A[Pool: mypool] --&gt;|Contains| B3[PG 3]</span><br><span class="line">        </span><br><span class="line">        B1 --&gt;|Stores| C1[Object A]</span><br><span class="line">        B1 --&gt;|Stores| C2[Object B]</span><br><span class="line">        </span><br><span class="line">        B2 --&gt;|Stores| C3[Object C]</span><br><span class="line">        B2 --&gt;|Stores| C4[Object D]</span><br><span class="line">        </span><br><span class="line">        B3 --&gt;|Stores| C5[Object E]</span><br><span class="line">        </span><br><span class="line">        subgraph OSD Nodes</span><br><span class="line">            D1[OSD.1] </span><br><span class="line">            D2[OSD.2] </span><br><span class="line">            D3[OSD.3] </span><br><span class="line">            D4[OSD.4] </span><br><span class="line">            D5[OSD.5] </span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        B1 --&gt;|Mapped to| D1</span><br><span class="line">        B1 --&gt;|Mapped to| D2</span><br><span class="line">        B1 --&gt;|Mapped to| D3</span><br><span class="line">        </span><br><span class="line">        B2 --&gt;|Mapped to| D2</span><br><span class="line">        B2 --&gt;|Mapped to| D3</span><br><span class="line">        B2 --&gt;|Mapped to| D4</span><br><span class="line">        </span><br><span class="line">        B3 --&gt;|Mapped to| D3</span><br><span class="line">        B3 --&gt;|Mapped to| D4</span><br><span class="line">        B3 --&gt;|Mapped to| D5</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><blockquote><p><strong>Pool (mypool)</strong> contains <strong>multiple PGs</strong> (Placement Groups).</p><p>Each <strong>PG stores multiple objects</strong>.</p><p>Each <strong>PG is mapped to multiple OSDs</strong>, based on the replication factor.</p><p><strong>Objects are stored within PGs</strong>, and Ceph automatically balances them across OSDs.</p></blockquote><p><strong>Understanding the Relationship Between Placement Groups (PGs), Pools, and Objects in Ceph</strong></p><p>In Ceph, <strong>objects</strong>, <strong>placement groups (PGs)</strong>, and <strong>pools</strong> work together to distribute and store data efficiently. Here’s how they relate:</p><h3 id="Pools"><a href="#Pools" class="headerlink" title="Pools"></a>Pools</h3><blockquote><p>The Logical Storage Units</p></blockquote><p>A pool in Ceph is a logical container that stores objects. Pools are used for different types of storage, such as:</p><ul><li><p><strong>RADOS block storage (RBD)</strong></p></li><li><p><strong>CephFS (Ceph File System)</strong></p></li><li><p><strong>RGW (RADOS Gateway for S3/Swift compatibility)</strong></p></li></ul><p>A pool has properties like:</p><ul><li><p><strong>Replication factor (e.g., 3x replication)</strong></p></li><li><p><strong>Erasure coding settings</strong></p></li><li><p><strong>Number of placement groups (PGs)</strong></p></li></ul><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create mypool 128</span><br></pre></td></tr></table></figure><p>This creates a pool named <strong>mypool</strong> with <strong>128 PGs</strong>.</p><h3 id="Placement-Groups-PGs"><a href="#Placement-Groups-PGs" class="headerlink" title="Placement Groups (PGs)"></a>Placement Groups (PGs)</h3><blockquote><p>Buckets for Objects</p></blockquote><p>A <strong>placement group (PG)</strong> is an internal grouping of objects within a pool. Instead of distributing objects directly across OSDs, Ceph first assigns objects to PGs. Then, PGs are mapped to OSDs.</p><p><strong>Why PGs?</strong></p><ul><li><p>They act as <strong>intermediaries</strong> between objects and OSDs.</p></li><li><p>They help balance data across OSDs efficiently.</p></li><li><p>They make recovery and replication easier.</p></li></ul><p><strong>PG Calculation Example</strong></p><p>If a pool has <strong>128 PGs</strong> and uses a <strong>3-replica</strong> setting, each PG will be mapped to 3 different OSDs.</p><h3 id="Objects"><a href="#Objects" class="headerlink" title="Objects"></a>Objects</h3><blockquote><p>The Data Units in Ceph</p></blockquote><p>An <strong>object</strong> is the fundamental data unit in Ceph. When you store a file in Ceph, it’s broken into one or more objects.</p><p><strong>How Objects Are Stored?</strong></p><ol><li><p><strong>Objects are stored in a Pool</strong> (e.g., mypool).</p></li><li><p><strong>Each object is mapped to a Placement Group (PG)</strong>.</p></li><li><p><strong>The PG is mapped to multiple OSDs</strong> (based on the pool’s replication settings).</p></li><li><p><strong>The primary OSD</strong> handles writes and ensures copies are stored on secondary OSDs.</p></li></ol><p><strong>Example Workflow:</strong></p><ul><li><p>A file is stored in <strong>RGW (Ceph S3) → mypool</strong>.</p></li><li><p>The file is broken into <strong>objects</strong>.</p></li><li><p>Each object is assigned to a <strong>PG</strong>.</p></li><li><p>The PG is mapped to <strong>3 OSDs</strong> (in a replicated pool).</p></li></ul><h3 id="Putting-It-All-Together"><a href="#Putting-It-All-Together" class="headerlink" title="Putting It All Together"></a>Putting It All Together</h3><p><strong>Example</strong></p><p>Assume:</p><ul><li><p>You create a <strong>pool</strong> called mypool with <strong>128 PGs</strong>.</p></li><li><p>You upload a file that gets broken into <strong>10 objects</strong>.</p></li><li><p>Each object is <strong>hashed and assigned</strong> to a PG.</p></li><li><p>The PGs are <strong>mapped to OSDs</strong>.</p></li></ul><table><thead><tr><th><strong>Component</strong></th><th><strong>Example Value</strong></th></tr></thead><tbody><tr><td><strong>Pool</strong></td><td>mypool</td></tr><tr><td><strong>PG Count</strong></td><td>128</td></tr><tr><td><strong>Object Name</strong></td><td>obj12345</td></tr><tr><td><strong>Object Hash</strong></td><td>hash(obj12345) → PG 45</td></tr><tr><td><strong>PG Assigned</strong></td><td>PG 45</td></tr><tr><td><strong>Mapped OSDs</strong></td><td>OSD.3, OSD.7, OSD.12</td></tr></tbody></table><p>In this case, <strong>obj12345</strong> will be stored in <strong>PG 45</strong>, which is replicated across <strong>OSD.3, OSD.7, and OSD.12</strong>.</p><p><strong>Key Takeaways</strong></p><ul><li><p><strong>Objects</strong> are stored in <strong>pools</strong>.</p></li><li><p><strong>Pools</strong> contain <strong>placement groups (PGs)</strong>.</p></li><li><p><strong>PGs</strong> group multiple objects and map them to multiple OSDs.</p></li><li><p><strong>OSDs</strong> store object data and handle replication.</p></li></ul><p><strong>Commands to Check PGs and Pools</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> List all pools:</span></span><br><span class="line">ceph osd pool ls</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check PG status:</span></span><br><span class="line">ceph pg stat</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See <span class="built_in">which</span> PGs belong to a pool:</span></span><br><span class="line">ceph pg dump | grep mypool</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check object distribution:</span></span><br><span class="line">ceph osd df</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;overview&quot;&gt;&lt;a href=&quot;#overview&quot; class=&quot;headerlink&quot; title=&quot;overview&quot;&gt;&lt;/a&gt;overview&lt;/h1&gt;&lt;p&gt;A Ceph Storage Cluster requires the following:</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="storage" scheme="https://www.willshirley.top/tags/storage/"/>
    
  </entry>
  
  <entry>
    <title>terminology</title>
    <link href="https://www.willshirley.top/2024/12/19/terminology/"/>
    <id>https://www.willshirley.top/2024/12/19/terminology/</id>
    <published>2024-12-19T07:52:40.000Z</published>
    <updated>2024-12-19T07:53:34.191Z</updated>
    
    <content type="html"><![CDATA[<h1 id="幂等-非幂等"><a href="#幂等-非幂等" class="headerlink" title="幂等/非幂等"></a>幂等/非幂等</h1><p>在计算机科学和网络通信中，幂等（Idempotent）和非幂等（Non-idempotent）描述了操作执行多次和执行一次的效果是否相同。</p><ol><li><p><strong>幂等操作</strong>：</p><ul><li><strong>定义</strong>：如果一个操作执行多次和执行一次的效果相同，那么这个操作被称为幂等操作。</li><li><strong>特点</strong>：幂等操作在重复执行时不会改变系统状态，即多次执行和执行一次的结果是相同的。</li><li><strong>例子</strong>：<ul><li><strong>GET请求</strong>：读取资源的操作是幂等的，因为无论读取多少次，资源的内容不会改变。</li><li><strong>PUT请求</strong>：更新资源的操作通常是幂等的，因为多次更新同一个资源到相同的状态，最终资源的状态不会改变。</li><li><strong>DELETE请求</strong>：删除资源的操作也是幂等的，因为一旦资源被删除，再次删除不会对系统产生进一步的影响。</li></ul></li></ul></li><li><p><strong>非幂等操作</strong>：</p><ul><li><strong>定义</strong>：如果一个操作执行多次和执行一次的效果不同，那么这个操作被称为非幂等操作。</li><li><strong>特点</strong>：非幂等操作在重复执行时会改变系统状态，即多次执行和执行一次的结果不同。</li><li><strong>例子</strong>：<ul><li><strong>POST请求</strong>：创建资源的操作是非幂等的，因为多次执行创建操作会创建多个资源实例。</li><li><strong>某些类型的UPDATE请求</strong>：如果更新操作涉及到计数器或其他会随时间变化的值，那么这些操作可能是非幂等的，因为多次更新可能会累积效果。</li></ul></li></ul></li></ol><p>在分布式系统和网络通信中，幂等性是一个重要的属性，因为它可以帮助确保系统的一致性和可靠性。例如，在网络请求中，如果一个请求由于网络问题被重复发送，幂等操作可以确保系统不会因为重复的请求而产生不一致的状态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;幂等-非幂等&quot;&gt;&lt;a href=&quot;#幂等-非幂等&quot; class=&quot;headerlink&quot; title=&quot;幂等/非幂等&quot;&gt;&lt;/a&gt;幂等/非幂等&lt;/h1&gt;&lt;p&gt;在计算机科学和网络通信中，幂等（Idempotent）和非幂等（Non-idempotent）描述了操作执行</summary>
      
    
    
    
    <category term="basic" scheme="https://www.willshirley.top/categories/basic/"/>
    
    
    <category term="terminology" scheme="https://www.willshirley.top/tags/terminology/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes pkg</title>
    <link href="https://www.willshirley.top/2024/12/17/kubernetes%20pkg/"/>
    <id>https://www.willshirley.top/2024/12/17/kubernetes%20pkg/</id>
    <published>2024-12-17T02:47:36.000Z</published>
    <updated>2025-03-02T04:09:12.419Z</updated>
    
    <content type="html"><![CDATA[<h1 id="klog"><a href="#klog" class="headerlink" title="klog"></a>klog</h1><h2 id="default-log-level"><a href="#default-log-level" class="headerlink" title="default log level"></a>default log level</h2><p>default use <code>klog.Info</code> is equivalent to <code>klog.V(0).Info</code>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> by default klog uses level 0, so logs with log.V(1) won<span class="string">&#x27;t appear unless you set --v=1 or a higher verbosity.</span></span></span><br><span class="line"></span><br><span class="line">containers:</span><br><span class="line">  - name: your-container</span><br><span class="line">    image: your-image</span><br><span class="line">    args:</span><br><span class="line">      - &quot;--v=1&quot; # if not config this, default klog uses level 0</span><br></pre></td></tr></table></figure><ul><li>For more detailed debug or trace messages, use klog.V(level).Info, where level is greater than 0 (e.g., klog.V(2).Info).</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;klog&quot;&gt;&lt;a href=&quot;#klog&quot; class=&quot;headerlink&quot; title=&quot;klog&quot;&gt;&lt;/a&gt;klog&lt;/h1&gt;&lt;h2 id=&quot;default-log-level&quot;&gt;&lt;a href=&quot;#default-log-level&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Kubernetes" scheme="https://www.willshirley.top/categories/Kubernetes/"/>
    
    
    <category term="pkg" scheme="https://www.willshirley.top/tags/pkg/"/>
    
  </entry>
  
</feed>
