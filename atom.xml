<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Discipline &amp; Reflect</title>
  
  
  <link href="https://www.willshirley.top/atom.xml" rel="self"/>
  
  <link href="https://www.willshirley.top/"/>
  <updated>2025-03-10T02:21:32.198Z</updated>
  <id>https://www.willshirley.top/</id>
  
  <author>
    <name>brook</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>network solution</title>
    <link href="https://www.willshirley.top/2025/03/10/network%20solution/"/>
    <id>https://www.willshirley.top/2025/03/10/network%20solution/</id>
    <published>2025-03-10T02:04:37.000Z</published>
    <updated>2025-03-10T02:21:32.198Z</updated>
    
    <content type="html"><![CDATA[<h1 id="make-bond"><a href="#make-bond" class="headerlink" title="make bond"></a>make bond</h1><p>Bonding Mode</p><table><thead><tr><th>Mode</th><th>Max Speed (Single Flow)</th><th>Max Speed (Multiple Flows)</th><th>Key Feature</th></tr></thead><tbody><tr><td>mode=0 (round-robin)</td><td>Sum of all slaves</td><td>Sum of all slaves</td><td>Best for maximizing bandwidth, may cause out-of-order packets.</td></tr><tr><td>mode=1 (active-backup)</td><td>One interface’s speed</td><td>One interface’s speed</td><td>Best for redundancy, no speed gain.</td></tr><tr><td>mode=2 (balance-xor)</td><td>One interface’s speed</td><td>Sum of all slaves</td><td>Good for performance; switch support required.</td></tr><tr><td>mode=4 (802.3ad - LACP)</td><td>One interface’s speed</td><td>Sum of all slaves</td><td>Efficient load balancing for multiple flows; requires switch support.</td></tr><tr><td>mode=5 (balance-tlb)</td><td>One interface’s speed</td><td>Sum of all slaves (outgoing only)</td><td>Adaptive transmit load balancing.</td></tr><tr><td>mode=6 (balance-alb)</td><td>One interface’s speed</td><td>Sum of all slaves</td><td>Adaptive load balancing without switch support.</td></tr></tbody></table><blockquote><p> assume make ib7s400p0 to bond1, which have assigned  ip 172.30.12.46 with ib7s400p0</p></blockquote><p><strong>optional</strong>: Load the Bonding Kernel Module</p><p>Ensure the bonding driver is loaded:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe bonding</span><br></pre></td></tr></table></figure><p>To persist this across reboots:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;bonding&quot; | sudo tee /etc/modules-load.d/bonding.conf</span><br></pre></td></tr></table></figure><p><strong>optional</strong>: Remove the IP Address from ib7s400p0</p><p>Since the bond interface will carry the IP, you must remove the IP from ib7s400p0 first:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr del 172.30.12.46/24 dev ib7s400p0</span><br></pre></td></tr></table></figure><p><strong>Step 2: Create the Bond Interface (bond1)</strong></p><p>Create the bond interface:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link add bond1 type bond</span><br></pre></td></tr></table></figure><p><strong>Step 3: Configure Bonding Mode</strong></p><p>For your use case, you can set mode=active-backup (best for redundancy with one NIC now) or mode=802.3ad (if planning for LACP in the future).</p><p><strong>Set Active-Backup Mode (Recommended for 1 NIC Now)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;active-backup&quot; | sudo tee /sys/class/net/bond1/bonding/mode</span><br></pre></td></tr></table></figure><p><strong>OR Set 802.3ad Mode (If Future Expansion is Planned)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;802.3ad&quot; | sudo tee /sys/class/net/bond1/bonding/mode</span><br></pre></td></tr></table></figure><p><strong>Step 4: Add ib7s400p0 as a Slave</strong></p><ol><li>Remove any IP address from ib7s400p0 (if it has one):</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev ib7s400p0</span><br></pre></td></tr></table></figure><ol start="2"><li>Add ib7s400p0 to bond1:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sudo ip link <span class="built_in">set</span> ib7s400p0 down</span></span><br><span class="line">sudo ip link set ib7s400p0 master bond1</span><br><span class="line">sudo ip link set ib7s400p0 up</span><br></pre></td></tr></table></figure><ol start="3"><li>Bring bond1 up:</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set bond1 up</span><br></pre></td></tr></table></figure><p><strong>Step 5: Assign an IP Address</strong></p><p>If using a <strong>static IP</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr add 172.30.12.46/24 dev bond1</span><br></pre></td></tr></table></figure><p>Or if using <strong>DHCP</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dhclient bond1</span><br></pre></td></tr></table></figure><p>Try to force traffic through bond1 by removing the direct route through ib7s400p0</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip route del 172.30.12.0/24 dev ib7s400p0</span><br></pre></td></tr></table></figure><p><strong>Step 6: Persistent Configuration (Rocky 9 / RHEL 9) (optional)</strong></p><p>To ensure the bond configuration persists after reboot:</p><ol><li>Create /etc/sysconfig/network-scripts/ifcfg-bond1</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=bond1</span><br><span class="line">TYPE=Bond</span><br><span class="line">BONDING_MASTER=yes</span><br><span class="line">BOOTPROTO=dhcp      # Or use &#x27;static&#x27; if assigning a static IP</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BONDING_OPTS=&quot;mode=active-backup miimon=100&quot;</span><br></pre></td></tr></table></figure><ol start="2"><li>Create /etc/sysconfig/network-scripts/ifcfg-ib7s400p0</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=ib7s400p0</span><br><span class="line">MASTER=bond1</span><br><span class="line">SLAVE=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">ONBOOT=yes</span><br></pre></td></tr></table></figure><ol start="3"><li>Restart NetworkManager to apply changes:</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart NetworkManager</span><br></pre></td></tr></table></figure><p><strong>Step 7: Verify Configuration</strong></p><ol><li>Check bond details:</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/net/bonding/bond1</span><br></pre></td></tr></table></figure><p>ib7s400p0 should now appear as a <strong>Slave Interface</strong>.</p><ol start="2"><li>Confirm IP address and route:</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip a</span><br><span class="line">ip route</span><br></pre></td></tr></table></figure><ol start="3"><li>Test connectivity:</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping 172.30.12.47 # other same bond addr</span><br></pre></td></tr></table></figure><p><strong>Step 8: Optional - Test Failover (For active-backup Mode)</strong></p><p>​    1.    Temporarily bring down ib7s400p0:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set ib7s400p0 down</span><br></pre></td></tr></table></figure><p>​    2.    Check bond1 status:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/net/bonding/bond1</span><br></pre></td></tr></table></figure><p>In active-backup mode, bond1 should remain active (even though ib7s400p0 is down).</p><p>In 802.3ad mode, bond1 would go down since no alternate NIC exists yet.</p><p><strong>Summary</strong></p><p>Use <strong>active-backup</strong> mode if ib7s400p0 is the only slave (recommended now).</p><p>Use <strong>802.3ad</strong> if planning to add more NICs for higher throughput in the future.</p><p>Ensure /etc/sysconfig/network-scripts/ configs are properly set for persistence.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;make-bond&quot;&gt;&lt;a href=&quot;#make-bond&quot; class=&quot;headerlink&quot; title=&quot;make bond&quot;&gt;&lt;/a&gt;make bond&lt;/h1&gt;&lt;p&gt;Bonding Mode&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;</summary>
      
    
    
    
    <category term="network" scheme="https://www.willshirley.top/categories/network/"/>
    
    
    <category term="solution" scheme="https://www.willshirley.top/tags/solution/"/>
    
  </entry>
  
  <entry>
    <title>llm vllm</title>
    <link href="https://www.willshirley.top/2025/03/06/llm%20vllm/"/>
    <id>https://www.willshirley.top/2025/03/06/llm%20vllm/</id>
    <published>2025-03-06T07:25:11.000Z</published>
    <updated>2025-03-06T07:28:51.215Z</updated>
    
    <content type="html"><![CDATA[<h1 id="route"><a href="#route" class="headerlink" title="route"></a>route</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">INFO llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 135.23 seconds</span><br><span class="line">INFO api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000</span><br><span class="line">Available routes are:</span><br><span class="line"> Route: /openapi.json, Methods: GET, HEAD</span><br><span class="line"> Route: /docs, Methods: GET, HEAD</span><br><span class="line"> Route: /docs/oauth2-redirect, Methods: GET, HEAD</span><br><span class="line"> Route: /redoc, Methods: GET, HEAD</span><br><span class="line"> Route: /health, Methods: GET</span><br><span class="line"> Route: /ping, Methods: GET, POST</span><br><span class="line"> Route: /tokenize, Methods: POST</span><br><span class="line"> Route: /detokenize, Methods: POST</span><br><span class="line"> Route: /v1/models, Methods: GET</span><br><span class="line"> Route: /version, Methods: GET</span><br><span class="line"> Route: /v1/chat/completions, Methods: POST</span><br><span class="line"> Route: /v1/completions, Methods: POST</span><br><span class="line"> Route: /v1/embeddings, Methods: POST</span><br><span class="line"> Route: /pooling, Methods: POST</span><br><span class="line"> Route: /score, Methods: POST</span><br><span class="line"> Route: /v1/score, Methods: POST</span><br><span class="line"> Route: /v1/audio/transcriptions, Methods: POST</span><br><span class="line"> Route: /rerank, Methods: POST</span><br><span class="line"> Route: /v1/rerank, Methods: POST</span><br><span class="line"> Route: /v2/rerank, Methods: POST</span><br><span class="line"> Route: /invocations, Methods: POST</span><br><span class="line">INFO:     Started server process [1]</span><br><span class="line">INFO:     Waiting for application startup.</span><br><span class="line">INFO:     Application startup complete.</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;route&quot;&gt;&lt;a href=&quot;#route&quot; class=&quot;headerlink&quot; title=&quot;route&quot;&gt;&lt;/a&gt;route&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;</summary>
      
    
    
    
    <category term="llm" scheme="https://www.willshirley.top/categories/llm/"/>
    
    
    <category term="sinppet" scheme="https://www.willshirley.top/tags/sinppet/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes RBAC</title>
    <link href="https://www.willshirley.top/2025/03/01/kubernetes%20RBAC/"/>
    <id>https://www.willshirley.top/2025/03/01/kubernetes%20RBAC/</id>
    <published>2025-03-01T11:48:15.000Z</published>
    <updated>2025-03-01T12:20:55.733Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>RBAC (Role-Based Access Control) in Kubernetes is a security model that controls who can access and perform actions on resources (like Pods, Deployments, Services, etc.) within a cluster.</p></blockquote><h1 id="Component"><a href="#Component" class="headerlink" title="Component"></a>Component</h1><p>Kubernetes RBAC consists of four main components:</p><table><thead><tr><th>Component</th><th>Description</th></tr></thead><tbody><tr><td>Role / ClusterRole</td><td>Defines what actions (verbs) can be performed on which resources.</td></tr><tr><td>RoleBinding / ClusterRoleBinding</td><td>Grants a Role or ClusterRole to a User, Group, or ServiceAccount.</td></tr><tr><td>Subjects (Users, Groups, ServiceAccounts)</td><td>Who is allowed to perform actions (User, Group, or ServiceAccount).</td></tr><tr><td>Resources &amp; API Groups</td><td>The objects that can be controlled (e.g., pods, deployments, services, etc.).</td></tr></tbody></table><h2 id="Role-amp-ClusterRole"><a href="#Role-amp-ClusterRole" class="headerlink" title="Role &amp; ClusterRole"></a>Role &amp; ClusterRole</h2><blockquote><p>RBAC defines what actions can be performed on which resources using Roles and ClusterRoles.</p></blockquote><h3 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h3><blockquote><p>A Role is used for namespace-scoped access control.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">my-namespace</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><p>✅ This Role allows a user to view (get, list, watch) Pods only in the namespace my-namespace.</p><p>❌ This does not grant access to other namespaces.</p><h3 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h3><blockquote><p> A ClusterRole is used for cluster-wide permissions or permissions across all namespaces.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><p>✅ This allows viewing (get, list, watch) Pods in all namespaces.</p><h2 id="RoleBinding-amp-ClusterRoleBinding"><a href="#RoleBinding-amp-ClusterRoleBinding" class="headerlink" title="RoleBinding &amp; ClusterRoleBinding"></a>RoleBinding &amp; ClusterRoleBinding</h2><blockquote><p>Who Gets These Permissions?</p></blockquote><h3 id="RoleBinding"><a href="#RoleBinding" class="headerlink" title="RoleBinding"></a>RoleBinding</h3><blockquote><p>A RoleBinding assigns a Role to a specific user, group, or ServiceAccount in a namespace.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">my-namespace</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><p>✅ This binds the pod-reader Role to my-user in the my-namespace namespace.</p><p>❌ It does not grant permissions outside my-namespace.</p><h3 id="ClusterRoleBinding"><a href="#ClusterRoleBinding" class="headerlink" title="ClusterRoleBinding"></a>ClusterRoleBinding</h3><blockquote><p>A ClusterRoleBinding assigns a ClusterRole to a User, Group, or ServiceAccount across all namespaces.</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-pod-reader-binding</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><p>✅ This grants my-user access to Pods across all namespaces.</p><h2 id="Subjects"><a href="#Subjects" class="headerlink" title="Subjects"></a>Subjects</h2><blockquote><p>A RoleBinding or ClusterRoleBinding grants permissions to a subject, which can be:</p></blockquote><table><thead><tr><th>Subject Type</th><th>Description</th></tr></thead><tbody><tr><td>User</td><td>A real human user (external identity).</td></tr><tr><td>Group</td><td>A group of users (e.g., dev-team).</td></tr><tr><td>ServiceAccount</td><td>An internal Kubernetes identity for a Pod or Controller.</td></tr></tbody></table><p>Example: Binding a ServiceAccount (my-sa) to a Role</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">my-namespace</span></span><br></pre></td></tr></table></figure><h2 id="API-Groups-amp-Resources"><a href="#API-Groups-amp-Resources" class="headerlink" title="API Groups &amp; Resources"></a>API Groups &amp; Resources</h2><p>Each RBAC rule specifies which resources in which API groups can be accessed.</p><table><thead><tr><th>API Group</th><th>Resources Example</th></tr></thead><tbody><tr><td>“” (core)</td><td>pods, services, nodes</td></tr><tr><td>apps</td><td>deployments, daemonsets</td></tr><tr><td>batch</td><td>jobs, cronjobs</td></tr><tr><td>rbac.authorization.k8s.io</td><td>roles, rolebindings, clusterroles, clusterrolebindings</td></tr></tbody></table><p>Example: Allow Access to Pods, Deployments, and Nodes</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>, <span class="string">&quot;services&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;apps&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;deployments&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>]</span><br></pre></td></tr></table></figure><p>✅ Allows reading Pods and Services (core API group).</p><p>✅ Allows reading Deployments (apps API group).</p><p>✅ Allows reading Nodes (core API group).</p><h1 id="Works-Scenario"><a href="#Works-Scenario" class="headerlink" title="Works Scenario"></a>Works Scenario</h1><h2 id="Scenario-1"><a href="#Scenario-1" class="headerlink" title="Scenario 1"></a>Scenario 1</h2><p>Use Case:</p><ul><li><p>A user (dev-user) needs read-only access to Pods in the dev namespace.</p></li><li><p>The ServiceAccount (cicd-runner-sa) needs to create Deployments in the cicd namespace.</p></li></ul><p>Solution: Define the RBAC Rules</p><ol><li>Create a Role for Read-Only Pod Access in dev Namespace</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><ol start="2"><li>Bind the Role to dev-user in dev Namespace</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dev-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><ol start="3"><li>Create a Role for cicd-runner-sa to Deploy Apps in cicd Namespace</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-creator</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cicd</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;apps&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;deployments&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br></pre></td></tr></table></figure><ol start="4"><li>Bind the Role to the cicd-runner-sa ServiceAccount</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cicd-runner-binding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cicd</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cicd-runner-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">cicd</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-creator</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><h2 id="Scenario-2"><a href="#Scenario-2" class="headerlink" title="Scenario 2"></a>Scenario 2</h2><p>Scenario: Grant Read-Only Access to Pods Across Multiple Namespaces</p><p>Use Case:</p><ul><li><p>A developer team needs read-only access to Pods in multiple namespaces (dev, staging, prod).</p></li><li><p>Instead of creating separate Roles in each namespace, we use one ClusterRole.</p></li><li><p>We then create RoleBindings in each namespace to assign the ClusterRole.</p></li></ul><ol><li>Create a ClusterRole to Read Pods Across Namespaces</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">multi-namespace-pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure><ol start="2"><li>Bind the ClusterRole to a User for Multiple Namespaces</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">multi-namespace-pod-reader-binding</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">developer-user</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">multi-namespace-pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><h1 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h1><p>Key Takeaways from Kubernetes RBAC Architecture</p><ol><li><p>Role / ClusterRole → Defines permissions (what actions are allowed?).</p></li><li><p>RoleBinding / ClusterRoleBinding → Assigns permissions (who gets access?).</p></li><li><p>Subjects → Users, Groups, ServiceAccounts (who is authorized?).</p></li><li><p>API Groups → Determines resources that can be controlled.</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;RBAC (Role-Based Access Control) in Kubernetes is a security model that controls who can access and perform actions on resou</summary>
      
    
    
    
    <category term="Kubernetes" scheme="https://www.willshirley.top/categories/Kubernetes/"/>
    
    
    <category term="RBAC" scheme="https://www.willshirley.top/tags/RBAC/"/>
    
  </entry>
  
  <entry>
    <title>podman snippet</title>
    <link href="https://www.willshirley.top/2025/02/10/podman%20snippet/"/>
    <id>https://www.willshirley.top/2025/02/10/podman%20snippet/</id>
    <published>2025-02-10T08:06:03.000Z</published>
    <updated>2025-02-10T08:07:20.811Z</updated>
    
    <content type="html"><![CDATA[<ul><li>不同用户下执行 <code>podman ps</code>，只能查看当前用户的运行容器（即使是root用户，也不能查看其他普通用户启用的容器信息）</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;不同用户下执行 &lt;code&gt;podman ps&lt;/code&gt;，只能查看当前用户的运行容器（即使是root用户，也不能查看其他普通用户启用的容器信息）&lt;/li&gt;
&lt;/ul&gt;
</summary>
      
    
    
    
    <category term="podman" scheme="https://www.willshirley.top/categories/podman/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>ceph rados</title>
    <link href="https://www.willshirley.top/2025/02/08/ceph%20rados/"/>
    <id>https://www.willshirley.top/2025/02/08/ceph%20rados/</id>
    <published>2025-02-08T03:40:20.000Z</published>
    <updated>2025-02-08T10:12:34.786Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Storage-Cluster">Ceph Storage Cluster</a> provides the basic storage service that allows <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph">Ceph</a> to uniquely deliver <strong>object, block, and file storage</strong> in one unified system. However, you are not limited to using the RESTful, block, or POSIX interfaces. Based upon RADOS, the <code>librados</code> API enables you to create your own interface to the Ceph Storage Cluster.</p><h1 id="command"><a href="#command" class="headerlink" title="command"></a>command</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> list objects by specify pool</span></span><br><span class="line">rados -p &lt;pool-name&gt; ls</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">read</span> object</span></span><br><span class="line">rados -p &lt;pool-name&gt; get &lt;object-name&gt; output.txt</span><br><span class="line">cat output.txt</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check xattr value</span></span><br><span class="line">rados -p &lt;pool-name&gt; getxattr &lt;object-name&gt; &lt;xattr-key&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The &lt;a href=&quot;https://docs.ceph.com/en/reef/glossary/#term-Ceph-Storage-Cluster&quot;&gt;Ceph Storage Cluster&lt;/a&gt; provides the basic storage servi</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="storage" scheme="https://www.willshirley.top/tags/storage/"/>
    
  </entry>
  
  <entry>
    <title>ceph deploy</title>
    <link href="https://www.willshirley.top/2025/02/07/ceph%20deploy/"/>
    <id>https://www.willshirley.top/2025/02/07/ceph%20deploy/</id>
    <published>2025-02-07T09:40:41.000Z</published>
    <updated>2025-02-08T08:09:19.197Z</updated>
    
    <content type="html"><![CDATA[<h1 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h1><h2 id="install-cephadm"><a href="#install-cephadm" class="headerlink" title="install cephadm"></a>install cephadm</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dnf search release-ceph</span><br><span class="line">dnf install --assumeyes centos-release-ceph-reef</span><br><span class="line">dnf install --assumeyes cephadm</span><br></pre></td></tr></table></figure><ul><li><p>enable ceph cli</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cephadm add-repo --release reef</span><br><span class="line">cephadm install ceph-common</span><br></pre></td></tr></table></figure></li></ul><h2 id="booststrap"><a href="#booststrap" class="headerlink" title="booststrap"></a>booststrap</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cephadm bootstrap --mon-ip 172.20.7.232</span><br></pre></td></tr></table></figure><ul><li>log<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Ceph Dashboard is now available at:</span><br><span class="line">        URL: https://dingo7232.com:8443/</span><br><span class="line">        User: admin</span><br><span class="line">    Password: &lt;password&gt;</span><br><span class="line">Enabling client.admin keyring and conf on hosts with &quot;admin&quot; label</span><br><span class="line">Saving cluster configuration to /var/lib/ceph/6a65c746-e532-11ef-8ac2-fa7c097efb00/config directory</span><br><span class="line">Enabling autotune for osd_memory_target</span><br><span class="line">You can access the Ceph CLI as following in case of multi-cluster or non-default config:</span><br><span class="line"></span><br><span class="line">        sudo /sbin/cephadm shell --fsid 6a65c746-e532-11ef-8ac2-fa7c097efb00 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line">Or, if you are only running a single cluster on this host:</span><br><span class="line"></span><br><span class="line">        sudo /sbin/cephadm shell</span><br><span class="line"></span><br><span class="line">Please consider enabling telemetry to help improve Ceph:</span><br><span class="line"></span><br><span class="line">        ceph telemetry on</span><br><span class="line"></span><br><span class="line">For more information see:</span><br><span class="line"></span><br><span class="line">        https://docs.ceph.com/en/latest/mgr/telemetry/</span><br><span class="line"></span><br><span class="line">Bootstrap complete.</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h2 id="add-hosts"><a href="#add-hosts" class="headerlink" title="add hosts"></a>add hosts</h2><ul><li><p>Install the cluster’s public SSH key in the new host’s root user’s authorized_keys file:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -f -i /etc/ceph/ceph.pub root@dingo7233</span><br><span class="line">ssh-copy-id -f -i /etc/ceph/ceph.pub root@dingo7234</span><br></pre></td></tr></table></figure></li><li><p>Tell Ceph that the new node is part of the cluster</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph orch host add *&lt;newhost&gt;* [*&lt;ip&gt;*] [*&lt;label1&gt; ...*]</span></span><br><span class="line">ceph orch host add dingo7233 172.20.7.233</span><br><span class="line">ceph orch host add dingo7234 172.20.7.234</span><br><span class="line">or</span><br><span class="line">ceph orch host add dingo7233 172.20.7.233 --labels _admin</span><br><span class="line">ceph orch host add dingo7234 172.20.7.234 --labels _admin</span><br></pre></td></tr></table></figure></li><li><p>add label (optional)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph orch host label add dingo7233 _admin</span><br><span class="line">ceph orch host label add dingo7234 _admin</span><br></pre></td></tr></table></figure></li><li><p>list hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch host ls --detail</span><br></pre></td></tr></table></figure></li></ul><h2 id="add-storage"><a href="#add-storage" class="headerlink" title="add storage"></a>add storage</h2><ul><li><p>check available devices</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch device ls</span><br></pre></td></tr></table></figure></li><li><p>apply osd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph orch apply osd --all-available-devices</span><br></pre></td></tr></table></figure></li></ul><h1 id="check"><a href="#check" class="headerlink" title="check"></a>check</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ceph status</span></span><br><span class="line">cluster:</span><br><span class="line">    id:     6a65c746-e532-11ef-8ac2-fa7c097efb00</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            1 mgr modules have recently crashed</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum dingo7232,dingo7234,dingo7233 (age 16m)</span><br><span class="line">    mgr: dingo7232.znvodw(active, since 24m), standbys: dingo7234.yenqrt</span><br><span class="line">    osd: 3 osds: 3 up (since 15m), 3 in (since 15m)</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 2 objects, 449 KiB</span><br><span class="line">    usage:   81 MiB used, 600 GiB / 600 GiB avail</span><br><span class="line">    pgs:     1 active+clean</span><br></pre></td></tr></table></figure><h1 id="thouble-shooting"><a href="#thouble-shooting" class="headerlink" title="thouble shooting"></a>thouble shooting</h1><h2 id="redeploy-cluster"><a href="#redeploy-cluster" class="headerlink" title="redeploy cluster"></a>redeploy cluster</h2><p>To remove an existing Ceph cluster deployed using <code>cephadm</code> and redeploy a new one, follow these steps:</p><ul><li>Step 1: Stop All Ceph Services</li></ul><p>First, stop all Ceph services on each host in the cluster.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop ceph.target</span><br></pre></td></tr></table></figure><ul><li>Step 2: Remove Ceph Configuration and Data</li></ul><p>Remove the Ceph configuration and data directories.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /etc/ceph</span><br><span class="line">sudo rm -rf /var/lib/ceph</span><br><span class="line">sudo rm -rf /var/<span class="built_in">log</span>/ceph</span><br></pre></td></tr></table></figure><ul><li><p>Step 3: deploy as below words</p></li><li><p>Step 4: Verify Cluster Health</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph -s</span><br></pre></td></tr></table></figure></li></ul><p>If you encounter any issues during the redeployment, check the logs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo journalctl -u ceph-* -f</span><br></pre></td></tr></table></figure><p>Or check the Ceph logs directly:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo less /var/<span class="built_in">log</span>/ceph/ceph.log</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;deploy&quot;&gt;&lt;a href=&quot;#deploy&quot; class=&quot;headerlink&quot; title=&quot;deploy&quot;&gt;&lt;/a&gt;deploy&lt;/h1&gt;&lt;h2 id=&quot;install-cephadm&quot;&gt;&lt;a href=&quot;#install-cephadm&quot; class</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="deploy" scheme="https://www.willshirley.top/tags/deploy/"/>
    
  </entry>
  
  <entry>
    <title>linux A vs B</title>
    <link href="https://www.willshirley.top/2025/01/22/linux%20A%20vs%20B/"/>
    <id>https://www.willshirley.top/2025/01/22/linux%20A%20vs%20B/</id>
    <published>2025-01-22T06:11:11.000Z</published>
    <updated>2025-01-22T06:20:04.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Systemd-limits-vs-PAM-limits"><a href="#Systemd-limits-vs-PAM-limits" class="headerlink" title="Systemd limits vs PAM limits"></a>Systemd limits vs PAM limits</h1><p><strong>Systemd limits</strong> and <strong>PAM limits</strong> are two different ways of configuring resource limits in Linux. They control aspects like <strong>open file limits (<strong>nofile</strong>)</strong>, <strong>memory</strong>, and <strong>CPU usage</strong>, but they work at different levels of the system.</p><h2 id="PAM-Limits"><a href="#PAM-Limits" class="headerlink" title="PAM Limits"></a>PAM Limits</h2><p>PAM (Pluggable Authentication Module) sets limits when <strong>a user logs in</strong> via <strong>SSH, TTY, or GUI login</strong>. It does <strong>not</strong> apply to system services. </p><p><strong>Configuration:</strong></p><ul><li><p>/etc/security/limits.conf</p></li><li><p>/etc/security/limits.d/*.conf</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Example (/etc/security/limits.conf or /etc/security/limits.d/custom.conf):**</span></span><br><span class="line">dongw soft nofile 65535</span><br><span class="line">dongw hard nofile 65535</span><br></pre></td></tr></table></figure><p><strong>Limitations of PAM:</strong></p><ul><li><p>Only affects interactive logins (TTY, SSH, GUI).</p></li><li><p>Does <strong>not</strong> apply to systemd-managed services.</p></li></ul><h2 id="Systemd-Limits"><a href="#Systemd-Limits" class="headerlink" title="Systemd Limits"></a>Systemd Limits</h2><p>Systemd sets limits <strong>for system services and user sessions</strong>. It applies to <strong>both login sessions and system services</strong>, making it more powerful than PAM.</p><p><strong>Configuration:</strong></p><ul><li><p><strong>Global (for all services and users)</strong>:</p></li><li><p>/etc/systemd/system.conf (system-wide limits)</p></li><li><p>/etc/systemd/user.conf (per-user session limits)</p></li><li><p><strong>Per-service limits</strong> (for specific services):</p></li><li><p>/etc/systemd/system/<service>.service</p></li></ul><h3 id="Global-Limits"><a href="#Global-Limits" class="headerlink" title="Global Limits"></a>Global Limits</h3><p>Applies to <strong>all</strong> processes managed by systemd. (/etc/systemd/system.conf or /etc/systemd/user.conf)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DefaultLimitNOFILE=65535</span><br></pre></td></tr></table></figure><h3 id="Per-Service-Limits"><a href="#Per-Service-Limits" class="headerlink" title="Per-Service Limits"></a>Per-Service Limits</h3><p> This applies to only on specify service. (e.g. /etc/systemd/system/myservice.service)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">LimitNOFILE=65535</span><br></pre></td></tr></table></figure><p><strong>Advantages of Systemd Limits:</strong></p><ul><li><p>Affects <strong>both user logins and system services</strong>.</p></li><li><p>Ensures limits persist across <strong>system reboots</strong>.</p></li><li><p>More reliable for applications like Ceph, Docker, and databases.</p></li></ul><h2 id="Comparison-Table"><a href="#Comparison-Table" class="headerlink" title="Comparison Table"></a><strong>Comparison Table</strong></h2><table><thead><tr><th align="left"><strong>Feature</strong></th><th><strong>PAM Limits (<code>limits.conf</code>)</strong></th><th><strong>Systemd Limits (<code>system.conf</code>/<code>.service</code>)</strong></th></tr></thead><tbody><tr><td align="left">Applies to interactive users</td><td>✅ Yes (SSH, TTY, GUI)</td><td>✅ Yes</td></tr><tr><td align="left">Applies to system services</td><td>❌ No</td><td>✅ Yes</td></tr><tr><td align="left">Persists across reboots</td><td>✅ Yes</td><td>✅ Yes</td></tr><tr><td align="left">Easy to configure</td><td>✅ Yes</td><td>✅ Yes</td></tr><tr><td align="left">Best for tuning servers</td><td>⚠️  Partial</td><td>✅ Yes</td></tr></tbody></table><ul><li><p><strong>For interactive user sessions</strong> → Use <strong>PAM (<strong>limits.conf</strong>)</strong>.</p></li><li><p><strong>For system services and all users</strong> → Use <strong>Systemd (<strong>system.conf</strong>,</strong> .service**)**.</p></li><li><p><strong>For the best persistence and reliability</strong>, configure <strong>both</strong>.</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Systemd-limits-vs-PAM-limits&quot;&gt;&lt;a href=&quot;#Systemd-limits-vs-PAM-limits&quot; class=&quot;headerlink&quot; title=&quot;Systemd limits vs PAM limits&quot;&gt;&lt;/a&gt;Sy</summary>
      
    
    
    
    <category term="linux" scheme="https://www.willshirley.top/categories/linux/"/>
    
    
    <category term="A vs B" scheme="https://www.willshirley.top/tags/A-vs-B/"/>
    
  </entry>
  
  <entry>
    <title>linux tune</title>
    <link href="https://www.willshirley.top/2025/01/21/linux%20tune/"/>
    <id>https://www.willshirley.top/2025/01/21/linux%20tune/</id>
    <published>2025-01-21T02:43:04.000Z</published>
    <updated>2025-02-24T06:50:54.511Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cpu"><a href="#cpu" class="headerlink" title="cpu"></a>cpu</h1><h2 id="governor-strategy"><a href="#governor-strategy" class="headerlink" title="governor strategy"></a>governor strategy</h2><ul><li><p>Check Available CPU Governors</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run the following <span class="built_in">command</span> to check available CPU governors:</span></span><br><span class="line">cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You should see something like below info:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> conservative ondemand userspace powersave performance schedutil</span></span><br></pre></td></tr></table></figure></li><li><p>Check Current CPU Governor</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</span><br><span class="line">or</span><br><span class="line">cpupower frequency-info --policy # should install cpupower command by &#x27;sudo dnf install kernel-tools -y&#x27;</span><br></pre></td></tr></table></figure></li><li><p>Set CPU Governor to Performance temporarily</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To temporarily <span class="built_in">set</span> the CPU governor to performance (until reboot):</span></span><br><span class="line">for cpu in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do</span><br><span class="line">  echo performance | sudo tee $cpu</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>Set CPU Governor Persistent</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> First, install cpupower:</span></span><br><span class="line">sudo dnf install kernel-tools -y</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Then, <span class="built_in">set</span> the governor:</span></span><br><span class="line">sudo cpupower frequency-set -g performance</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> To apply this setting at boot, <span class="built_in">enable</span> the service:</span></span><br><span class="line">sudo systemctl enable --now cpupower.service</span><br></pre></td></tr></table></figure></li></ul><h1 id="nvme"><a href="#nvme" class="headerlink" title="nvme"></a>nvme</h1><h2 id="I-O-scheduler"><a href="#I-O-scheduler" class="headerlink" title="I/O scheduler"></a>I/O scheduler</h2><ul><li><p>check current scheduler pattern</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/block/nvme[01]\*/queue/scheduler</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> console <span class="built_in">print</span> below info</span></span><br><span class="line">none [mq-deadline] kyber bfq</span><br></pre></td></tr></table></figure><p>The <strong>scheduler currently in use</strong> is indicated by the square brackets [ ].</p><p><strong>Common NVMe I/O Schedulers</strong></p><ol><li><p><strong>none</strong> – Default for NVMe, provides minimal latency.</p></li><li><p><strong>mq-deadline</strong> – Multi-queue deadline scheduler, balances fairness and performance.</p></li><li><p><strong>kyber</strong> – Optimized for high-performance SSDs.</p></li><li><p><strong>bfq</strong> – Budget Fair Queueing, useful for low-latency workloads.</p></li></ol></li><li><p>Change the I/O Scheduler</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To <span class="built_in">set</span> the none scheduler <span class="keyword">for</span> nvme0n1, use:</span></span><br><span class="line">echo none | sudo tee /sys/block/nvme0n1/queue/scheduler</span><br></pre></td></tr></table></figure><p><strong>When to Change the Scheduler?</strong></p><ul><li><p>Use none <strong>(default)</strong> if latency is the priority (most NVMe drives handle queuing internally).</p></li><li><p>Use mq-deadline if fairness in I/O operations is needed.</p></li><li><p>Use kyber for workloads requiring fast response time.</p></li><li><p>Use bfq for interactive desktop workloads.</p></li></ul></li></ul><h1 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h1><h2 id="open-file-size"><a href="#open-file-size" class="headerlink" title="open file size"></a>open file size</h2><blockquote><p>To configure the <strong>maximum open file size</strong> (ulimit) for a specific user </p></blockquote><ul><li><p>Check Current Limits</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To see the current limits <span class="keyword">for</span> a user:</span></span><br><span class="line">ulimit -n # Check open file limit</span><br><span class="line">ulimit -a # Check all limits</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> To check system-wide limits:</span></span><br><span class="line">cat /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure></li><li><p>Set Open File Limits for a Specific User</p><blockquote><p>The changes in limits.conf are only applied to new login sessions</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/security/limits.conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the following lines (replace username with the actual user):</span></span><br><span class="line">username soft nofile 65535</span><br><span class="line">username hard nofile 65535</span><br></pre></td></tr></table></figure></li></ul><p>​    <strong>soft</strong>: The default limit the user gets.</p><p>​    <strong>hard</strong>: The maximum limit a user can set.</p><table><thead><tr><th>Feature</th><th>/etc/security/limits.conf</th><th>/etc/security/limits.d/*.conf</th></tr></thead><tbody><tr><td><strong>Default File</strong></td><td>Yes</td><td>No (Custom files in limits.d directory)</td></tr><tr><td><strong>Priority</strong></td><td>Read first</td><td>Read after limits.conf</td></tr><tr><td><strong>Organization</strong></td><td>All rules in one file</td><td>Modular approach, one file per service or user</td></tr><tr><td><strong>Example</strong></td><td><code>* soft nofile 65536</code></td><td><code>dongw soft nofile 65536</code> (in <code>/etc/security/limits.d/dongw.conf</code>)</td></tr><tr><td><strong>Syntax</strong></td><td>Same syntax for both files</td><td>Same syntax as limits.conf</td></tr></tbody></table><h1 id="HP-and-THP"><a href="#HP-and-THP" class="headerlink" title="HP and THP"></a>HP and THP</h1><blockquote><p>Huge Pages (HP) and Transparent Huge Pages (THP)</p></blockquote><p>Understanding Huge Pages (HP) and Transparent Huge Pages (THP)</p><p><strong>Huge Pages (HP)</strong>: A manually configured fixed-size memory allocation system, beneficial for workloads that require large contiguous memory allocations.</p><p><strong>Transparent Huge Pages (THP)</strong>: An automated memory management feature that dynamically allocates large memory pages based on usage patterns.</p><p>for performance-sensitive applications, THP can sometimes cause performance issues due to fragmentation and unexpected latency spikes. Thus, it’s often recommended to <strong>disable THP</strong> and manually configure HP.</p><ul><li><p>Checking Current Huge Pages Configuration</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/meminfo | grep HugePages</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Example output:</span></span><br><span class="line">AnonHugePages:  16850944 kB # Memory allocated via THP.</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">HugePages_Total:       0 # Number of configured huge pages.</span><br><span class="line">HugePages_Free:        0 # Available huge pages.</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br></pre></td></tr></table></figure></li><li><p>Configuring Static Huge Pages</p><p><strong>Step 1: Set the Number of Huge Pages</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To allocate a specific number of 2MB huge pages, calculate based on your memory requirements. Example:</span></span><br><span class="line">echo 1024 | sudo tee /proc/sys/vm/nr_hugepages</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Make it persistent:</span></span><br><span class="line">echo &quot;vm.nr_hugepages=1024&quot; | sudo tee -a /etc/sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p><strong>Step 2: Allocate Huge Pages at Boot (Recommended)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/default/grub</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add hugepages kernel parameter:</span></span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;default_hugepagesz=2M hugepagesz=2M hugepages=1024&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Update GRUB:</span></span><br><span class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Reboot the system:</span></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p><strong>Step 3: Mount Huge Pages File System (Optional)</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To <span class="built_in">enable</span> shared access to huge pages:</span></span><br><span class="line">mkdir -p /mnt/huge</span><br><span class="line">mount -t hugetlbfs nodev /mnt/huge</span><br><span class="line"><span class="meta">#</span><span class="bash"> To make this persistent, add to /etc/fstab:</span></span><br><span class="line">nodev /mnt/huge hugetlbfs defaults 0 0</span><br></pre></td></tr></table></figure></li><li><p>Disabling Transparent Huge Pages (THP)</p><p>THP should be <strong>disabled</strong> for performance-sensitive applications</p><p><strong>Step 1: Disable THP at Runtime</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">echo never | sudo tee /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure><p><strong>Step 2: Make THP Disabled at Boot</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/default/grub</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add:</span></span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;transparent_hugepage=never&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Update GRUB:</span></span><br><span class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Reboot:</span></span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></li><li><p>Verification After Reboot</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Check Huge Pages Allocation</span></span><br><span class="line">cat /proc/meminfo | grep HugePages</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check THP Status</span></span><br><span class="line">cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> Expected output:</span></span><br><span class="line">[always] madvise never</span><br></pre></td></tr></table></figure></li></ul><p>Summary: Should You Configure HP &amp; THP Together?</p><p>For performance-sensitive applications, it’s recommended to <strong>disable THP</strong> and <strong>manually configure Huge Pages (HP)</strong> for better performance.</p><p>THP can cause latency spikes and memory fragmentation, making it less ideal for high-performance file systems.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;cpu&quot;&gt;&lt;a href=&quot;#cpu&quot; class=&quot;headerlink&quot; title=&quot;cpu&quot;&gt;&lt;/a&gt;cpu&lt;/h1&gt;&lt;h2 id=&quot;governor-strategy&quot;&gt;&lt;a href=&quot;#governor-strategy&quot; class=&quot;header</summary>
      
    
    
    
    <category term="linux" scheme="https://www.willshirley.top/categories/linux/"/>
    
    
    <category term="tune" scheme="https://www.willshirley.top/tags/tune/"/>
    
  </entry>
  
  <entry>
    <title>nvme</title>
    <link href="https://www.willshirley.top/2025/01/17/fs:%20nvme/"/>
    <id>https://www.willshirley.top/2025/01/17/fs:%20nvme/</id>
    <published>2025-01-17T04:21:01.000Z</published>
    <updated>2025-01-17T04:34:14.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NVMe"><a href="#NVMe" class="headerlink" title="NVMe"></a>NVMe</h1><p><strong>NVMe (Non-Volatile Memory Express)</strong> is a high-performance storage protocol designed to take full advantage of the speed and efficiency offered by modern solid-state drives (SSDs), particularly those using <strong>NAND flash</strong> or <strong>3D NAND</strong> memory. It was developed to overcome the limitations of older storage protocols (such as SATA and SAS) by optimizing the interaction between the storage device and the CPU over faster interfaces like <strong>PCIe (Peripheral Component Interconnect Express)</strong>.</p><h2 id="1-What-NVMe-is"><a href="#1-What-NVMe-is" class="headerlink" title="1. What NVMe is"></a>1. What NVMe is</h2><p>NVMe is both a <strong>protocol</strong> and a <strong>standard</strong> for connecting <strong>non-volatile memory</strong> (such as NAND flash memory) directly to a computer system via high-speed interfaces, most commonly <strong>PCIe</strong>. It allows data to be transferred at much faster speeds compared to older storage protocols, enabling devices to reach their full potential and significantly reducing latency.</p><h2 id="2-Key-Advantages-of-NVMe"><a href="#2-Key-Advantages-of-NVMe" class="headerlink" title="2. Key Advantages of NVMe"></a>2. Key Advantages of NVMe</h2><ul><li><p><strong>Faster Speeds</strong>: NVMe takes full advantage of the high throughput and low latency of <strong>PCIe lanes</strong>. For example, the PCIe Gen 3.0 standard provides a maximum theoretical speed of about <strong>32 GB/s</strong> (using 16 lanes), while PCIe Gen 4.0 can offer speeds up to <strong>64 GB/s</strong>. NVMe drives can use multiple PCIe lanes (usually 4 or more) to transfer data simultaneously, making them vastly faster than older SATA SSDs or hard disk drives (HDDs).</p></li><li><p><strong>Low Latency</strong>: NVMe’s protocol is optimized to handle thousands of input/output operations per second (IOPS), resulting in very low latency (often measured in microseconds). This is crucial for applications requiring rapid access to large datasets, such as databases or high-performance computing tasks.</p></li><li><p><strong>Parallelism</strong>: NVMe is designed to support <strong>multi-core processors</strong> and can handle many more <strong>queues</strong> and <strong>commands</strong> simultaneously than previous protocols. It can support <strong>64K queues</strong>, each with up to <strong>64K commands</strong>, which is a huge improvement over the older <strong>SATA</strong> interface, which could handle only one queue with a limited number of commands.</p></li><li><p><strong>Efficiency</strong>: NVMe uses a streamlined protocol that minimizes overhead. SATA and SAS, by comparison, were originally designed for spinning hard drives and include more layers of abstraction that slow down communication.</p></li></ul><h2 id="3-NVMe-vs-SATA-SAS"><a href="#3-NVMe-vs-SATA-SAS" class="headerlink" title="3. NVMe vs. SATA/SAS"></a>3. NVMe vs. SATA/SAS</h2><ul><li><p><strong>SATA</strong> (Serial Advanced Technology Attachment) is an older protocol used mainly for connecting hard drives and SSDs. It’s limited by the speed of the interface itself (roughly <strong>600 MB/s</strong> max in the case of SATA III) and was designed when mechanical HDDs were the primary storage medium. Even with SSDs, the SATA interface is a bottleneck.</p></li><li><p><strong>SAS</strong> (Serial Attached SCSI) is often used in enterprise environments for high-reliability storage solutions, but it, too, has limitations in speed and efficiency compared to NVMe.</p></li></ul><h2 id="4-How-NVMe-Works"><a href="#4-How-NVMe-Works" class="headerlink" title="4. How NVMe Works"></a>4. How NVMe Works</h2><p>NVMe devices communicate directly with the CPU via PCIe, bypassing older storage controllers. The <strong>PCIe interface</strong> connects storage devices such as <strong>NVMe SSDs</strong> directly to the motherboard or via expansion cards, allowing faster and more direct data transfer.</p><p>NVMe itself is built around a <strong>command set</strong> that is designed for <strong>flash memory</strong>, as opposed to the traditional command sets used in older storage systems. This reduces unnecessary overhead and allows NVMe devices to process requests more quickly.</p><h2 id="5-NVMe-Form-Factors"><a href="#5-NVMe-Form-Factors" class="headerlink" title="5. NVMe Form Factors"></a>5. NVMe Form Factors</h2><p>There are several physical form factors for NVMe drives, including:</p><ul><li><p><strong>M.2</strong>: A small form factor typically used in laptops and desktops. It plugs directly into an M.2 slot on the motherboard, and modern M.2 NVMe drives can reach speeds of over <strong>7,000 MB/s</strong> with PCIe Gen 3.0 or Gen 4.0 support.</p></li><li><p><strong>U.2</strong>: A connector used for enterprise-class SSDs. These drives are often used in servers and data centers.</p></li><li><p><strong>Add-in Card (AIC)</strong>: These are full-sized PCIe cards that can be inserted into a motherboard’s PCIe slot. They offer high storage capacity and performance, making them ideal for high-end workstations or servers.</p></li></ul><h2 id="6-Applications-of-NVMe"><a href="#6-Applications-of-NVMe" class="headerlink" title="6. Applications of NVMe"></a>6. Applications of NVMe</h2><ul><li><p><strong>Gaming</strong>: NVMe SSDs dramatically reduce game load times, texture rendering, and video streaming, providing a smoother gaming experience.</p></li><li><p><strong>Content Creation</strong>: Video editing, 3D rendering, and other high-bandwidth tasks benefit from the rapid access to large files.</p></li><li><p><strong>Data Centers</strong>: NVMe drives are ideal for enterprise storage solutions, especially when handling high-volume data throughput, real-time analytics, or AI workloads.</p></li><li><p><strong>Databases</strong>: NVMe’s low latency and high throughput make it an excellent choice for applications involving large databases and data warehousing.</p></li></ul><h2 id="7-NVMe-and-PCIe-Versions"><a href="#7-NVMe-and-PCIe-Versions" class="headerlink" title="7. NVMe and PCIe Versions"></a>7. NVMe and PCIe Versions</h2><p>NVMe’s performance scales with <strong>PCIe</strong> versions:</p><ul><li><p><strong>PCIe 3.0</strong> supports a maximum theoretical bandwidth of <strong>1 GB/s per lane</strong> (total of <strong>4 GB/s</strong> for a 4-lane SSD).</p></li><li><p><strong>PCIe 4.0</strong> doubles this bandwidth to <strong>2 GB/s per lane</strong> (total of <strong>8 GB/s</strong> for a 4-lane SSD).</p></li><li><p><strong>PCIe 5.0</strong> further increases the bandwidth to <strong>4 GB/s per lane</strong> (total of <strong>16 GB/s</strong> for a 4-lane SSD).</p></li></ul><h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8. Conclusion"></a>8. Conclusion</h2><p>NVMe is revolutionizing storage by offering higher speeds, lower latency, and better scalability compared to older protocols. Whether for consumer laptops, high-performance desktops, or enterprise-level data centers, NVMe is fast becoming the go-to solution for anyone who demands top-tier storage performance.</p><h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><p>The relationship between <strong>NVMe</strong> and <strong>SSD</strong> is one of complementary technologies. Here’s a breakdown of how the two are related and how they differ:</p><h2 id="1-What-is-an-SSD"><a href="#1-What-is-an-SSD" class="headerlink" title="1. What is an SSD?"></a>1. What is an SSD?</h2><p><strong>SSD (Solid-State Drive)</strong> refers to a type of storage device that uses <strong>non-volatile flash memory</strong> (usually NAND flash) to store data, rather than the spinning platters and moving heads of traditional <strong>HDDs (Hard Disk Drives)</strong>. SSDs are much faster, more reliable, and use less power than HDDs, making them a popular choice for both consumer and enterprise storage solutions.</p><p><strong>SSDs can use various interfaces and protocols</strong> to communicate with the computer system. These include:</p><ul><li><p><strong>SATA (Serial ATA)</strong>: An older protocol, often used for <strong>SATA-based SSDs</strong>. While fast compared to HDDs, it’s limited by the bandwidth of the SATA interface (roughly <strong>600 MB/s</strong>).</p></li><li><p><strong>SAS (Serial Attached SCSI)</strong>: A more advanced, enterprise-grade protocol used for connecting SSDs in servers.</p></li><li><p><strong>PCIe (Peripheral Component Interconnect Express)</strong>: A high-speed interface used by <strong>NVMe SSDs</strong>.</p></li></ul><h2 id="2-How-NVMe-and-SSDs-are-Related"><a href="#2-How-NVMe-and-SSDs-are-Related" class="headerlink" title="2. How NVMe and SSDs are Related"></a>2. How NVMe and SSDs are Related</h2><p>NVMe and SSDs are connected in the following way:</p><ul><li><p><strong>NVMe is a protocol for accessing and transferring data from an SSD</strong>. It is the language or set of instructions that defines how data moves between the SSD and the computer’s CPU.</p></li><li><p><strong>SSD is the storage device</strong>, and it can use different interfaces and protocols (SATA, SAS, or PCIe). <strong>NVMe SSDs</strong> are SSDs that use the <strong>NVMe protocol over the PCIe interface</strong> to provide high-speed data transfer.</p></li></ul><h2 id="3-Key-Differences-Between-NVMe-SSDs-and-Other-SSDs"><a href="#3-Key-Differences-Between-NVMe-SSDs-and-Other-SSDs" class="headerlink" title="3. Key Differences Between NVMe SSDs and Other SSDs"></a>3. Key Differences Between NVMe SSDs and Other SSDs</h2><p>The main difference between <strong>NVMe SSDs</strong> and <strong>non-NVMe SSDs</strong> (like <strong>SATA SSDs</strong>) lies in the <strong>interface</strong> and <strong>protocol</strong> they use for data transfer:</p><ul><li><p><strong>NVMe SSDs</strong>:</p><ul><li><p><strong>Interface</strong>: Use <strong>PCIe</strong> lanes to communicate with the system.</p></li><li><p><strong>Protocol</strong>: Use the <strong>NVMe protocol</strong> to provide faster, more efficient data transfers.</p></li><li><p><strong>Speed</strong>: Much faster than SATA-based SSDs, as PCIe supports higher throughput.</p></li><li><p><strong>Latency</strong>: Lower latency compared to SATA SSDs, as NVMe is optimized for flash memory and can handle many more I/O operations in parallel.</p></li><li><p><strong>Form Factors</strong>: NVMe SSDs typically come in <strong>M.2</strong>, <strong>U.2</strong>, or <strong>Add-in Card</strong> (AIC) form factors.</p></li></ul></li><li><p><strong>Non-NVMe SSDs</strong> (e.g., <strong>SATA SSDs</strong>):</p><ul><li><p><strong>Interface</strong>: Use the <strong>SATA</strong> interface, which was originally designed for spinning hard drives.</p></li><li><p><strong>Protocol</strong>: Use the older <strong>SATA protocol</strong>, which is slower than NVMe.</p></li><li><p><strong>Speed</strong>: Limited to <strong>around 600 MB/s</strong> (the maximum bandwidth of SATA III).</p></li><li><p><strong>Latency</strong>: Higher latency compared to NVMe SSDs because of the older protocol and interface.</p></li><li><p><strong>Form Factor</strong>: Commonly come in <strong>2.5-inch</strong> or <strong>mSATA</strong> form factors, similar to the size of traditional HDDs.</p></li></ul></li></ul><h2 id="4-Performance-Comparison"><a href="#4-Performance-Comparison" class="headerlink" title="4. Performance Comparison"></a><strong>4. Performance Comparison</strong></h2><ul><li><p><strong>SATA SSDs</strong>:</p><ul><li><p>Maximum read/write speeds are capped at around <strong>550-600 MB/s</strong> due to the limitations of the <strong>SATA III interface</strong>.</p></li><li><p>Suitable for everyday consumer use, including boot drives, gaming, and regular workloads, but not optimal for high-performance or data-intensive tasks.</p></li></ul></li><li><p><strong>NVMe SSDs</strong>:</p><ul><li><p>The <strong>PCIe 3.0</strong> interface provides <strong>up to 4 GB/s</strong> of bandwidth for a 4-lane SSD, while <strong>PCIe 4.0</strong> can go up to <strong>8 GB/s</strong> or higher for consumer drives.</p></li><li><p><strong>Latency</strong> is much lower compared to SATA, with NVMe SSDs often providing sub-millisecond access times.</p></li><li><p>Ideal for <strong>high-end gaming</strong>, <strong>video editing</strong>, <strong>3D rendering</strong>, and other professional or enterprise-level applications that require quick access to large datasets.</p></li></ul></li></ul><h2 id="5-Form-Factor-and-Compatibility"><a href="#5-Form-Factor-and-Compatibility" class="headerlink" title="5. Form Factor and Compatibility"></a>5. Form Factor and Compatibility</h2><ul><li><p><strong>SATA SSDs</strong>: Often found in the <strong>2.5-inch form factor</strong> (similar to HDDs) and can be used in most computers and laptops with a SATA interface.</p></li><li><p><strong>NVMe SSDs</strong>: Use <strong>M.2</strong> or <strong>U.2</strong> form factors (for consumer or enterprise-level SSDs) and require a motherboard with an <strong>M.2 PCIe slot</strong> or a <strong>U.2 connector</strong> for compatibility. Some systems may require an adapter to support NVMe drives.</p></li></ul><h2 id="6-Summary-of-the-Relationship-Between-NVMe-and-SSD"><a href="#6-Summary-of-the-Relationship-Between-NVMe-and-SSD" class="headerlink" title="6. Summary of the Relationship Between NVMe and SSD"></a>6. Summary of the Relationship Between NVMe and SSD</h2><ul><li><p><strong>NVMe</strong> is a <strong>protocol</strong> designed to maximize the speed and efficiency of <strong>PCIe-based SSDs</strong>.</p></li><li><p><strong>SSD</strong> is a type of storage device that can use different interfaces like SATA, SAS, or PCIe, with <strong>NVMe SSDs</strong> specifically referring to <strong>SSDs that use the NVMe protocol</strong> over a <strong>PCIe interface</strong>.</p></li><li><p><strong>NVMe SSDs</strong> offer significant performance improvements (speed, latency, and scalability) over <strong>SATA SSDs</strong>, making them ideal for users with high-performance storage needs, such as gamers, content creators, and data centers.</p></li></ul><p>In short: <strong>NVMe SSD</strong> refers to a <strong>high-speed SSD</strong> that uses the <strong>NVMe protocol</strong> and <strong>PCIe interface</strong> for faster data transfer, while <strong>SATA SSDs</strong> use the older <strong>SATA interface</strong> and protocol, which are slower in comparison.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NVMe&quot;&gt;&lt;a href=&quot;#NVMe&quot; class=&quot;headerlink&quot; title=&quot;NVMe&quot;&gt;&lt;/a&gt;NVMe&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;NVMe (Non-Volatile Memory Express)&lt;/strong&gt; is a high-</summary>
      
    
    
    
    <category term="fs" scheme="https://www.willshirley.top/categories/fs/"/>
    
    
    <category term="nvme" scheme="https://www.willshirley.top/tags/nvme/"/>
    
  </entry>
  
  <entry>
    <title>ceph storage cluster</title>
    <link href="https://www.willshirley.top/2025/01/16/ceph%20storage/"/>
    <id>https://www.willshirley.top/2025/01/16/ceph%20storage/</id>
    <published>2025-01-16T06:57:29.000Z</published>
    <updated>2025-02-08T03:40:08.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><p>A Ceph Storage Cluster requires the following: at least one Ceph Monitor and at least one Ceph Manager, and at least as many <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-OSD">Ceph Object Storage Daemon</a>s (OSDs) as there are copies of a given object stored in the Ceph cluster (for example, if three copies of a given object are stored in the Ceph cluster, then at least three OSDs must exist in that Ceph cluster).</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    %% Define Ceph Components</span><br><span class="line">    subgraph &quot;Ceph Cluster&quot;</span><br><span class="line">        direction TB</span><br><span class="line">        OSDs[&quot;OSDs - Object Storage Daemons\n(Store and replicate data)&quot;]</span><br><span class="line">        Monitors[&quot;Monitors (MON) - Cluster Coordination\n(Maintain cluster map &amp; quorum)&quot;]</span><br><span class="line">        Managers[&quot;Managers (MGR) - Metrics &amp; Control\n(Provide monitoring, orchestration, and services)&quot;]</span><br><span class="line">        MDSs[&quot;MDSs - Metadata Servers\n(Manage CephFS metadata)&quot;]</span><br><span class="line">        RADOS[&quot;RADOS - Reliable Autonomic Distributed Object Store\n(Core distributed storage layer)&quot;]</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    %% Relationships</span><br><span class="line">    OSDs --&gt;|Store &amp; Replicate Data| RADOS</span><br><span class="line">    Monitors --&gt;|Maintain Cluster State &amp; Quorum| RADOS</span><br><span class="line">    Managers --&gt;|Monitor, Orchestrate &amp; Provide Services| RADOS</span><br><span class="line">    MDSs --&gt;|Manage CephFS Metadata| RADOS</span><br><span class="line">    RADOS --&gt;|Foundation of Ceph Storage| Ceph-Cluster</span><br><span class="line"></span><br><span class="line">    %% Descriptions</span><br><span class="line">    subgraph &quot;Component Descriptions&quot;</span><br><span class="line">        direction TB</span><br><span class="line">        MDSs_Description[&quot;MDSs: Handle metadata for CephFS, enabling file system operations like directory listing and file creation.&quot;]</span><br><span class="line">        RADOS_Description[&quot;RADOS: Core storage layer ensuring data durability, fault tolerance, and scalability.&quot;]</span><br><span class="line">        OSDs_Description[&quot;OSDs: Store, replicate, and recover object data across the cluster.&quot;]</span><br><span class="line">        Monitors_Description[&quot;Monitors: Maintain cluster state, handle authentication, and ensure consensus.&quot;]</span><br><span class="line">        Managers_Description[&quot;Managers: Provide additional cluster management features, including dashboards, orchestration, and monitoring.&quot;]</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    %% Link Descriptions to Components</span><br><span class="line">    MDSs --&gt; MDSs_Description</span><br><span class="line">    RADOS --&gt; RADOS_Description</span><br><span class="line">    OSDs --&gt; OSDs_Description</span><br><span class="line">    Monitors --&gt; Monitors_Description</span><br><span class="line">    Managers --&gt; Managers_Description</span><br></pre></td></tr></table></figure><ul><li><p><strong>Monitors</strong>: A <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Monitor">Ceph Monitor</a> (<code>ceph-mon</code>) maintains maps of the cluster state, including the <a href="https://docs.ceph.com/en/reef/rados/operations/monitoring/#display-mon-map">monitor map</a>, manager map, the OSD map, the MDS map, and the CRUSH map. These maps are critical cluster state required for Ceph daemons to coordinate with each other. Monitors are also responsible for managing authentication between daemons and clients. At least three monitors are normally required for redundancy and high availability.</p></li><li><p><strong>Managers</strong>: A <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Manager">Ceph Manager</a> daemon (<code>ceph-mgr</code>) is responsible for keeping track of runtime metrics and the current state of the Ceph cluster, including storage utilization, current performance metrics, and system load. The Ceph Manager daemons also host python-based modules to manage and expose Ceph cluster information, including a web-based <a href="https://docs.ceph.com/en/reef/mgr/dashboard/#mgr-dashboard">Ceph Dashboard</a> and <a href="https://docs.ceph.com/en/mgr/restful">REST API</a>. At least two managers are normally required for high availability.</p></li><li><p><strong>Ceph OSDs</strong>: An Object Storage Daemon (<a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-OSD">Ceph OSD</a>, <code>ceph-osd</code>) stores data, handles data replication, recovery, rebalancing, and provides some monitoring information to Ceph Monitors and Managers by checking other Ceph OSD Daemons for a heartbeat. At least three Ceph OSDs are normally required for redundancy and high availability.</p></li><li><p><strong>MDSs</strong>: A <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Metadata-Server">Ceph Metadata Server</a> (MDS, <code>ceph-mds</code>) stores metadata for the <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-File-System">Ceph File System</a>. Ceph Metadata Servers allow CephFS users to run basic commands (like <code>ls</code>, <code>find</code>, etc.) without placing a burden on the Ceph Storage Cluster.</p></li><li><p><strong>RADOS</strong>: (Reliable Autonomic Distributed Object Store) Any Cluster that supports <a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Object-Storage">Ceph Object Storage</a> runs Ceph RADOS Gateway daemons (<code>radosgw</code>).</p></li></ul><h1 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h1><img src="/images/ceph/stack.webp" style="zoom: 60%"><h2 id="ceph-block-device"><a href="#ceph-block-device" class="headerlink" title="ceph block device"></a>ceph block device</h2><blockquote><p>RBD (Ceph Block Devices) </p></blockquote><img src="/images/ceph/block-device.webp" style="zoom: 60%"><p>Ceph block devices are thin-provisioned, resizable, and store data striped over multiple OSDs. Ceph block devices leverage RADOS capabilities including snapshotting, replication and strong consistency. Ceph block storage clients communicate with Ceph clusters through kernel modules or the <code>librbd</code> library.</p><h2 id="ceph-object-storage"><a href="#ceph-object-storage" class="headerlink" title="ceph object storage"></a>ceph object storage</h2><blockquote><p>RGW (Ceph Object Storage Gateway)</p></blockquote><img src="/images/ceph/object-storage.webp" style="zoom: 60%"><p>Ceph Object Storage uses the Ceph Object Gateway daemon (<code>radosgw</code>), an HTTP server designed to interact with a Ceph Storage Cluster. The Ceph Object Gateway provides interfaces that are compatible with both Amazon S3 and OpenStack Swift, and it has its own user management. Ceph Object Gateway can use a single Ceph Storage cluster to store data from Ceph File System and from Ceph Block device clients. The S3 API and the Swift API share a common namespace, which means that it is possible to write data to a Ceph Storage Cluster with one API and then retrieve that data with the other API.</p><h2 id="ceph-file-system"><a href="#ceph-file-system" class="headerlink" title="ceph file system"></a>ceph file system</h2><blockquote><p>cephFS</p></blockquote><img src="/images/ceph/cephfs.svg" style="zoom: 40%"><p>The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, <strong>RADOS</strong>. CephFS endeavors to provide a state-of-the-art, multi-use, highly available, and performant file store for a variety of applications, including traditional use-cases like shared home directories, HPC scratch space, and distributed workflow shared storage.</p><h1 id="Storage-Interfaces"><a href="#Storage-Interfaces" class="headerlink" title="Storage Interfaces"></a>Storage Interfaces</h1><p>Ceph offers several “storage interfaces”, which is another way of saying “ways of storing data”. These storage interfaces include: - <strong>CephFS</strong>(a file system) - <strong>RBD</strong> (block devices) - <strong>RADOS</strong> (an object store).</p><p>Deep down, though, all three of these are really RADOS object stores. CephFS and RBD are just presenting themselves as file systems and block devices.</p><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><h2 id="pools-vs-placement-groups-vs-object"><a href="#pools-vs-placement-groups-vs-object" class="headerlink" title="pools vs placement groups vs object"></a>pools vs placement groups vs object</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">graph TD;</span><br><span class="line">    </span><br><span class="line">    subgraph Ceph Cluster</span><br><span class="line">        A[Pool: mypool] --&gt;|Contains| B1[PG 1]</span><br><span class="line">        A[Pool: mypool] --&gt;|Contains| B2[PG 2]</span><br><span class="line">        A[Pool: mypool] --&gt;|Contains| B3[PG 3]</span><br><span class="line">        </span><br><span class="line">        B1 --&gt;|Stores| C1[Object A]</span><br><span class="line">        B1 --&gt;|Stores| C2[Object B]</span><br><span class="line">        </span><br><span class="line">        B2 --&gt;|Stores| C3[Object C]</span><br><span class="line">        B2 --&gt;|Stores| C4[Object D]</span><br><span class="line">        </span><br><span class="line">        B3 --&gt;|Stores| C5[Object E]</span><br><span class="line">        </span><br><span class="line">        subgraph OSD Nodes</span><br><span class="line">            D1[OSD.1] </span><br><span class="line">            D2[OSD.2] </span><br><span class="line">            D3[OSD.3] </span><br><span class="line">            D4[OSD.4] </span><br><span class="line">            D5[OSD.5] </span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        B1 --&gt;|Mapped to| D1</span><br><span class="line">        B1 --&gt;|Mapped to| D2</span><br><span class="line">        B1 --&gt;|Mapped to| D3</span><br><span class="line">        </span><br><span class="line">        B2 --&gt;|Mapped to| D2</span><br><span class="line">        B2 --&gt;|Mapped to| D3</span><br><span class="line">        B2 --&gt;|Mapped to| D4</span><br><span class="line">        </span><br><span class="line">        B3 --&gt;|Mapped to| D3</span><br><span class="line">        B3 --&gt;|Mapped to| D4</span><br><span class="line">        B3 --&gt;|Mapped to| D5</span><br><span class="line">    end</span><br></pre></td></tr></table></figure><blockquote><p><strong>Pool (mypool)</strong> contains <strong>multiple PGs</strong> (Placement Groups).</p><p>Each <strong>PG stores multiple objects</strong>.</p><p>Each <strong>PG is mapped to multiple OSDs</strong>, based on the replication factor.</p><p><strong>Objects are stored within PGs</strong>, and Ceph automatically balances them across OSDs.</p></blockquote><p><strong>Understanding the Relationship Between Placement Groups (PGs), Pools, and Objects in Ceph</strong></p><p>In Ceph, <strong>objects</strong>, <strong>placement groups (PGs)</strong>, and <strong>pools</strong> work together to distribute and store data efficiently. Here’s how they relate:</p><h3 id="Pools"><a href="#Pools" class="headerlink" title="Pools"></a>Pools</h3><blockquote><p>The Logical Storage Units</p></blockquote><p>A pool in Ceph is a logical container that stores objects. Pools are used for different types of storage, such as:</p><ul><li><p><strong>RADOS block storage (RBD)</strong></p></li><li><p><strong>CephFS (Ceph File System)</strong></p></li><li><p><strong>RGW (RADOS Gateway for S3/Swift compatibility)</strong></p></li></ul><p>A pool has properties like:</p><ul><li><p><strong>Replication factor (e.g., 3x replication)</strong></p></li><li><p><strong>Erasure coding settings</strong></p></li><li><p><strong>Number of placement groups (PGs)</strong></p></li></ul><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create mypool 128</span><br></pre></td></tr></table></figure><p>This creates a pool named <strong>mypool</strong> with <strong>128 PGs</strong>.</p><h3 id="Placement-Groups-PGs"><a href="#Placement-Groups-PGs" class="headerlink" title="Placement Groups (PGs)"></a>Placement Groups (PGs)</h3><blockquote><p>Buckets for Objects</p></blockquote><p>A <strong>placement group (PG)</strong> is an internal grouping of objects within a pool. Instead of distributing objects directly across OSDs, Ceph first assigns objects to PGs. Then, PGs are mapped to OSDs.</p><p><strong>Why PGs?</strong></p><ul><li><p>They act as <strong>intermediaries</strong> between objects and OSDs.</p></li><li><p>They help balance data across OSDs efficiently.</p></li><li><p>They make recovery and replication easier.</p></li></ul><p><strong>PG Calculation Example</strong></p><p>If a pool has <strong>128 PGs</strong> and uses a <strong>3-replica</strong> setting, each PG will be mapped to 3 different OSDs.</p><h3 id="Objects"><a href="#Objects" class="headerlink" title="Objects"></a>Objects</h3><blockquote><p>The Data Units in Ceph</p></blockquote><p>An <strong>object</strong> is the fundamental data unit in Ceph. When you store a file in Ceph, it’s broken into one or more objects.</p><p><strong>How Objects Are Stored?</strong></p><ol><li><p><strong>Objects are stored in a Pool</strong> (e.g., mypool).</p></li><li><p><strong>Each object is mapped to a Placement Group (PG)</strong>.</p></li><li><p><strong>The PG is mapped to multiple OSDs</strong> (based on the pool’s replication settings).</p></li><li><p><strong>The primary OSD</strong> handles writes and ensures copies are stored on secondary OSDs.</p></li></ol><p><strong>Example Workflow:</strong></p><ul><li><p>A file is stored in <strong>RGW (Ceph S3) → mypool</strong>.</p></li><li><p>The file is broken into <strong>objects</strong>.</p></li><li><p>Each object is assigned to a <strong>PG</strong>.</p></li><li><p>The PG is mapped to <strong>3 OSDs</strong> (in a replicated pool).</p></li></ul><h3 id="Putting-It-All-Together"><a href="#Putting-It-All-Together" class="headerlink" title="Putting It All Together"></a>Putting It All Together</h3><p><strong>Example</strong></p><p>Assume:</p><ul><li><p>You create a <strong>pool</strong> called mypool with <strong>128 PGs</strong>.</p></li><li><p>You upload a file that gets broken into <strong>10 objects</strong>.</p></li><li><p>Each object is <strong>hashed and assigned</strong> to a PG.</p></li><li><p>The PGs are <strong>mapped to OSDs</strong>.</p></li></ul><table><thead><tr><th><strong>Component</strong></th><th><strong>Example Value</strong></th></tr></thead><tbody><tr><td><strong>Pool</strong></td><td>mypool</td></tr><tr><td><strong>PG Count</strong></td><td>128</td></tr><tr><td><strong>Object Name</strong></td><td>obj12345</td></tr><tr><td><strong>Object Hash</strong></td><td>hash(obj12345) → PG 45</td></tr><tr><td><strong>PG Assigned</strong></td><td>PG 45</td></tr><tr><td><strong>Mapped OSDs</strong></td><td>OSD.3, OSD.7, OSD.12</td></tr></tbody></table><p>In this case, <strong>obj12345</strong> will be stored in <strong>PG 45</strong>, which is replicated across <strong>OSD.3, OSD.7, and OSD.12</strong>.</p><p><strong>Key Takeaways</strong></p><ul><li><p><strong>Objects</strong> are stored in <strong>pools</strong>.</p></li><li><p><strong>Pools</strong> contain <strong>placement groups (PGs)</strong>.</p></li><li><p><strong>PGs</strong> group multiple objects and map them to multiple OSDs.</p></li><li><p><strong>OSDs</strong> store object data and handle replication.</p></li></ul><p><strong>Commands to Check PGs and Pools</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> List all pools:</span></span><br><span class="line">ceph osd pool ls</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check PG status:</span></span><br><span class="line">ceph pg stat</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See <span class="built_in">which</span> PGs belong to a pool:</span></span><br><span class="line">ceph pg dump | grep mypool</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check object distribution:</span></span><br><span class="line">ceph osd df</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;overview&quot;&gt;&lt;a href=&quot;#overview&quot; class=&quot;headerlink&quot; title=&quot;overview&quot;&gt;&lt;/a&gt;overview&lt;/h1&gt;&lt;p&gt;A Ceph Storage Cluster requires the following:</summary>
      
    
    
    
    <category term="ceph" scheme="https://www.willshirley.top/categories/ceph/"/>
    
    
    <category term="storage" scheme="https://www.willshirley.top/tags/storage/"/>
    
  </entry>
  
  <entry>
    <title>terminology</title>
    <link href="https://www.willshirley.top/2024/12/19/terminology/"/>
    <id>https://www.willshirley.top/2024/12/19/terminology/</id>
    <published>2024-12-19T07:52:40.000Z</published>
    <updated>2024-12-19T07:53:34.191Z</updated>
    
    <content type="html"><![CDATA[<h1 id="幂等-非幂等"><a href="#幂等-非幂等" class="headerlink" title="幂等/非幂等"></a>幂等/非幂等</h1><p>在计算机科学和网络通信中，幂等（Idempotent）和非幂等（Non-idempotent）描述了操作执行多次和执行一次的效果是否相同。</p><ol><li><p><strong>幂等操作</strong>：</p><ul><li><strong>定义</strong>：如果一个操作执行多次和执行一次的效果相同，那么这个操作被称为幂等操作。</li><li><strong>特点</strong>：幂等操作在重复执行时不会改变系统状态，即多次执行和执行一次的结果是相同的。</li><li><strong>例子</strong>：<ul><li><strong>GET请求</strong>：读取资源的操作是幂等的，因为无论读取多少次，资源的内容不会改变。</li><li><strong>PUT请求</strong>：更新资源的操作通常是幂等的，因为多次更新同一个资源到相同的状态，最终资源的状态不会改变。</li><li><strong>DELETE请求</strong>：删除资源的操作也是幂等的，因为一旦资源被删除，再次删除不会对系统产生进一步的影响。</li></ul></li></ul></li><li><p><strong>非幂等操作</strong>：</p><ul><li><strong>定义</strong>：如果一个操作执行多次和执行一次的效果不同，那么这个操作被称为非幂等操作。</li><li><strong>特点</strong>：非幂等操作在重复执行时会改变系统状态，即多次执行和执行一次的结果不同。</li><li><strong>例子</strong>：<ul><li><strong>POST请求</strong>：创建资源的操作是非幂等的，因为多次执行创建操作会创建多个资源实例。</li><li><strong>某些类型的UPDATE请求</strong>：如果更新操作涉及到计数器或其他会随时间变化的值，那么这些操作可能是非幂等的，因为多次更新可能会累积效果。</li></ul></li></ul></li></ol><p>在分布式系统和网络通信中，幂等性是一个重要的属性，因为它可以帮助确保系统的一致性和可靠性。例如，在网络请求中，如果一个请求由于网络问题被重复发送，幂等操作可以确保系统不会因为重复的请求而产生不一致的状态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;幂等-非幂等&quot;&gt;&lt;a href=&quot;#幂等-非幂等&quot; class=&quot;headerlink&quot; title=&quot;幂等/非幂等&quot;&gt;&lt;/a&gt;幂等/非幂等&lt;/h1&gt;&lt;p&gt;在计算机科学和网络通信中，幂等（Idempotent）和非幂等（Non-idempotent）描述了操作执行</summary>
      
    
    
    
    <category term="basic" scheme="https://www.willshirley.top/categories/basic/"/>
    
    
    <category term="terminology" scheme="https://www.willshirley.top/tags/terminology/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes pkg</title>
    <link href="https://www.willshirley.top/2024/12/17/kubernetes%20pkg/"/>
    <id>https://www.willshirley.top/2024/12/17/kubernetes%20pkg/</id>
    <published>2024-12-17T02:47:36.000Z</published>
    <updated>2025-03-02T04:09:12.419Z</updated>
    
    <content type="html"><![CDATA[<h1 id="klog"><a href="#klog" class="headerlink" title="klog"></a>klog</h1><h2 id="default-log-level"><a href="#default-log-level" class="headerlink" title="default log level"></a>default log level</h2><p>default use <code>klog.Info</code> is equivalent to <code>klog.V(0).Info</code>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> by default klog uses level 0, so logs with log.V(1) won<span class="string">&#x27;t appear unless you set --v=1 or a higher verbosity.</span></span></span><br><span class="line"></span><br><span class="line">containers:</span><br><span class="line">  - name: your-container</span><br><span class="line">    image: your-image</span><br><span class="line">    args:</span><br><span class="line">      - &quot;--v=1&quot; # if not config this, default klog uses level 0</span><br></pre></td></tr></table></figure><ul><li>For more detailed debug or trace messages, use klog.V(level).Info, where level is greater than 0 (e.g., klog.V(2).Info).</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;klog&quot;&gt;&lt;a href=&quot;#klog&quot; class=&quot;headerlink&quot; title=&quot;klog&quot;&gt;&lt;/a&gt;klog&lt;/h1&gt;&lt;h2 id=&quot;default-log-level&quot;&gt;&lt;a href=&quot;#default-log-level&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Kubernetes" scheme="https://www.willshirley.top/categories/Kubernetes/"/>
    
    
    <category term="pkg" scheme="https://www.willshirley.top/tags/pkg/"/>
    
  </entry>
  
  <entry>
    <title>cmake</title>
    <link href="https://www.willshirley.top/2024/12/16/cmake/"/>
    <id>https://www.willshirley.top/2024/12/16/cmake/</id>
    <published>2024-12-16T03:43:12.000Z</published>
    <updated>2024-12-16T04:14:48.137Z</updated>
    
    <content type="html"><![CDATA[<p>TBD</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TBD&lt;/p&gt;
</summary>
      
    
    
    
    <category term="cmake" scheme="https://www.willshirley.top/categories/cmake/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes controller</title>
    <link href="https://www.willshirley.top/2024/12/15/kubernetes%20controller/"/>
    <id>https://www.willshirley.top/2024/12/15/kubernetes%20controller/</id>
    <published>2024-12-15T14:20:05.000Z</published>
    <updated>2024-12-16T02:03:21.035Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>From <a href="https://buraksekili.github.io/articles/controller-runtime-1/">Diving into controller-runtime | Manager</a></p></blockquote><p><a href="https://github.com/kubernetes-sigs/controller-runtime"><code>controller-runtime</code></a> package has become a fundamental tool for most Kubernetes controllers, simplifying the creation of controllers to manage resources within a Kubernetes environment efficiently. Users tend to prefer it over <a href="https://github.com/kubernetes/client-go"><code>client-go</code></a>.</p><p>In Kubernetes, controllers observe resources, such as Deployments, in a control loop to ensure the cluster resources conform to the desired state specified in the resource specification (e.g., YAML files).</p><p>On the other hand, according to Redhat, a Kubernetes Operator is an application-specific controller <a href="https://buraksekili.github.io/articles/controller-runtime-1/#fn:2">2</a>. For instance, the Prometheus Operator manages the lifecycle of a Prometheus instance in the cluster, including managing configurations and updating Kubernetes resources, such as ConfigMaps.</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>controllers follow a special architecture to</p><ul><li>observe the resources,</li><li>inform any events (updating, deleting, adding) done on the resources,</li><li>keep a local cache to decrease the load on API Server,</li><li>keep a work queue to pick up events,</li><li>run workers to perform reconciliation on resources picked up from work queue.</li></ul><img src="/images/k8s/controller-runtime-1-client-go-controller-interaction.jpeg" style="zoom: 80%"><h2 id="informer"><a href="#informer" class="headerlink" title="informer"></a>informer</h2><p>Informers leverage certain components like <strong>Reflector</strong>, <strong>Queue</strong> and <strong>Indexer</strong>, as shown in the above diagram.</p><p>Informers watch Kubernetes API server to detect changes in resources that we want to. It keeps a local cache - in-memory cache implementing <a href="https://pkg.go.dev/k8s.io/client-go/tools/cache#Store">Store</a> interface - including the objects observed through Kubernetes API. Then controllers and operators use this cache for all getter requests - GET and LIST - to prevent load on Kubernetes API server. Moreover, Informers invoke controllers by sending objects to the controllers (registering Event Handlers).</p><h3 id="Reflector"><a href="#Reflector" class="headerlink" title="Reflector"></a>Reflector</h3><blockquote><p>Reflector watches a specified resource and causes all changes to be reflected in the given store.</p></blockquote><p>The store is actually a cache - with two options; simple one and FIFO. Reflector pushes objects to Delta Fifo queue.</p><p>By monitoring the server (Kubernetes API Server), the Reflector maintains a local cache of the resources. Upon any event occurring on the watched resource, implying a new operation on the Kubernetes resource, the Reflector updates the cache (Delta FIFO queue, as illustrated in the diagram). Subsequently, the Informer reads objects from this Delta FIFO queue, indexes them for future retrievals, and dispatches the object to the controller.</p><h3 id="Indexer"><a href="#Indexer" class="headerlink" title="Indexer"></a>Indexer</h3><p>Indexer saves objects into thread-safe Store by indexing the objects. This approach facilitates efficient querying of objects from the cache.</p><p>Custom indexers, based on specific needs, can be created. For example, a custom indexer can be generated to retrieve all objects based on certain fields, such as Annotations.</p><p>More details about how Kubernetes indexing works, check <a href="https://buraksekili.github.io/articles/client-k8s-indexing">Kubernetes Client-Side Indexing</a>.</p><hr><p>reference</p><ul><li><p><a href="https://www.zhaohuabing.com/post/2023-03-09-how-to-create-a-k8s-controller/">Kubernetes Controller 机制详解</a></p></li><li><p><a href="https://cloud.tencent.com/developer/article/1989055">Controller Runtime 的四种使用姿势</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;From &lt;a href=&quot;https://buraksekili.github.io/articles/controller-runtime-1/&quot;&gt;Diving into controller-runtime | Manager&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="Kubernetes" scheme="https://www.willshirley.top/categories/Kubernetes/"/>
    
    
    <category term="controller runtime" scheme="https://www.willshirley.top/tags/controller-runtime/"/>
    
  </entry>
  
  <entry>
    <title>GO PKG</title>
    <link href="https://www.willshirley.top/2024/12/13/golang%20pkg/"/>
    <id>https://www.willshirley.top/2024/12/13/golang%20pkg/</id>
    <published>2024-12-13T03:40:09.000Z</published>
    <updated>2025-02-13T07:22:32.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="fmt"><a href="#fmt" class="headerlink" title="fmt"></a>fmt</h1><ul><li><p><code>%v</code> vs <code>%#v</code></p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Example <span class="keyword">struct</span> &#123;</span><br><span class="line">    Name  <span class="keyword">string</span></span><br><span class="line">    Value <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    ex := Example&#123;Name: <span class="string">&quot;test&quot;</span>, Value: <span class="number">42</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Using %v</span></span><br><span class="line">    fmt.Printf(<span class="string">&quot;Default format: %v\n&quot;</span>, ex)</span><br><span class="line">    <span class="comment">// Output: Default format: &#123;test 42&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Using %#v</span></span><br><span class="line">    fmt.Printf(<span class="string">&quot;Go syntax representation: %#v\n&quot;</span>, ex)</span><br><span class="line">    <span class="comment">// Output: Go syntax representation: main.Example&#123;Name:&quot;test&quot;, Value:42&#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id="json"><a href="#json" class="headerlink" title="json"></a>json</h1><ul><li><p><code>json:&quot;,inline&quot;</code></p><blockquote><p>This means that the fields of the embedded struct will appear at the same level as the fields of the parent struct in the resulting JSON object.</p></blockquote><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;encoding/json&quot;</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Meta <span class="keyword">struct</span> &#123;</span><br><span class="line">    Name <span class="keyword">string</span> <span class="string">`json:&quot;name&quot;`</span></span><br><span class="line">    Age  <span class="keyword">int</span>    <span class="string">`json:&quot;age&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span> &#123;</span><br><span class="line">    Meta <span class="string">`json:&quot;,inline&quot;`</span></span><br><span class="line">    Address <span class="keyword">string</span> <span class="string">`json:&quot;address&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    p := Person&#123;</span><br><span class="line">        Meta: Meta&#123;</span><br><span class="line">            Name: <span class="string">&quot;John&quot;</span>,</span><br><span class="line">            Age:  <span class="number">30</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        Address: <span class="string">&quot;123 Main St&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    data, _ := json.Marshal(p)</span><br><span class="line">    fmt.Println(<span class="keyword">string</span>(data))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;John&quot;</span>,</span><br><span class="line">    <span class="string">&quot;age&quot;</span>: <span class="number">30</span>,</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: <span class="string">&quot;123 Main St&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id="cobra"><a href="#cobra" class="headerlink" title="cobra"></a>cobra</h1><h2 id="PersistentFlag-vs-Flag"><a href="#PersistentFlag-vs-Flag" class="headerlink" title="PersistentFlag vs Flag"></a>PersistentFlag vs Flag</h2><p><strong>PersistentFlag</strong></p><ul><li>Flags that are available to the command and all its subcommands. but not explicitly added to the main command’s help output.</li></ul><p><strong>Flag</strong></p><ul><li><p>Definition: Flags that are only available to the specific command they are defined on.</p></li><li><p>Example: If you set a flag on a specific command, it will not be available to its parent or sibling commands.</p></li></ul><h1 id="klog"><a href="#klog" class="headerlink" title="klog"></a>klog</h1><blockquote><p>k8s.io/klog/v2</p></blockquote><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">klog.Info(<span class="string">&quot;setting.MountOptions&quot;</span>, setting.MountOptions)</span><br><span class="line"></span><br><span class="line">klog.Infof(<span class="string">&quot;volumeId[%s] using patch:%#v&quot;</span>, setting.VolumeId, patch)</span><br><span class="line"></span><br><span class="line">klog.ErrorS(err, <span class="string">&quot;dingofs create fs error: %v&quot;</span>, err)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;fmt&quot;&gt;&lt;a href=&quot;#fmt&quot; class=&quot;headerlink&quot; title=&quot;fmt&quot;&gt;&lt;/a&gt;fmt&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;%v&lt;/code&gt; vs &lt;code&gt;%#v&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;hi</summary>
      
    
    
    
    <category term="GO" scheme="https://www.willshirley.top/categories/GO/"/>
    
    
    <category term="pkg" scheme="https://www.willshirley.top/tags/pkg/"/>
    
  </entry>
  
  <entry>
    <title>lvm senario</title>
    <link href="https://www.willshirley.top/2024/12/06/fs:%20lvm%20senario/"/>
    <id>https://www.willshirley.top/2024/12/06/fs:%20lvm%20senario/</id>
    <published>2024-12-06T02:43:30.000Z</published>
    <updated>2025-02-10T10:22:22.997Z</updated>
    
    <content type="html"><![CDATA[<h1 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h1><img src="/images/fs/lvm_overview-1.svg"><h1 id="create-LVs"><a href="#create-LVs" class="headerlink" title="create LVs"></a>create LVs</h1><blockquote><p> If you want to create <strong>three Logical Volumes (LVs)</strong> of sizes 1TB, 1TB, and 5TB on a single Volume Group (VG) for each disk, here’s how you can do it step by step.</p></blockquote><p>Assume:</p><p><strong>Disks</strong>: /dev/nvme0n1, /dev/nvme1n1, /dev/nvme2n1</p><p><strong>Volume Groups</strong>: vg_nvme0, vg_nvme1, vg_nvme2</p><p><strong>Logical Volumes (LVs)</strong>: lv1, lv2, lv3 for each VG</p><p><strong>LV Sizes</strong>: 1TB, 1TB, and 5TB</p><h2 id="1-Create-Physical-Volumes-PVs"><a href="#1-Create-Physical-Volumes-PVs" class="headerlink" title="1. Create Physical Volumes (PVs)"></a>1. Create Physical Volumes (PVs)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Initialize each disk as a PV:</span></span><br><span class="line">sudo pvcreate /dev/nvme0n1</span><br><span class="line">sudo pvcreate /dev/nvme1n1</span><br><span class="line">sudo pvcreate /dev/nvme2n1</span><br></pre></td></tr></table></figure><h2 id="2-Create-Volume-Groups-VGs"><a href="#2-Create-Volume-Groups-VGs" class="headerlink" title="2. Create Volume Groups (VGs)"></a>2. Create Volume Groups (VGs)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Create a VG <span class="keyword">for</span> each disk:</span></span><br><span class="line">sudo vgcreate vg_nvme0 /dev/nvme0n1</span><br><span class="line">sudo vgcreate vg_nvme1 /dev/nvme1n1</span><br><span class="line">sudo vgcreate vg_nvme2 /dev/nvme2n1</span><br></pre></td></tr></table></figure><h2 id="3-Create-Logical-Volumes-LVs"><a href="#3-Create-Logical-Volumes-LVs" class="headerlink" title="3. Create Logical Volumes (LVs)"></a>3. Create Logical Volumes (LVs)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Allocate 1TB, 1TB, and 5TB LVs from each VG.</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For vg_nvme0:</span></span><br><span class="line">sudo lvcreate -L 1T -n lv1 vg_nvme0</span><br><span class="line">sudo lvcreate -L 1T -n lv2 vg_nvme0</span><br><span class="line">sudo lvcreate -L 5T -n lv3 vg_nvme0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For vg_nvme1:</span></span><br><span class="line">sudo lvcreate -L 1T -n lv1 vg_nvme1</span><br><span class="line">sudo lvcreate -L 1T -n lv2 vg_nvme1</span><br><span class="line">sudo lvcreate -L 5T -n lv3 vg_nvme1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For vg_nvme2:</span></span><br><span class="line">sudo lvcreate -L 1T -n lv1 vg_nvme2</span><br><span class="line">sudo lvcreate -L 1T -n lv2 vg_nvme2</span><br><span class="line">sudo lvcreate -L 5T -n lv3 vg_nvme2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use whole vg</span></span><br><span class="line">sudo lvcreate -l 100%FREE -n lv vg_xxx</span><br></pre></td></tr></table></figure><h2 id="4-Verify-the-Setup"><a href="#4-Verify-the-Setup" class="headerlink" title="4. Verify the Setup"></a>4. Verify the Setup</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Check the LVs and their sizes:</span></span><br><span class="line">sudo lvs</span><br></pre></td></tr></table></figure><p>You should see something like:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">LV   VG        Attr       LSize</span><br><span class="line"></span><br><span class="line">lv1  vg_nvme0  -wi-a----- 1.00t                           </span><br><span class="line">lv2  vg_nvme0  -wi-a----- 1.00t                           </span><br><span class="line">lv3  vg_nvme0  -wi-a----- 5.00t                           </span><br><span class="line">lv1  vg_nvme1  -wi-a----- 1.00t                           </span><br><span class="line">lv2  vg_nvme1  -wi-a----- 1.00t                           </span><br><span class="line">lv3  vg_nvme1  -wi-a----- 5.00t                           </span><br><span class="line">lv1  vg_nvme2  -wi-a----- 1.00t                           </span><br><span class="line">lv2  vg_nvme2  -wi-a----- 1.00t                           </span><br><span class="line">lv3  vg_nvme2  -wi-a----- 5.00t</span><br></pre></td></tr></table></figure><h2 id="5-Format-the-LVs"><a href="#5-Format-the-LVs" class="headerlink" title="5. Format the LVs"></a>5. Format the LVs</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Use xfs or another filesystem to format the LVs:</span></span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme0/lv1</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme0/lv2</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme0/lv3</span><br><span class="line"></span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme1/lv1</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme1/lv2</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme1/lv3</span><br><span class="line"></span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme2/lv1</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme2/lv2</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme2/lv3</span><br></pre></td></tr></table></figure><h2 id="6-Mount-the-LVs"><a href="#6-Mount-the-LVs" class="headerlink" title="6. Mount the LVs"></a>6. Mount the LVs</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Create directories <span class="keyword">for</span> each LV and mount them:</span></span><br><span class="line">sudo mkdir -p /mnt/nvme0/lv1 /mnt/nvme0/lv2 /mnt/nvme0/lv3</span><br><span class="line">sudo mkdir -p /mnt/nvme1/lv1 /mnt/nvme1/lv2 /mnt/nvme1/lv3</span><br><span class="line">sudo mkdir -p /mnt/nvme2/lv1 /mnt/nvme2/lv2 /mnt/nvme2/lv3</span><br><span class="line"></span><br><span class="line">sudo mount /dev/vg_nvme0/lv1 /mnt/nvme0/lv1</span><br><span class="line">sudo mount /dev/vg_nvme0/lv2 /mnt/nvme0/lv2</span><br><span class="line">sudo mount /dev/vg_nvme0/lv3 /mnt/nvme0/lv3</span><br><span class="line"></span><br><span class="line">sudo mount /dev/vg_nvme1/lv1 /mnt/nvme1/lv1</span><br><span class="line">sudo mount /dev/vg_nvme1/lv2 /mnt/nvme1/lv2</span><br><span class="line">sudo mount /dev/vg_nvme1/lv3 /mnt/nvme1/lv3</span><br><span class="line"></span><br><span class="line">sudo mount /dev/vg_nvme2/lv1 /mnt/nvme2/lv1</span><br><span class="line">sudo mount /dev/vg_nvme2/lv2 /mnt/nvme2/lv2</span><br><span class="line">sudo mount /dev/vg_nvme2/lv3 /mnt/nvme2/lv3</span><br></pre></td></tr></table></figure><h2 id="7-Persist-the-Mounts"><a href="#7-Persist-the-Mounts" class="headerlink" title="7. Persist the Mounts"></a>7. Persist the Mounts</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Add the mounts to /etc/fstab <span class="keyword">for</span> automatic remounting on boot:</span></span><br><span class="line">echo &#x27;/dev/vg_nvme0/lv1 /mnt/nvme0/lv1 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme0/lv2 /mnt/nvme0/lv2 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme0/lv3 /mnt/nvme0/lv3 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line"></span><br><span class="line">echo &#x27;/dev/vg_nvme1/lv1 /mnt/nvme1/lv1 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme1/lv2 /mnt/nvme1/lv2 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme1/lv3 /mnt/nvme1/lv3 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line"></span><br><span class="line">echo &#x27;/dev/vg_nvme2/lv1 /mnt/nvme2/lv1 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme2/lv2 /mnt/nvme2/lv2 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme2/lv3 /mnt/nvme2/lv3 xfs defaults 0 0&#x27; | sudo tee -a /etc/fstab</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> **Final Directory Layout**</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> **Disk****VG Name****LV Name****Size****Mount Point**</span></span><br><span class="line">/dev/nvme0n1vg_nvme0lv11T/mnt/nvme0/lv1</span><br><span class="line">lv21T/mnt/nvme0/lv2</span><br><span class="line">lv35T/mnt/nvme0/lv3</span><br><span class="line"></span><br><span class="line">/dev/nvme1n1vg_nvme1lv11T/mnt/nvme1/lv1</span><br><span class="line">lv21T/mnt/nvme1/lv2</span><br><span class="line">lv35T/mnt/nvme1/lv3</span><br><span class="line"></span><br><span class="line">/dev/nvme2n1vg_nvme2lv11T/mnt/nvme2/lv1</span><br><span class="line">lv21T/mnt/nvme2/lv2</span><br><span class="line">lv35T/mnt/nvme2/lv3</span><br></pre></td></tr></table></figure><blockquote><p>defaults 0 x</p><p>The difference is in the last number (0 vs 2) in the fstab entry, which represents the filesystem check (fsck) pass number. Here’s what these numbers mean:</p><p>Last field (6th field) in fstab:</p><ul><li><code>0</code> = No filesystem check will be done at boot time</li><li><code>1</code> = Filesystem will be checked first (typically used for root filesystem /)</li><li><code>2</code> = Filesystem will be checked after pass 1 filesystems (typically used for other filesystems)</li></ul><p>So:</p><ul><li><code>defaults 0 0</code> means the filesystem will never be automatically checked during boot</li><li><code>defaults 0 2</code> means the filesystem will be checked during boot, but after the root filesystem</li></ul><p>Best practices:</p><ul><li>Use <code>0 1</code> for the root filesystem (/)</li><li>Use <code>0 2</code> for other important filesystems that should be checked</li><li>Use <code>0 0</code> for pseudo-filesystems (like proc, sysfs) or filesystems that don’t need checking (like swap)</li></ul></blockquote><h1 id="wipe-disk-and-create-lvm"><a href="#wipe-disk-and-create-lvm" class="headerlink" title="wipe disk and create lvm"></a>wipe disk and create lvm</h1><blockquote><p>assum dev is /dev/nvme2n1</p></blockquote><p>To erase all partitions on the device /dev/nvme2n1 and create multiple logical volumes (LVs) using the LVM framework, follow these steps:</p><h2 id="1-Verify-Device-and-Backup-Data"><a href="#1-Verify-Device-and-Backup-Data" class="headerlink" title="1. Verify Device and Backup Data"></a>1. Verify Device and Backup Data</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Ensure you are working on the correct device. Erasing partitions will delete all data on the device.</span></span><br><span class="line">sudo lsblk -o NAME,SIZE,TYPE,MOUNTPOINT /dev/nvme2n1</span><br></pre></td></tr></table></figure><h2 id="2-Erase-Existing-Partitions"><a href="#2-Erase-Existing-Partitions" class="headerlink" title="2. Erase Existing Partitions"></a>2. Erase Existing Partitions</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Clear the Partition Table, To wipe the partition table completely:</span></span><br><span class="line">sudo wipefs -a /dev/nvme2n1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Verify the Disk is Clean, Check that no partitions remain</span></span><br><span class="line">sudo lsblk /dev/nvme2n1</span><br></pre></td></tr></table></figure><h2 id="3-Create-Physical-Volume-PV"><a href="#3-Create-Physical-Volume-PV" class="headerlink" title="3. Create Physical Volume (PV)"></a>3. Create Physical Volume (PV)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Convert the entire disk into an LVM physical volume:</span></span><br><span class="line">sudo pvcreate /dev/nvme2n1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Verify the PV</span></span><br><span class="line">sudo pvdisplay</span><br></pre></td></tr></table></figure><h2 id="4-Create-Volume-Group-VG"><a href="#4-Create-Volume-Group-VG" class="headerlink" title="4. Create Volume Group (VG)"></a>4. Create Volume Group (VG)</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Create a volume group that spans the entire disk:</span></span><br><span class="line">sudo vgcreate vg_nvme2n1 /dev/nvme2n1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Verify the VG:</span></span><br><span class="line">sudo vgdisplay</span><br></pre></td></tr></table></figure><h2 id="5-Create-Logical-Volumes-LVs"><a href="#5-Create-Logical-Volumes-LVs" class="headerlink" title="5. Create Logical Volumes (LVs)"></a>5. Create Logical Volumes (LVs)</h2><p>Example: Create Three LVs</p><p><strong>LV1</strong>: 1TB</p><p><strong>LV2</strong>: 1TB</p><p><strong>LV3</strong>: Remaining space</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo lvcreate -L 1T -n lv1 vg_nvme2n1</span><br><span class="line">sudo lvcreate -L 1T -n lv2 vg_nvme2n1</span><br><span class="line">sudo lvcreate -l 100%FREE -n lv3 vg_nvme2n1 --wipesignatures y</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意如果在lvcreate的时候一直提示 warning wipe offset xxx,那执行 sudo lvcreate xxx -y (加-y参数)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Verify the LVs:</span></span><br><span class="line">sudo lvdisplay</span><br></pre></td></tr></table></figure><h2 id="6-Format-Logical-Volumes"><a href="#6-Format-Logical-Volumes" class="headerlink" title="6. Format Logical Volumes"></a>6. Format Logical Volumes</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Format each logical volume with your desired file system (e.g., XFS):</span></span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme2n1/lv1</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme2n1/lv2</span><br><span class="line">sudo mkfs.xfs /dev/vg_nvme2n1/lv3</span><br></pre></td></tr></table></figure><h2 id="7-Mount-Logical-Volumes"><a href="#7-Mount-Logical-Volumes" class="headerlink" title="7. Mount Logical Volumes"></a>7. Mount Logical Volumes</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Create mount points and mount the LVs:</span></span><br><span class="line">sudo mkdir -p /mnt/nvme2n1/lv1 /mnt/nvme2n1/lv2 /mnt/nvme2n1/lv3</span><br><span class="line"></span><br><span class="line">sudo mount /dev/vg_nvme2n1/lv1 /mnt/nvme2n1/lv1</span><br><span class="line">sudo mount /dev/vg_nvme2n1/lv2 /mnt/nvme2n1/lv2</span><br><span class="line">sudo mount /dev/vg_nvme2n1/lv3 /mnt/nvme2n1/lv3</span><br><span class="line"><span class="meta">#</span><span class="bash"> Verify the mounts:</span></span><br><span class="line">df -h</span><br></pre></td></tr></table></figure><h2 id="8-Make-the-Mounts-Persistent"><a href="#8-Make-the-Mounts-Persistent" class="headerlink" title="8. Make the Mounts Persistent"></a>8. Make the Mounts Persistent</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Add entries to /etc/fstab to ensure the LVs are mounted on reboot:</span></span><br><span class="line">echo &#x27;/dev/vg_nvme2n1/lv1 /mnt/nvme2n1/lv1 xfs defaults 0 2&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme2n1/lv2 /mnt/nvme2n1/lv2 xfs defaults 0 2&#x27; | sudo tee -a /etc/fstab</span><br><span class="line">echo &#x27;/dev/vg_nvme2n1/lv3 /mnt/nvme2n1/lv3 xfs defaults 0 2&#x27; | sudo tee -a /etc/fstab</span><br></pre></td></tr></table></figure><h1 id="wipe-existed-lv"><a href="#wipe-existed-lv" class="headerlink" title="wipe existed lv"></a>wipe existed lv</h1><h2 id="1-Check-What’s-Using-the-LV"><a href="#1-Check-What’s-Using-the-LV" class="headerlink" title="1. Check What’s Using the LV"></a>1. Check What’s Using the LV</h2><p>First, identify what is still using the LV:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof | grep /dev/mapper/&lt;lv-name&gt;</span><br></pre></td></tr></table></figure><p>Also, check active processes using the device:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo fuser -m /dev/mapper/&lt;lv-name&gt;</span><br></pre></td></tr></table></figure><p>If any process is using the LV, stop it:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo kill -9 &lt;PID&gt;</span><br></pre></td></tr></table></figure><h2 id="2-Unmount-If-Mounted"><a href="#2-Unmount-If-Mounted" class="headerlink" title="2. Unmount If Mounted"></a>2. Unmount If Mounted</h2><p>Check if the LV is mounted:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount | grep /dev/mapper/&lt;lv-name&gt;</span><br></pre></td></tr></table></figure><p>If it is mounted, unmount it:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo umount -l /dev/mapper/&lt;lv-name&gt;</span><br></pre></td></tr></table></figure><p>Use -l (lazy unmount) to force unmount if needed.</p><h2 id="3-Disable-the-LV"><a href="#3-Disable-the-LV" class="headerlink" title="3. Disable the LV"></a>3. Disable the LV</h2><p>Before removing the LV, <strong>deactivate it</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lvchange -an /dev/&lt;vg-name&gt;/&lt;lv-name&gt;</span><br></pre></td></tr></table></figure><p>Now try to remove it:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dmsetup remove /dev/mapper/&lt;lv-name&gt;</span><br></pre></td></tr></table></figure><h2 id="4-Forcefully-Remove-LV-VG-and-PV"><a href="#4-Forcefully-Remove-LV-VG-and-PV" class="headerlink" title="4. Forcefully Remove LV, VG, and PV"></a>4. Forcefully Remove LV, VG, and PV</h2><p>If the above steps don’t work, forcefully remove everything:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo lvremove -f /dev/&lt;vg-name&gt;/&lt;lv-name&gt;</span><br><span class="line">sudo vgremove -f &lt;vg-name&gt;</span><br><span class="line">sudo pvremove -f /dev/&lt;device-name&gt;</span><br></pre></td></tr></table></figure><p>Check again with:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lsblk</span><br><span class="line">sudo vgs</span><br><span class="line">sudo lvs</span><br></pre></td></tr></table></figure><h2 id="5-If-Everything-Fails"><a href="#5-If-Everything-Fails" class="headerlink" title="5. If Everything Fails"></a>5. If Everything Fails</h2><p>If none of the above works, the safest option is to <strong>reboot</strong>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo wipefs --all --force /dev/&lt;device-name&gt;</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;overview&quot;&gt;&lt;a href=&quot;#overview&quot; class=&quot;headerlink&quot; title=&quot;overview&quot;&gt;&lt;/a&gt;overview&lt;/h1&gt;&lt;img src=&quot;/images/fs/lvm_overview-1.svg&quot;&gt;

&lt;h1 id</summary>
      
    
    
    
    <category term="fs" scheme="https://www.willshirley.top/categories/fs/"/>
    
    
    <category term="lvm" scheme="https://www.willshirley.top/tags/lvm/"/>
    
  </entry>
  
  <entry>
    <title>network snippet</title>
    <link href="https://www.willshirley.top/2024/12/05/network%20snippet/"/>
    <id>https://www.willshirley.top/2024/12/05/network%20snippet/</id>
    <published>2024-12-05T07:50:13.000Z</published>
    <updated>2025-03-10T02:03:52.811Z</updated>
    
    <content type="html"><![CDATA[<h1 id="basic"><a href="#basic" class="headerlink" title="basic"></a>basic</h1><p><strong>Common Name (CN)</strong></p><p><strong>Alternative Name (SAN)</strong></p><p><strong>Transport Layer Security (TLS)</strong></p><h1 id="A-VS-B"><a href="#A-VS-B" class="headerlink" title="A VS B"></a>A VS B</h1><h2 id="SSL-VS-TLS"><a href="#SSL-VS-TLS" class="headerlink" title="SSL VS TLS"></a>SSL VS TLS</h2><blockquote><p>TLS (Transport Layer Security) and SSL (Secure Sockets Layer) are both cryptographic protocols used for secure communication over the internet. However, they have some key differences:</p></blockquote><p><strong>SSL (Secure Sockets Layer)</strong></p><ul><li>Developed by Netscape in 1994 as a predecessor to TLS</li><li>Primarily focused on web browsing security</li><li>Used for encrypting data between a client’s browser and a server</li><li>Typically used with port 443 (HTTPS)</li><li>Considered insecure due to weaknesses in its encryption algorithms and key exchange mechanisms</li></ul><p><strong>TLS (Transport Layer Security)</strong></p><ul><li>Developed by the Internet Engineering Task Force (IETF) as a successor to SSL</li><li>Focuses on secure communication between any two endpoints, not just web browsing</li><li>Uses more robust encryption algorithms, such as AES-256, and improved key exchange mechanisms</li><li>Supports multiple versions of the protocol (TLS 1.0, TLS 1.1, TLS 1.2)</li><li>Now considered a standard for secure internet communications</li></ul><p><strong>Key differences:</strong></p><ol><li><strong>Name</strong>: SSL was originally used for web browsing security, while TLS is more general-purpose.</li><li><strong>Encryption algorithms</strong>: TLS uses stronger encryption algorithms than SSL.</li><li><strong>Key exchange mechanisms</strong>: TLS has improved key exchange mechanisms compared to SSL.</li><li><strong>Protocol versions</strong>: TLS supports multiple protocol versions (TLS 1.0, TLS 1.1, TLS 1.2), while SSL is typically associated with a single version.</li></ol><p><strong>When to use each:</strong></p><ul><li>Use SSL when you need to maintain compatibility with older systems or browsers that only support the original SSL protocol.</li><li>Use TLS for new applications and systems where security is paramount.</li></ul><p>Keep in mind that most modern systems, including web servers and clients, have already adopted TLS as the standard for secure communication. If you’re developing a new application or system, it’s recommended to use TLS to ensure the highest level of security.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;basic&quot;&gt;&lt;a href=&quot;#basic&quot; class=&quot;headerlink&quot; title=&quot;basic&quot;&gt;&lt;/a&gt;basic&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Common Name (CN)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alternati</summary>
      
    
    
    
    <category term="network" scheme="https://www.willshirley.top/categories/network/"/>
    
    
    <category term="snippet" scheme="https://www.willshirley.top/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>bazel</title>
    <link href="https://www.willshirley.top/2024/11/25/bazel/"/>
    <id>https://www.willshirley.top/2024/11/25/bazel/</id>
    <published>2024-11-25T03:42:10.000Z</published>
    <updated>2024-11-29T09:17:20.858Z</updated>
    
    <content type="html"><![CDATA[<p><strong>MODULE.bazel</strong></p><blockquote><p>is equal WORKSPACE at old bazel version</p></blockquote><ul><li>The <code>MODULE.bazel</code> file, which identifies the directory and its contents as a Bazel workspace and lives at the root of the project’s directory structure. It’s also where you specify your external dependencies.</li></ul><p><strong>BUILD</strong></p><ul><li>One or more <a href="https://bazel.build/reference/glossary#build-file"><code>BUILD</code> files</a>, which tell Bazel how to build different parts of the project. A directory within the workspace that contains a <code>BUILD</code> file is a package. (More on packages later in this tutorial.)</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;MODULE.bazel&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;is equal WORKSPACE at old bazel version&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;MODULE.baz</summary>
      
    
    
    
    <category term="build" scheme="https://www.willshirley.top/categories/build/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus &amp; Grafana</title>
    <link href="https://www.willshirley.top/2024/10/25/prometheus%20&amp;%20grafana/"/>
    <id>https://www.willshirley.top/2024/10/25/prometheus%20&amp;%20grafana/</id>
    <published>2024-10-25T07:02:38.000Z</published>
    <updated>2025-03-07T10:25:01.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h1><h2 id="node-exporter"><a href="#node-exporter" class="headerlink" title="node_exporter"></a>node_exporter</h2><ul><li><p>package</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>prepare user and directory</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd -f dingofs</span><br><span class="line">sudo useradd -g dingofs dingofs</span><br><span class="line">sudo mkdir /etc/node_exporter</span><br><span class="line">sudo chown dingofs:dingofs /etc/node_exporter</span><br><span class="line"></span><br><span class="line">sudo cp /path/to/node_exporter-files/node_exporter /usr/bin/</span><br><span class="line">sudo chown dingofs:dingofs /usr/bin/node_exporter</span><br></pre></td></tr></table></figure></li><li><p>config system service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /usr/lib/systemd/system/node_exporter.service</span><br><span class="line">sudo chmod 664 /usr/lib/systemd/system/node_exporter.service</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">Unit</span>]</span><br><span class="line"><span class="string">Description=Node</span> <span class="string">Exporter</span></span><br><span class="line"><span class="string">Documentation=https://prometheus.io/docs/guides/node-exporter/</span></span><br><span class="line"><span class="string">Wants=network-online.target</span></span><br><span class="line"><span class="string">After=network-online.target</span></span><br><span class="line"></span><br><span class="line">[<span class="string">Service</span>]</span><br><span class="line"><span class="string">User=dingofs</span></span><br><span class="line"><span class="string">Group=dingofs</span></span><br><span class="line"><span class="string">Type=simple</span></span><br><span class="line"><span class="string">Restart=on-failure</span></span><br><span class="line"><span class="string">ExecStart=/usr/bin/node_exporter</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--web.listen-address=:19100</span></span><br><span class="line"></span><br><span class="line">[<span class="string">Install</span>]</span><br><span class="line"><span class="string">WantedBy=multi-user.target</span></span><br></pre></td></tr></table></figure></li><li><p>start system service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl start node_exporter</span><br><span class="line">sudo systemctl status node_exporter</span><br><span class="line">sudo systemctl enable node_exporter.service # Configure node_exporter to start at boot</span><br></pre></td></tr></table></figure></li></ul><h2 id="prometheus-server"><a href="#prometheus-server" class="headerlink" title="prometheus server"></a>prometheus server</h2><ul><li><p>prometheus.yml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># By default, scrape targets every 15 seconds.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Attach these extra labels to all timeseries collected by this Prometheus instance.</span></span><br><span class="line">  <span class="attr">external_labels:</span></span><br><span class="line">    <span class="attr">monitor:</span> <span class="string">&#x27;codelab-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#rule_files:</span></span><br><span class="line"><span class="comment">#- &#x27;prometheus.rules.yml&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Override the global default and scrape targets from this job every 5 seconds.</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;172.20.7.232:19090&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;node&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Override the global default and scrape targets from this job every 5 seconds.</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;172.20.7.232:19100&#x27;</span>,<span class="string">&#x27;172.20.7.233:19100&#x27;</span>,<span class="string">&#x27;172.20.7.234:19100&#x27;</span>]</span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">group:</span> <span class="string">&#x27;metric1&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>docker container</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Create persistent volume <span class="keyword">for</span> your data</span></span><br><span class="line">docker volume create prometheus-data</span><br><span class="line"><span class="meta">#</span><span class="bash"> Start Prometheus container</span></span><br><span class="line">docker run -d \</span><br><span class="line">    -p 19090:19090 \</span><br><span class="line">    -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \</span><br><span class="line">    -v prometheus-data:/prometheus \</span><br><span class="line">    prom/prometheus</span><br></pre></td></tr></table></figure><h1 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h1><ul><li><p>deploy container</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> create a persistent volume <span class="keyword">for</span> your data</span></span><br><span class="line">docker volume create grafana-storage</span><br><span class="line"><span class="meta">#</span><span class="bash"> verify that the volume was created correctly</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> you should see some JSON output</span></span><br><span class="line">docker volume inspect grafana-storage</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> start grafana</span></span><br><span class="line">docker run -d -p 13000:3000 --name=grafana \</span><br><span class="line">  --volume grafana-storage:/var/lib/grafana \</span><br><span class="line">  grafana/grafana</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> web login</span></span><br><span class="line">admin/admin</span><br></pre></td></tr></table></figure></li></ul><h2 id="dashboard"><a href="#dashboard" class="headerlink" title="dashboard"></a>dashboard</h2><p><a href="https://grafana.com/grafana/dashboards/">https://grafana.com/grafana/dashboards/</a></p><p><strong>just import from existed dashboard json</strong></p><p>enjoy it</p><ul><li>reference<ul><li><a href="https://grafana.com/docs/grafana/latest/setup-grafana/installation/docker/">official deploy</a></li><li><a href="https://grafana.com/docs/grafana/latest/datasources/prometheus/configure-prometheus-data-source/">config prometheous</a></li><li><a href="https://grafana.com/docs/grafana/latest/getting-started/get-started-grafana-prometheus/">Get started with Grafana and Prometheus</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Prometheus&quot;&gt;&lt;a href=&quot;#Prometheus&quot; class=&quot;headerlink&quot; title=&quot;Prometheus&quot;&gt;&lt;/a&gt;Prometheus&lt;/h1&gt;&lt;h2 id=&quot;node-exporter&quot;&gt;&lt;a href=&quot;#node-exp</summary>
      
    
    
    
    <category term="monitor" scheme="https://www.willshirley.top/categories/monitor/"/>
    
    
    <category term="tool" scheme="https://www.willshirley.top/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>devops</title>
    <link href="https://www.willshirley.top/2024/10/10/devops/"/>
    <id>https://www.willshirley.top/2024/10/10/devops/</id>
    <published>2024-10-10T07:17:52.000Z</published>
    <updated>2024-10-10T07:29:04.512Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/devops/cicd.jpg" style="zoom: 100%">]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;/images/devops/cicd.jpg&quot; style=&quot;zoom: 100%&quot;&gt;

</summary>
      
    
    
    
    <category term="devops" scheme="https://www.willshirley.top/categories/devops/"/>
    
    
    <category term="snippet" scheme="https://www.willshirley.top/tags/snippet/"/>
    
  </entry>
  
</feed>
