<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>source is the essence</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-12-30T18:01:42.899Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>brook</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>yarn point</title>
    <link href="http://yoursite.com/2021/12/31/yarn%20snippet/"/>
    <id>http://yoursite.com/2021/12/31/yarn%20snippet/</id>
    <published>2021-12-30T18:01:13.000Z</published>
    <updated>2021-12-30T18:01:42.899Z</updated>
    
    <content type="html"><![CDATA[<p>YARN 看做一个云操作系统，它负责为应用程序启 动 ApplicationMaster（相当于主线程），然后再由 ApplicationMaster 负责数据切分、任务分配、 启动和监控等工作，而由 ApplicationMaster 启动的各个 Task（相当于子线程）仅负责自己的计 算任务。当所有任务计算完成后，ApplicationMaster 认为应用程序运行完成，然后退出。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;YARN 看做一个云操作系统，它负责为应用程序启 动 ApplicationMaster（相当于主线程），然后再由 ApplicationMaster 负责数据切分、任务分配、 启动和监控等工作，而由 ApplicationMaster 启动的各个 Task（相当于子线程）</summary>
      
    
    
    
    <category term="yarn" scheme="http://yoursite.com/categories/yarn/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>数据湖</title>
    <link href="http://yoursite.com/2021/12/30/%E6%95%B0%E6%8D%AE%E6%B9%96/"/>
    <id>http://yoursite.com/2021/12/30/%E6%95%B0%E6%8D%AE%E6%B9%96/</id>
    <published>2021-12-30T01:52:19.000Z</published>
    <updated>2021-12-30T01:52:32.124Z</updated>
    
    <content type="html"><![CDATA[<p>数据湖，目前关注度比较高的有 Databricks 推出的 Delta Lake、Uber 的 Hudi 以及 Netflix 的 Iceberg</p><p>reference <a href="https://mp.weixin.qq.com/s/m8-iFg-ekykWGrG3gXlLew">https://mp.weixin.qq.com/s/m8-iFg-ekykWGrG3gXlLew</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;数据湖，目前关注度比较高的有 Databricks 推出的 Delta Lake、Uber 的 Hudi 以及 Netflix 的 Iceberg&lt;/p&gt;
&lt;p&gt;reference &lt;a href=&quot;https://mp.weixin.qq.com/s/m8-iFg-eky</summary>
      
    
    
    
    <category term="DSA" scheme="http://yoursite.com/categories/DSA/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse points</title>
    <link href="http://yoursite.com/2021/12/29/clickhouse/"/>
    <id>http://yoursite.com/2021/12/29/clickhouse/</id>
    <published>2021-12-29T11:11:30.000Z</published>
    <updated>2021-12-29T11:16:57.430Z</updated>
    
    <content type="html"><![CDATA[<h2 id="存储架构"><a href="#存储架构" class="headerlink" title="存储架构"></a>存储架构</h2><blockquote><p>Clickhouse 存储中的最小单位是 DataPart，写入链路为了提升吞吐，放弃了部分写入实时可见性，即数据攒批写入，一次批量写入的数据会落盘成一个 DataPart.</p><p>它不像 Druid 那样一条一条实时摄入。但 ClickHouse 把数据延迟攒批写入的工作交给来客户端实现，比如达到 10 条记录或每过 5s 间隔写入，换句话说就是可以在用户侧平衡吞吐量和时延，如果在业务高峰期流量不是太大，可以结合实际场景将参数调小，以达到极致的实时效果。</p></blockquote><h2 id="查询架构"><a href="#查询架构" class="headerlink" title="查询架构"></a>查询架构</h2><h3 id="计算能力方面"><a href="#计算能力方面" class="headerlink" title="计算能力方面"></a>计算能力方面</h3><p>Clickhouse 采用向量化函数和 aggregator 算子极大地提升了聚合计算性能，配合完备的 SQL 能力使得数据分析变得更加简单、灵活。</p><h3 id="数据扫描方面"><a href="#数据扫描方面" class="headerlink" title="数据扫描方面"></a>数据扫描方面</h3><p>ClickHouse 是完全列式的存储计算引擎，而且是以有序存储为核心，在查询扫描数据的过程中，首先会根据存储的有序性、列存块统计信息、分区键等信息推断出需要扫描的列存块，然后进行并行的数据扫描，像表达式计算、聚合算子都是在正规的计算引擎中处理。从计算引擎到数据扫描，数据流转都是以列存块为单位，高度向量化的。</p><h3 id="高并发服务方面"><a href="#高并发服务方面" class="headerlink" title="高并发服务方面"></a>高并发服务方面</h3><p>Clickhouse 的并发能力其实是与并行计算量和机器资源决定的。如果查询需要扫描的数据量和计算复杂度很大，并发度就会降低，但是如果保证单个 query 的 latency 足够低（增加内存和 cpu 资源），部分场景下用户可以通过设置合适的系统参数来提升并发能力，比如 max_threads 等。其他分析型系统（例如 Elasticsearch）的并发能力为什么很好，从 Cache 设计层面来看，ES 的 Cache 包括 Query Cache, Request Cache，Data Cache，Index Cache，从查询结果到索引扫描结果层层的 Cache 加速，因为 Elasticsearch 认为它的场景下存在热点数据，可能被反复查询。反观 ClickHouse，只有一个面向 IO 的 UnCompressedBlockCache 和系统的 PageCache，为了实现更优秀的并发，我们很容易想到在 Clickhouse 外面加一层 Cache，比如 redis，但是分析场景下的数据和查询都是多变的，查询结果等 Cache 都不容易命中，而且在广投业务中实时查询的数据是基于 T 之后不断更新的数据，如果外挂缓存将降低数据查询的时效性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;存储架构&quot;&gt;&lt;a href=&quot;#存储架构&quot; class=&quot;headerlink&quot; title=&quot;存储架构&quot;&gt;&lt;/a&gt;存储架构&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Clickhouse 存储中的最小单位是 DataPart，写入链路为了提升吞吐，放弃了部分写入实</summary>
      
    
    
    
    <category term="clickhouse" scheme="http://yoursite.com/categories/clickhouse/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>cdc &amp; 实时数仓 points</title>
    <link href="http://yoursite.com/2021/12/29/cdc/"/>
    <id>http://yoursite.com/2021/12/29/cdc/</id>
    <published>2021-12-29T09:42:30.000Z</published>
    <updated>2021-12-30T02:00:22.199Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Change Data Capture（变更数据获取）</strong></p><p>核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li><strong>数据同步</strong>，用于备份，容灾；</li><li><strong>数据分发</strong>，一个数据源分发给多个下游；</li><li><strong>数据采集</strong>(E)，面向数据仓库/数据湖的 ETL 数据集成。</li></ul><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>主要分为<strong>基于查询</strong>和<strong>基于 Binlog</strong> 两种方式</p><h3 id="传统-CDC-ETL"><a href="#传统-CDC-ETL" class="headerlink" title="传统 CDC ETL"></a>传统 CDC ETL</h3><p><img src="/images/cdc/cdc_etl.png"></p><h3 id="基于-Flink-CDC-的-ETL-分析"><a href="#基于-Flink-CDC-的-ETL-分析" class="headerlink" title="基于 Flink CDC 的 ETL 分析"></a>基于 Flink CDC 的 ETL 分析</h3><p><img src="/images/cdc/flink_cdc_etl.png"></p><h3 id="基于-Flink-CDC-的聚合分析"><a href="#基于-Flink-CDC-的聚合分析" class="headerlink" title="基于 Flink CDC 的聚合分析"></a>基于 Flink CDC 的聚合分析</h3><p><img src="/images/cdc/flink_cdc_aggregate.png"></p><h3 id="基于-Flink-CDC-的数据打宽"><a href="#基于-Flink-CDC-的数据打宽" class="headerlink" title="基于 Flink CDC 的数据打宽"></a>基于 Flink CDC 的数据打宽</h3><p><img src="/images/cdc/flink_cdc_merge.png"></p><h2 id="性能点"><a href="#性能点" class="headerlink" title="性能点"></a>性能点</h2><p>大数据领域的 4 类场景：</p><p><strong>B</strong>    batch    离线计算</p><p><strong>A</strong>    Analytical    交互式分析</p><p><strong>S</strong>    Servering    高并发的在线服务</p><p><strong>T</strong>    Transaction    事务隔离机制</p><blockquote><p>离线计算通常在计算层，所以应该重点考虑 A、S 和 T</p></blockquote><h2 id="考虑点"><a href="#考虑点" class="headerlink" title="考虑点"></a>考虑点</h2><ul><li><p>保证端到端的数据一致性，包括维度一致性以及全流程数据一致性;</p></li><li><p>实时流处理过程中数据到达顺序无法预知时，如何保证双流 join 时数据能及时关联同时不造成数据堵塞；</p></li><li><p>Oracle</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.Oracle 是第三方厂商维护的，不允许对线上系统有过多的侵入，容易造成监听故障甚至系统瘫痪，</span><br><span class="line">2.归档日志是在开启那一刻起才开始生成的，之前的存量数据难以进入 kafka，但是后来实时数据又必须依赖前面的计算结果</span><br></pre></td></tr></table></figure></li></ul><h2 id="实时数仓方案"><a href="#实时数仓方案" class="headerlink" title="实时数仓方案"></a>实时数仓方案</h2><h3 id="Lambda-架构"><a href="#Lambda-架构" class="headerlink" title="Lambda 架构"></a>Lambda 架构</h3><blockquote><p>目前主流的一套实时数仓架构，存在离线和实时两条链路。实时部分以消息队列的方式实时增量消费，一般以 Flink+Kafka 的组合实现，维度表存在关系型数据库或者 HBase；离线部分一般采用 T+1 周期调度分析历史存量数据，每天凌晨产出，更新覆盖前一天的结果数据，计算引擎通常会选择 Hive 或者 Spark。</p></blockquote><p><img src="/images/cdc/structure_lambda.png"></p><h3 id="Kappa-架构"><a href="#Kappa-架构" class="headerlink" title="Kappa 架构"></a>Kappa 架构</h3><blockquote><p>相较于 Lambda 架构，它移除了离线生产链路，思路是通过传递任意想要的 offset(偏移量)来达到重新消费处理历史数据的目的。优点是架构相对简化，数据来源单一，共用一套代码，开发效率高；缺点是必须要求消息队列中保存了存量数据，而且主要业务逻辑在计算层，比较消耗内存资源。</p></blockquote><p><img src="/images/cdc/structure_kappa.png"></p><h3 id="OLAP-变体架构"><a href="#OLAP-变体架构" class="headerlink" title="OLAP 变体架构"></a>OLAP 变体架构</h3><blockquote><p>是 Kappa 架构的进一步演化，它的思路是将聚合分析计算由 OLAP 引擎承担，减轻实时计算部分的聚合处理压力。优点是自由度高，可以满足数据分析师的实时自助分析需求，减轻了计算引擎的处理压力；缺点是必须要求消息队列中保存存量数据，且因为是将计算部分的压力转移到了查询层，对查询引擎的吞吐和实时摄入性能要求较高。</p></blockquote><p><img src="/images/cdc/structure_olap.png"></p><h3 id="数据湖架构"><a href="#数据湖架构" class="headerlink" title="数据湖架构"></a>数据湖架构</h3><blockquote><p>存储、计算和查询，分别由三个独立产品负责，分别是数据湖、Flink 和 Clickhouse。数仓分层存储和维度表管理均由数据湖承担，Flink SQL 负责批流任务的 SQL 化协同开发，Clickhouse 实现变体的事务机制，为用户提供离线分析和交互查询。CDC 到消息队列这一链路将来是完全可以去掉的，只需要 Flink CDC 家族中再添加 Oracle CDC 一员。未来，实时数仓架构将得到极致的简化并且性能有质的提升。</p></blockquote><p><img src="/images/cdc/structure_rtdb.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Change Data Capture（变更数据获取）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="cdc" scheme="http://yoursite.com/categories/cdc/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>flink points</title>
    <link href="http://yoursite.com/2021/12/28/flink%20points/"/>
    <id>http://yoursite.com/2021/12/28/flink%20points/</id>
    <published>2021-12-28T10:43:00.000Z</published>
    <updated>2021-12-28T17:11:26.738Z</updated>
    
    <content type="html"><![CDATA[<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><blockquote><p>基于 Chandy-Lamport 算法实现了一个分布式的一致性的快照，从而提供了一致性的语义。</p><p>Chandy-Lamport 算法实际上在 1985 年的时候已经被提出来，但并没有被很广泛的应用，而 Flink 则把这个算法发扬光大了。</p></blockquote><h3 id="state"><a href="#state" class="headerlink" title="state"></a>state</h3><blockquote><p>丰富的State API：ValueState、ListState、MapState、 BroadcastState</p></blockquote><h3 id="time"><a href="#time" class="headerlink" title="time"></a>time</h3><blockquote><p>实现了 Watermark 的机制，能够支持基于事件的时间的处理，或者说基于系统时间的处理，能够容忍数据的延时、容忍数据的迟到、容忍乱序的数据。</p></blockquote><p><img src="/images/flink/flink_time.png"></p><ul><li><p>Event Time：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。</p></li><li><p>Ingestion Time：是数据进入 Flink 的时间。</p></li><li><p>Processing Time：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是 Processing Time。</p><blockquote><p>例如，一条日志进入 Flink 的时间为 2019-08-12 10:00:00.123，到达 Window 的系统时间为 2019-08-12 10:00:01.234，日志的内容如下：</p><p>2019-08-02 18:37:15.624 INFO Fail over to rm2</p><p>对于业务来说，要统计 1min 内的故障日志个数，哪个时间是最有意义的？—— eventTime，因为我们要根据日志的生成时间进行统计。</p></blockquote></li></ul><h3 id="window"><a href="#window" class="headerlink" title="window"></a>window</h3><blockquote><p>Flink 提供了开箱即用的各种窗口，比如滑动窗口、滚动窗口、会话窗口以及非常灵活的自定义的窗口。</p></blockquote><p><img src="/images/flink/flink_window.png"></p><ul><li><p>滚动窗口（Tumbling Window）</p><p>将数据依据固定的窗口长度对数据进行切片, 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中，滚动窗口有一个固定的大小，并且不会出现重叠</p><p>特点：<strong>时间对齐，窗口长度固定，没有重叠</strong></p><p>适用场景：适合做 BI 统计等（做每个时间段的聚合计算）</p><p>例如：如果你指定了一个 5 分钟大小的滚动窗口，窗口的创建如下图所示：</p><p><img src="/images/flink/flink_window_tumbling.png"></p></li><li><p>滑动窗口（Sliding Window）</p><p>滑动窗口是固定窗口的更广义的一种形式，滑动窗口由固定的窗口长度和滑动间隔组成。</p><p>特点：<strong>时间对齐，窗口长度固定，有重叠</strong></p><p>滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。</p><p>适用场景：对最近一个时间段内的统计（求某接口最近 5min 的失败率来决定是否要报警）。</p><p>例如，你有 10 分钟的窗口和 5 分钟的滑动，那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据，如下图所示：</p><p><img src="/images/flink/flink_window_sliding.png"></p></li><li><p>会话窗口（Session Window）</p><p>由一系列事件组合一个指定时间长度的 timeout 间隙组成，类似于 web 应用的 session，也就是一段时间没有接收到新数据就会生成新的窗口。</p><p>特点：<strong>时间无对齐</strong></p><p>session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将被分配到新的 session 窗口中去。</p></li></ul><p>​        <img src="/images/flink/flink_window_session.png"></p><hr><p>flink API</p><p><img src="/images/flink/flink_runtime.png"></p><blockquote><p>Flink 分别提供了面向流式处理的接口（DataStream API）和面向批处理的接口（DataSet API）。因此，Flink 既可以完成流处理，也可以完成批处理。Flink 支持的拓展库涉及机器学习（FlinkML）、复杂事件处理（CEP）、以及图计算（Gelly），还有分别针对流处理和批处理的 Table API。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;checkpoint&quot;&gt;&lt;a href=&quot;#checkpoint&quot; class=&quot;headerlink&quot; title=&quot;checkpoint&quot;&gt;&lt;/a&gt;checkpoint&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;基于 Chandy-Lamport 算法实现了一个</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="point" scheme="http://yoursite.com/tags/point/"/>
    
  </entry>
  
  <entry>
    <title>flink table</title>
    <link href="http://yoursite.com/2021/12/24/flink%20table/"/>
    <id>http://yoursite.com/2021/12/24/flink%20table/</id>
    <published>2021-12-24T01:00:00.000Z</published>
    <updated>2021-12-29T05:33:18.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="时间属性"><a href="#时间属性" class="headerlink" title="时间属性"></a>时间属性</h2><blockquote><p> 像窗口（在 <a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/tableapi/#group-windows">Table API</a> 和 <a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-agg/">SQL</a> ）这种基于时间的操作，需要有时间信息。因此，Table API 中的表就需要提供<em>逻辑时间属性</em>来表示时间，以及支持时间相关的操作。</p></blockquote><p>每种类型的表都可以有时间属性，时间属性可以通过</p><ol><li>用CREATE TABLE DDL创建表的时候指定</li><li>可以在 <code>DataStream</code> 中指定</li><li>可以在定义 <code>TableSource</code> 时指定。</li></ol><p>一旦时间属性定义好，它就可以像普通列一样使用，也可以在时间相关的操作中使用，只要时间属性没有被修改，而是简单地从一个表传递到另一个表，它就仍然是一个有效的时间属性。</p><p>时间属性可以像普通的时间戳的列一样被使用和计算。一旦时间属性被用在了计算中，它就会被物化，进而变成一个普通的时间戳。普通的时间戳是无法跟 Flink 的时间以及watermark等一起使用的，所以普通的时间戳就无法用在时间相关的操作中（这句话是只限于被修改的普通时间戳，还是包含未被修改的时间戳）。</p><h3 id="处理时间"><a href="#处理时间" class="headerlink" title="处理时间"></a>处理时间</h3><blockquote><p>处理时间是基于机器的本地时间来处理数据，它是最简单的一种时间概念，但是它不能提供确定性。它既不需要从数据里获取时间，也不需要生成 watermark。</p></blockquote><p>定义处理时间的三种方法:</p><ol><li><p>在创建表的 DDL 中定义</p><blockquote><p>用 <code>PROCTIME()</code> 就可以定义处理时间，函数 <code>PROCTIME()</code> 的返回类型是 TIMESTAMP_LTZ </p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CREATE TABLE <span class="title">user_actions</span> <span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">  user_name STRING,</span></span></span><br><span class="line"><span class="params"><span class="function">  data STRING,</span></span></span><br><span class="line"><span class="params"><span class="function">  user_action_time AS PROCTIME()</span> -- 声明一个额外的列作为处理时间属性</span></span><br><span class="line"><span class="function">) <span class="title">WITH</span> <span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>在 DataStream 到 Table 转换时定义</p><blockquote><p>处理时间属性可以在 schema 定义的时候用 <code>.proctime</code> 后缀来定义。时间属性一定不能定义在一个已有字段上，所以它只能定义在 schema 定义的最后。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明一个额外的字段作为时间属性字段</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, $(<span class="string">&quot;user_name&quot;</span>), $(<span class="string">&quot;data&quot;</span>), $(<span class="string">&quot;user_action_time&quot;</span>).proctime());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(</span><br><span class="line">        Tumble.over(lit(<span class="number">10</span>).minutes())</span><br><span class="line">            .on($(<span class="string">&quot;user_action_time&quot;</span>))</span><br><span class="line">            .as(<span class="string">&quot;userActionWindow&quot;</span>));</span><br></pre></td></tr></table></figure></li><li><p>使用 TableSource 定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个由处理时间属性的 table source</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActionSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DefinedProctimeAttribute</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getReturnType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String[] names = <span class="keyword">new</span> String[] &#123;<span class="string">&quot;user_name&quot;</span> , <span class="string">&quot;data&quot;</span>&#125;;</span><br><span class="line">TypeInformation[] types = <span class="keyword">new</span> TypeInformation[] &#123;Types.STRING(), Types.STRING()&#125;;</span><br><span class="line"><span class="keyword">return</span> Types.ROW(names, types);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line"><span class="comment">// create stream</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...;</span><br><span class="line"><span class="keyword">return</span> stream;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getProctimeAttribute</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// 这个名字的列会被追加到最后，作为第三列</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;user_action_time&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register table source</span></span><br><span class="line">tEnv.registerTableSource(<span class="string">&quot;user_actions&quot;</span>, <span class="keyword">new</span> UserActionSource());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = tEnv</span><br><span class="line">.from(<span class="string">&quot;user_actions&quot;</span>)</span><br><span class="line">.window(Tumble</span><br><span class="line">    .over(lit(<span class="number">10</span>).minutes())</span><br><span class="line">    .on($(<span class="string">&quot;user_action_time&quot;</span>))</span><br><span class="line">    .as(<span class="string">&quot;userActionWindow&quot;</span>));</span><br></pre></td></tr></table></figure></li></ol><h3 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h3><blockquote><p>事件时间允许程序按照数据中包含的时间来处理，这样可以在有乱序或者晚到的数据的情况下产生一致的处理结果。为了能够处理乱序的事件，并且区分正常到达和晚到的事件，Flink 需要从事件中获取事件时间并且产生 <a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/concepts/time/">watermarks</a>。</p></blockquote><p>定义事件时间的三种方法:</p><ol><li><p>在 DDL 中定义</p><blockquote><p>WATERMARK 语句在一个已有字段上定义一个 watermark 生成表达式，同时标记这个已有字段为时间属性字段。</p></blockquote><p>Flink 支持和在 TIMESTAMP 列和 TIMESTAMP_LTZ 列上定义事件时间。</p><ul><li><p>如果源数据中的时间戳数据表示为年-月-日-时-分-秒，则通常为不带时区信息的字符串值，例如 <code>2020-04-15 20:13:40.564</code>，建议将事件时间属性定义在 <code>TIMESTAMP</code> 列上,</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CREATE TABLE <span class="title">user_actions</span> <span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">  user_name STRING,</span></span></span><br><span class="line"><span class="params"><span class="function">  data STRING,</span></span></span><br><span class="line"><span class="params"><span class="function">  user_action_time TIMESTAMP(<span class="number">3</span>)</span>,</span></span><br><span class="line"><span class="function">  -- 声明 user_action_time 是事件时间属性，并且用 延迟 5 秒的策略来生成 watermark</span></span><br><span class="line"><span class="function">  WATERMARK FOR user_action_time AS user_action_time - INTERVAL &#x27;5&#x27; SECOND</span></span><br><span class="line"><span class="function">) <span class="title">WITH</span> <span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">SELECT <span class="title">TUMBLE_START</span><span class="params">(user_action_time, INTERVAL <span class="string">&#x27;10&#x27;</span> MINUTE)</span>, <span class="title">COUNT</span><span class="params">(DISTINCT user_name)</span></span></span><br><span class="line"><span class="function">FROM user_actions</span></span><br><span class="line"><span class="function">GROUP BY <span class="title">TUMBLE</span><span class="params">(user_action_time, INTERVAL <span class="string">&#x27;10&#x27;</span> MINUTE)</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>如果源数据中的时间戳数据为带时区信息的字符串值，例如源数据中的时间戳数据表示为一个纪元 (epoch) 时间，通常是一个 long 值，例如 <code>1618989564564</code>，建议将事件时间属性定义在 <code>TIMESTAMP_LTZ</code> 列上：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CREATE TABLE <span class="title">user_actions</span> <span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function"> user_name STRING,</span></span></span><br><span class="line"><span class="params"><span class="function"> data STRING,</span></span></span><br><span class="line"><span class="params"><span class="function"> ts BIGINT,</span></span></span><br><span class="line"><span class="params"><span class="function"> time_ltz AS TO_TIMESTAMP_LTZ(ts, <span class="number">3</span>)</span>,</span></span><br><span class="line"><span class="function"> -- declare time_ltz as event time attribute and use 5 seconds delayed watermark strategy</span></span><br><span class="line"><span class="function"> WATERMARK FOR time_ltz AS time_ltz - INTERVAL &#x27;5&#x27; SECOND</span></span><br><span class="line"><span class="function">) <span class="title">WITH</span> <span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function"> ...</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">SELECT <span class="title">TUMBLE_START</span><span class="params">(time_ltz, INTERVAL <span class="string">&#x27;10&#x27;</span> MINUTE)</span>, <span class="title">COUNT</span><span class="params">(DISTINCT user_name)</span></span></span><br><span class="line"><span class="function">FROM user_actions</span></span><br><span class="line"><span class="function">GROUP BY <span class="title">TUMBLE</span><span class="params">(time_ltz, INTERVAL <span class="string">&#x27;10&#x27;</span> MINUTE)</span></span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>在 DataStream 到 Table 转换时定义</p><blockquote><p>事件时间属性可以用 <code>.rowtime</code> 后缀在定义 <code>DataStream</code> schema 的时候来定义。</p><p><a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/concepts/time/">时间戳和 watermark</a> 在这之前一定是在 <code>DataStream</code> 上已经定义好了。 在从 DataStream 转换到 Table 时，由于 <code>DataStream</code> 没有时区概念，因此 Flink 总是将 <code>rowtime</code> 属性解析成 <code>TIMESTAMP WITHOUT TIME ZONE</code> 类型，并且将所有事件时间的值都视为 UTC 时区的值。</p></blockquote><ul><li><p>Option 1</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 基于 stream 中的事件产生时间戳和 watermark</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明一个额外的逻辑字段作为事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, $(<span class="string">&quot;user_name&quot;</span>), $(<span class="string">&quot;data&quot;</span>), $(<span class="string">&quot;user_action_time&quot;</span>).rowtime());</span><br></pre></td></tr></table></figure></li><li><p>Option 2</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从第一个字段获取事件时间，并且产生 watermark</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第一个字段已经用作事件时间抽取了，不用再用一个新字段来表示事件时间了</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, $(<span class="string">&quot;user_action_time&quot;</span>).rowtime(), $(<span class="string">&quot;user_name&quot;</span>), $(<span class="string">&quot;data&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Usage:</span></span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble</span><br><span class="line">       .over(lit(<span class="number">10</span>).minutes())</span><br><span class="line">       .on($(<span class="string">&quot;user_action_time&quot;</span>))</span><br><span class="line">       .as(<span class="string">&quot;userActionWindow&quot;</span>));</span><br></pre></td></tr></table></figure></li></ul></li><li><p>使用 TableSource 定义</p><blockquote><p>事件时间属性可以在实现了 <code>DefinedRowTimeAttributes</code> 的 <code>TableSource</code> 中定义。<code>getRowtimeAttributeDescriptors()</code> 方法返回 <code>RowtimeAttributeDescriptor</code> 的列表，包含了描述事件时间属性的字段名字、如何计算事件时间、以及 watermark 生成策略等信息。</p><p>同时需要确保 <code>getDataStream</code> 返回的 <code>DataStream</code> 已经定义好了时间属性。</p><p> 只有在定义了 <code>StreamRecordTimestamp</code> 时间戳分配器的时候，才认为 <code>DataStream</code> 是有时间戳信息的。 只有定义了 <code>PreserveWatermarks</code> watermark 生成策略的 <code>DataStream</code> 的 watermark 才会被保留。反之，则只有时间字段的值是生效的。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个有事件时间属性的 table source</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActionSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getReturnType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String[] names = <span class="keyword">new</span> String[] &#123;<span class="string">&quot;user_name&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;user_action_time&quot;</span>&#125;;</span><br><span class="line">TypeInformation[] types =</span><br><span class="line">    <span class="keyword">new</span> TypeInformation[] &#123;Types.STRING(), Types.STRING(), Types.LONG()&#125;;</span><br><span class="line"><span class="keyword">return</span> Types.ROW(names, types);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 构造 DataStream</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="comment">// 基于 &quot;user_action_time&quot; 定义 watermark</span></span><br><span class="line">DataStream&lt;Row&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="keyword">return</span> stream;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;RowtimeAttributeDescriptor&gt; <span class="title">getRowtimeAttributeDescriptors</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// 标记 &quot;user_action_time&quot; 字段是事件时间字段</span></span><br><span class="line"><span class="comment">// 给 &quot;user_action_time&quot; 构造一个时间属性描述符</span></span><br><span class="line">RowtimeAttributeDescriptor rowtimeAttrDescr = <span class="keyword">new</span> RowtimeAttributeDescriptor(</span><br><span class="line"><span class="string">&quot;user_action_time&quot;</span>,</span><br><span class="line"><span class="keyword">new</span> ExistingField(<span class="string">&quot;user_action_time&quot;</span>),</span><br><span class="line"><span class="keyword">new</span> AscendingTimestamps());</span><br><span class="line">List&lt;RowtimeAttributeDescriptor&gt; listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr);</span><br><span class="line"><span class="keyword">return</span> listRowtimeAttrDescr;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register the table source</span></span><br><span class="line">tEnv.registerTableSource(<span class="string">&quot;user_actions&quot;</span>, <span class="keyword">new</span> UserActionSource());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = tEnv</span><br><span class="line">.from(<span class="string">&quot;user_actions&quot;</span>)</span><br><span class="line">.window(Tumble.over(lit(<span class="number">10</span>).minutes()).on($(<span class="string">&quot;user_action_time&quot;</span>)).as(<span class="string">&quot;userActionWindow&quot;</span>));</span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="format"><a href="#format" class="headerlink" title="format"></a>format</h3><ul><li><p>timestamp</p><p>可以将时间戳类型数据最高精确微秒(百万分之一秒)，数据类型定义为timestamp(N),N取值范围为0-6，默认为0，如需要精确到毫秒则设置为Timestamp(3)，如需要精确到微秒则设置为timestamp(6)，数据精度提高的代价是其内部存储空间的变大，但仍未改变时间戳类型的最小和最大取值范围。</p></li></ul><h3 id="connector-kafka"><a href="#connector-kafka" class="headerlink" title="connector kafka"></a>connector kafka</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;时间属性&quot;&gt;&lt;a href=&quot;#时间属性&quot; class=&quot;headerlink&quot; title=&quot;时间属性&quot;&gt;&lt;/a&gt;时间属性&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt; 像窗口（在 &lt;a href=&quot;https://nightlies.apache.org/flin</summary>
      
    
    
    
    <category term="flink" scheme="http://yoursite.com/categories/flink/"/>
    
    
    <category term="learn" scheme="http://yoursite.com/tags/learn/"/>
    
  </entry>
  
  <entry>
    <title>macos snippet</title>
    <link href="http://yoursite.com/2021/12/20/macos%20snippet/"/>
    <id>http://yoursite.com/2021/12/20/macos%20snippet/</id>
    <published>2021-12-20T03:25:00.000Z</published>
    <updated>2021-12-24T02:11:52.714Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Installing-pkg"><a href="#Installing-pkg" class="headerlink" title="Installing .pkg"></a>Installing .pkg</h3><p>a.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo installer -pkg /path/to/package.pkg -target /</span><br></pre></td></tr></table></figure><blockquote><p>will install the package in /Applications.</p></blockquote><p>is all that’s needed. Here <code>/</code> is the mount point of <code>Macintosh HD</code> volume. <code>-target</code> accepts path like <code>&quot;/Volumes/Macintosh HD&quot;</code>, or <code>/dev/disk0</code> also.</p><p>b.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">installer -pkg myapp.pkg -target CurrentUserHomeDirectory</span><br></pre></td></tr></table></figure><blockquote><p>will install the package in ~/Applications.</p></blockquote><h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><ul><li> <a href="https://stackoverflow.com/questions/10757169/location-of-my-cnf-file-on-macos">Location of my.cnf file on macOS</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Installing-pkg&quot;&gt;&lt;a href=&quot;#Installing-pkg&quot; class=&quot;headerlink&quot; title=&quot;Installing .pkg&quot;&gt;&lt;/a&gt;Installing .pkg&lt;/h3&gt;&lt;p&gt;a.&lt;/p&gt;
&lt;figure class</summary>
      
    
    
    
    <category term="macos" scheme="http://yoursite.com/categories/macos/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>docker image mysql</title>
    <link href="http://yoursite.com/2021/12/17/docker%20image%20mysql/"/>
    <id>http://yoursite.com/2021/12/17/docker%20image%20mysql/</id>
    <published>2021-12-17T10:57:57.000Z</published>
    <updated>2021-12-17T10:59:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mysql镜像制作"><a href="#mysql镜像制作" class="headerlink" title="mysql镜像制作"></a>mysql镜像制作</h2><ol><li><p>需要备份当前需要同步的全量数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it dlabel_mysql mysqldump -uroot -p123456 dls &gt; /path/to/backup.sql</span><br></pre></td></tr></table></figure><blockquote><p>注意事项：</p><p>其中dlabel_mysql，是在第二步中设置的name的名称</p><p>/path/to/backup.sql是导出sql的地址路径，根据操作系统不同，需要自行更改</p><p>假定以下操作是在/path/to的目录下</p></blockquote></li></ol><ol start="2"><li><p>在/path/to目录下创建Dockerfile文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Derived from official mysql image (our base image)</span></span><br><span class="line">FROM mysql:5.7.30</span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the content of the sql-scripts/ directory to your image</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> All scripts <span class="keyword">in</span> docker-entrypoint-initdb.d/ are automatically</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> executed during container startup</span></span><br><span class="line">COPY ./backup.sql /docker-entrypoint-initdb.d/</span><br></pre></td></tr></table></figure><blockquote><p>注意COPY指令中，backup.sql需要和操作1中的导出文件名保持一致</p></blockquote></li><li><p>创建镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t dlabel:mysql20211216 .</span><br></pre></td></tr></table></figure><blockquote><p>dlabel:mysql20211216是 REPOSITORY:TAG格式，可自行更改</p></blockquote></li><li><p>登录远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login hostAddress</span><br></pre></td></tr></table></figure><p>根据提示，输入用户名admin，密码Harbor12345</p></li><li><p>映射远程仓库REPOSITORY:TAG</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image tag dlabel:mysql20211216 hostAddress/dlabel/service:mysql20211216</span><br></pre></td></tr></table></figure><blockquote><p>其中dlabel:mysql20211216和操作3中保持一致</p><p>hostAddress/dlabel/service:mysql20211216，格式为hostAddress/library/REPOSITORY:TAG，其中可自行修改service:mysql20211216名称</p></blockquote></li><li><p>推送当地镜像到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push hostAddress/dlabel/service:mysql20211216</span><br></pre></td></tr></table></figure></li><li><p>登录<strong><a href="http://hostaddress/">http://hostAddress</a></strong>查看镜像上传情况</p></li><li><p>在镜像详情界面，点击“拉取命名”按钮进行命令复制，在终端执行命令即可拉取该镜像</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;mysql镜像制作&quot;&gt;&lt;a href=&quot;#mysql镜像制作&quot; class=&quot;headerlink&quot; title=&quot;mysql镜像制作&quot;&gt;&lt;/a&gt;mysql镜像制作&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;需要备份当前需要同步的全量数据&lt;/p&gt;
&lt;figure class</summary>
      
    
    
    
    <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>docker es</title>
    <link href="http://yoursite.com/2021/12/17/docker%20image%20es/"/>
    <id>http://yoursite.com/2021/12/17/docker%20image%20es/</id>
    <published>2021-12-17T10:51:56.000Z</published>
    <updated>2021-12-17T11:01:02.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文中hostAddress需要替换具体的ip地址</p></blockquote><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><blockquote><p>查看现有环境相关参数ulimit -a</p></blockquote><ul><li><p>设置文件句柄数，在**/etc/security/limits.conf**中设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># End of file</span><br><span class="line">* hard nofile 65536</span><br><span class="line">* soft nofile 65536</span><br></pre></td></tr></table></figure></li><li><p>修改max user processes进程数，在**/etc/security/limits.conf**中设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* soft nproc 65536</span><br><span class="line">* hard nproc 65536</span><br></pre></td></tr></table></figure></li><li><p>调整vm.max_map_count的大小，该参数会限制一个进程可以拥有的VMA(虚拟内存区域)的数量</p><p>通过修改**/etc/sysctl.conf**参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vm.max_map_count=655360</span><br></pre></td></tr></table></figure><p>然后执行<code>sysctl -p</code></p></li><li><p>调整stack size的大小（可选），在**/etc/security/limits.conf**中设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* soft stack 1024</span><br><span class="line">* hard stack 1024</span><br></pre></td></tr></table></figure></li></ul><h3 id="manual-init-data"><a href="#manual-init-data" class="headerlink" title="manual init data"></a>manual init data</h3><ul><li><p>create index</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &#x27;Content-Type: application/json&#x27; -d &#x27;@/data/es_mapping.json&#x27; -X PUT &#x27;http://localhost:9200/indexName&#x27;</span><br></pre></td></tr></table></figure></li><li><p>import data</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &#x27;Content-Type: application/json&#x27; --data-binary &#x27;@/data/es_init_data.txt&#x27; &#x27;http://localhost:9200/_bulk&#x27;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>拉取远程仓库镜像文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull hostAddress/dlabel/service:elasticsearch</span><br></pre></td></tr></table></figure></li><li><p>启动容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d --name es_origin -e ES_JAVA_POTS=&quot;-Xms6g -Xmx6g&quot; -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.1</span><br></pre></td></tr></table></figure></li></ul><h3 id="制作elasticsearch镜像"><a href="#制作elasticsearch镜像" class="headerlink" title="制作elasticsearch镜像"></a>制作elasticsearch镜像</h3><ol><li>导出ES的已有索引和数据</li></ol><ul><li><p>环境准备</p><ul><li>安装nodejs，安装文件地址<a href="https://nodejs.org/en/download/">nodejs</a></li><li>安装elasticdump，安装命令<code>npm install -g elasticdump</code></li></ul></li><li><p>导出es索引文件<strong>es_mapping.json</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">/$</span><span class="bash">nodejs_home/lib/node_modules/elasticdump/bin/elasticdump \               --input=http://127.0.0.1:9200/indexName \</span></span><br><span class="line"><span class="bash">  --output=/data/es_mapping.json \</span></span><br><span class="line"><span class="bash">  --<span class="built_in">type</span>=mapping</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：$nodejs_home代表nodejs的安装目录</p></blockquote></li><li><p>导出es数据<strong>es_init_data.txt</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">/$</span><span class="bash">nodejs_home/lib/node_modules/elasticdump/bin/elasticdump \ </span>          </span><br><span class="line">  --input=http://127.0.0.1:9200/indexName \</span><br><span class="line">  --output=/data/es_init_data.txt \</span><br><span class="line">  --searchBody &#x27;&#123;&quot;query&quot;:&#123;&quot;match_all&quot;:&#123; &#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li><p>编写es数据初始化脚本 <strong>initEs.sh</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">create index</span></span><br><span class="line">curl -H &#x27;Content-Type: application/json&#x27; -d &#x27;@/data/es_mapping.json&#x27; -X PUT &#x27;http://127.0.0.1:9200/indexName&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">import data</span></span><br><span class="line">curl -H &#x27;Content-Type: application/json&#x27; --data-binary &#x27;@/data/es_init_data.txt&#x27; &#x27;http://127.0.0.1:9200/_bulk&#x27;</span><br></pre></td></tr></table></figure><blockquote><p>initEs.sh文件同1,2操作中的文件存放路径保持一致，均放在/data目录下</p></blockquote></li><li><p>在/data目录下创建Dockerfile文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM elasticsearch:7.16.1</span><br><span class="line">COPY ./data/* /data/</span><br><span class="line">RUN chown -R elasticsearch:root /data </span><br><span class="line">USER elasticsearch</span><br><span class="line">RUN elasticsearch -E discovery.type=single-node -p /tmp/epid &amp; /bin/bash /data/wait-for-it.sh -t 0 localhost:9200 -- /data/initEs.sh; kill $(cat /tmp/epid) &amp;&amp; wait $(cat /tmp/epid); exit 0;</span><br></pre></td></tr></table></figure></li><li><p>创建镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t dlabel:elasticsearch .</span><br></pre></td></tr></table></figure><blockquote><p>dlabel:es是 REPOSITORY:TAG格式，可自行更改</p></blockquote></li><li><p>登录远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login hostAddress</span><br></pre></td></tr></table></figure><p>根据提示，输入用户名admin，密码Harbor12345</p></li><li><p>映射远程仓库REPOSITORY:TAG</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image tag dlabel:elasticsearch hostAddress/dlabel/service:elasticsearch</span><br></pre></td></tr></table></figure><blockquote><p>其中dlabel:elasticsearch和操作3中保持一致</p></blockquote></li><li><p>推送当地镜像到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push hostAddress/dlabel/service:elasticsearch</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;文中hostAddress需要替换具体的ip地址&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h3&gt;&lt;block</summary>
      
    
    
    
    <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>docker compose</title>
    <link href="http://yoursite.com/2021/12/17/docker%20compose/"/>
    <id>http://yoursite.com/2021/12/17/docker%20compose/</id>
    <published>2021-12-17T03:21:10.000Z</published>
    <updated>2021-12-17T06:52:28.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Docker-File-vs-Docker-Compose"><a href="#Docker-File-vs-Docker-Compose" class="headerlink" title="Docker File vs Docker Compose"></a>Docker File vs Docker Compose</h3><p>Dockerfile is what’s used to create a container image, and a Compose file is what’s used to deploy an instance of that image as a container.</p><blockquote><p>Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。</p></blockquote><h4 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h4><p>Dockerfile the predecessor of a container image. You build an image from a Dockerfile. A typical Dockerfile contains special build instructions, commands like <code>RUN</code>, <code>ADD</code>, <code>COPY</code>, <code>ENTRYPOINT</code>, etc.</p><h4 id="Compose-file"><a href="#Compose-file" class="headerlink" title="Compose file"></a>Compose file</h4><p>Compose files are used in two types of deployments: in the non-cluster deployment with <code>docker-compose</code> and a cluster deployment with <code>docker swarm</code>.</p><p>Compose files are used in two types of deployments: in the non-cluster deployment with <code>docker-compose</code> and a cluster deployment with <code>docker swarm</code>.</p><p>To distinguish the two types, I’m going to address the compose file responsible for cluster deployment as stack files. I’ll talk about stack files in a moment.</p><p>Compose files are part of a tool called <code>docker-compose</code>. It’s a client application to the docker daemon server, kind of like the <code>docker</code> CLI client, but instead of typing the whole <code>run</code> commands every time, with <code>docker-compose</code> you can re-use the same YAML file over and over again, and deploy the same container with the same configuration as you did in the first time.</p><p>It’s more readable, more maintainable, more intuitive. A single compose file can contain multiple container deployment configurations.</p><ul><li><p>执行<code>docker-compose up</code>，报错</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Couldn’t connect to Docker daemon at http+docker:<span class="comment">//localhost - is it running?</span></span><br></pre></td></tr></table></figure><p>其中<code>docker-compose.yml</code>信息如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3.7&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">web:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">.</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;5000:5000&quot;</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;redis:alpine&quot;</span></span><br></pre></td></tr></table></figure><ul><li><p>解决，使用sudo权限</p><ol><li><p>Add user to docker group (if not already added)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure></li><li><p>create a symbolic link to /usr/bin using the following command</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure></li><li><p>Restart docker service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></li><li><p>execute</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker-compose up</span><br></pre></td></tr></table></figure></li></ol></li></ul></li></ul><p>运行docker compose up或docker compose up -d(后台运行)运行您的整个应用程序。 <code>注意：每次修改任一配置文件后，都要使用 docker-compose up --build 重新构建</code></p><blockquote><p>有了docker-compose，当我们想启动多个服务时，无需再一个一个进行docker run操作，而只需要编写docker-compose.yml配置文件，即可一次运行你的全部服务。</p></blockquote><hr><table><thead><tr><th>属性</th><th>描述</th></tr></thead><tbody><tr><td>docker-compose build</td><td>(构建yml中某个服务的镜像)</td></tr><tr><td>docker-compose ps</td><td>(查看已经启动的服务状态）</td></tr><tr><td>docker-compose kill</td><td>(停止某个服务）</td></tr><tr><td>docker-compose logs</td><td>(可以查看某个服务的log）</td></tr><tr><td>docker-compose port</td><td>(打印绑定的public port）</td></tr><tr><td>docker-compose pull</td><td>(pull服务镜像)</td></tr><tr><td>docker-compose up</td><td>(启动yml定义的所有服务）</td></tr><tr><td>docker-compose stop</td><td>(停止yml中定义的所有服务）</td></tr><tr><td>docker-compose start</td><td>(启动被停止的yml中的所有服务）</td></tr><tr><td>docker-compose kill</td><td>(强行停止yml中定义的所有服务）</td></tr><tr><td>docker-compose rm</td><td>（删除yml中定义的所有服务）</td></tr><tr><td>docker-compose restart</td><td>(重启yml中定义的所有服务）</td></tr><tr><td>docker-compose scale</td><td>(扩展某个服务的个数，可以向上或向下）</td></tr><tr><td>docker-compose version</td><td>（查看compose的版本）</td></tr></tbody></table><p>日志输出  </p><p>终端输出：<code>docker-compose --verbose up $service_name</code></p><p>或者docker-compose.yml配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stdin_open: true</span><br><span class="line">tty: true</span><br></pre></td></tr></table></figure><h3 id="镜像重新编译"><a href="#镜像重新编译" class="headerlink" title="镜像重新编译"></a>镜像重新编译</h3><p>如果修改了 Dockerfile内容里面相关的信息，需要重新编译镜像，如果使用docker compose，则需要使用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up --build</span><br></pre></td></tr></table></figure><h3 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Docker-File-vs-Docker-Compose&quot;&gt;&lt;a href=&quot;#Docker-File-vs-Docker-Compose&quot; class=&quot;headerlink&quot; title=&quot;Docker File vs Docker Compose&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>dockerFile</title>
    <link href="http://yoursite.com/2021/12/17/dockerFile/"/>
    <id>http://yoursite.com/2021/12/17/dockerFile/</id>
    <published>2021-12-17T03:20:31.000Z</published>
    <updated>2021-12-17T03:20:35.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><strong>RUN</strong> is executed while the image is being build</p><p>while <strong>ENTRYPOINT</strong> is executed after the image has been built.</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RUN&lt;/strong&gt; is executed while the image is being build&lt;/p&gt;
&lt;p&gt;while &lt;strong&gt;ENTRYPOINT&lt;/strong&gt; is executed after the i</summary>
      
    
    
    
    <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>docker migration</title>
    <link href="http://yoursite.com/2021/12/14/docker%20migration/"/>
    <id>http://yoursite.com/2021/12/14/docker%20migration/</id>
    <published>2021-12-14T06:52:21.000Z</published>
    <updated>2021-12-16T07:01:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>reference </p><p><a href="https://bobcares.com/blog/move-docker-container-to-another-host/">5 ways to move Docker container to another host</a></p><p><a href="https://morioh.com/p/d8d9e7732952">Build a Docker Image with MySQL Database</a></p><h2 id="Plan-A"><a href="#Plan-A" class="headerlink" title="Plan A"></a>Plan A</h2><h3 id="Step1-create-an-Image-From-a-Container"><a href="#Step1-create-an-Image-From-a-Container" class="headerlink" title="Step1    create an Image From a Container"></a>Step1    create an Image From a Container</h3><blockquote><p>Create a new image from a container’s changes</p><p><a href="https://docs.docker.com/engine/reference/commandline/commit/">commit command</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</span><br></pre></td></tr></table></figure><ul><li><p>options</p><table><thead><tr><th>Name, shorthand</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><code>--author</code> , <code>-a</code></td><td></td><td>Author (e.g., “will brook”)</td></tr><tr><td><code>--change</code> , <code>-c</code></td><td></td><td>Apply Dockerfile instruction to the created image</td></tr><tr><td><code>--message</code> , <code>-m</code></td><td></td><td>Commit message</td></tr><tr><td><code>--pause</code> , <code>-p</code></td><td><code>true</code></td><td>Pause container during commit</td></tr></tbody></table></li></ul><h3 id="Step-2-export-the-image-to-a-file"><a href="#Step-2-export-the-image-to-a-file" class="headerlink" title="Step 2    export the image to a file"></a>Step 2    export the image to a file</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker save -o /path/to/your_image.tar your_image_name</span><br></pre></td></tr></table></figure><h3 id="Step-3-load-the-Docker-image-file"><a href="#Step-3-load-the-Docker-image-file" class="headerlink" title="Step 3 load the Docker image file"></a>Step 3 load the Docker image file</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker load -i your_image.tar</span><br></pre></td></tr></table></figure><hr><h2 id="Plan-B"><a href="#Plan-B" class="headerlink" title="Plan B"></a>Plan B</h2><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>First save the new image by finding the container ID (using <a href="https://docs.docker.com/engine/reference/commandline/ps/"><code>docker container ls</code></a>) and then committing it to a new image name. Note that only <code>a-z0-9-_.</code> are allowed when naming images:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> create image from container</span></span><br><span class="line">docker container commit c16378f943fe rhel-httpd:latest</span><br></pre></td></tr></table></figure><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p> tag the image with the host name or IP address, and the port of the registry:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> re-tag repository:tag info about image</span></span><br><span class="line">docker image tag rhel-httpd:latest registry-host:5000/myadmin/rhel-httpd:latest</span><br><span class="line">or</span><br><span class="line">docker tag 0e5574283393 registry-host:5000/myadmin/rhel-httpd:latest</span><br></pre></td></tr></table></figure><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>log in from Docker client:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login &lt;harbor_address&gt;</span><br></pre></td></tr></table></figure><h3 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h3><p>push the image to the registry using the image ID. </p><p>In this example the registry is on host named <code>registry-host</code> and listening on port <code>5000</code>. (harbor默认配置端口80，详见harbor.yml)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> push repository:tag,</span></span><br><span class="line">docker image push registry-host:5000/myadmin/rhel-httpd:latest</span><br><span class="line">or</span><br><span class="line">docker push registry-host:5000/myname/myimage</span><br></pre></td></tr></table></figure><h2 id="Pull-Image-from-Harbor"><a href="#Pull-Image-from-Harbor" class="headerlink" title="Pull Image from Harbor"></a>Pull Image from Harbor</h2><p><a href="https://goharbor.io/docs/2.0.0/install-config/run-installer-script/#connect-http">Connecting to Harbor via HTTP</a></p><h3 id="Step-1-1"><a href="#Step-1-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>add the option <code>--insecure-registry</code> to your client’s Docker daemon. By default, the daemon file is located at <code>/etc/docker/daemon.json</code>.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;insecure-registries&quot; : [&quot;ip:port&quot;, &quot;0.0.0.0&quot;] #如果port为80，则可省略</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Restart Docker Engine.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="Step-2-1"><a href="#Step-2-1" class="headerlink" title="Step 2"></a>Step 2</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull hostAddress/library/REPOSITORY:TAG</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;reference &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://bobcares.com/blog/move-docker-container-to-another-host/&quot;&gt;5 ways to move Docker container to another h</summary>
      
    
    
    
    <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
    <category term="learn" scheme="http://yoursite.com/tags/learn/"/>
    
  </entry>
  
  <entry>
    <title>docker container</title>
    <link href="http://yoursite.com/2021/12/13/docker%20container/"/>
    <id>http://yoursite.com/2021/12/13/docker%20container/</id>
    <published>2021-12-13T10:07:46.000Z</published>
    <updated>2021-12-17T11:01:34.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">查看正在运行</span><br><span class="line">docker ps</span><br><span class="line">查看所有</span><br><span class="line">docker ps -a</span><br><span class="line">启动</span><br><span class="line">docker start 容器名或容器id</span><br><span class="line">终止</span><br><span class="line">docker stop [NAME]/[CONTAINER ID]:将容器退出。</span><br><span class="line">docker kill [NAME]/[CONTAINER ID]:强制停止一个容器。</span><br><span class="line"></span><br><span class="line">查看容器端口</span><br><span class="line">docker port 容器名或容器id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line">docker rm -f 容器id</span><br><span class="line">导出</span><br><span class="line">docker export 容器id &gt; xxx.tar</span><br><span class="line">导入</span><br><span class="line">docker import - test/xxx:v1</span><br><span class="line">重启</span><br><span class="line">docker restart $container_id</span><br><span class="line">日志</span><br><span class="line">docker logs $container_id</span><br></pre></td></tr></table></figure><h3 id="查看container现在工作网络模式"><a href="#查看container现在工作网络模式" class="headerlink" title="查看container现在工作网络模式"></a>查看container现在工作网络模式</h3><ul><li><p>列出docker的所有网络模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network ls</span><br></pre></td></tr></table></figure></li><li><p>针对bridge和host分别查找有哪些container在其中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker network inspect bridge</span><br><span class="line">docker network inspect host</span><br></pre></td></tr></table></figure></li><li><p>直接查看container的信息，找到network段查看。或者用grep筛选出network。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker inspect 容器名/容器ID</span><br><span class="line">docker inspect 容器名/容器ID | grep -i “network” # 其中grep的“-i”表示不区分大小写。</span><br></pre></td></tr></table></figure></li></ul><h4 id="Exit-Codes"><a href="#Exit-Codes" class="headerlink" title="Exit Codes"></a>Exit Codes</h4><p>Common exit codes associated with docker containers are:</p><ul><li><p><strong>Exit Code 0</strong>: Absence of an attached foreground process</p></li><li><p><strong>Exit Code 1</strong>: Indicates failure due to application error</p></li><li><p><strong>Exit Code 137</strong>: Indicates failure as container received SIGKILL (Manual intervention or ‘oom-killer’ [OUT-OF-MEMORY])</p></li><li><p><strong>Exit Code 139</strong>: Indicates failure as container received SIGSEGV</p></li><li><p><strong>Exit Code 143</strong>: Indicates failure as container received SIGTERM</p></li><li><p><strong>Exit Code 126</strong>: Permission problem or command is not executable</p></li><li><p><strong>Exit Code 127</strong>: Possible typos in shell script with unrecognizable characters</p></li></ul><h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><ul><li><p>密码123456</p></li><li><p>创建容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql-server -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7</span><br><span class="line">注意：</span><br><span class="line">-d:让容器在后台运行</span><br><span class="line">-P(大写):是容器内部端口随机映射到主机的高端口</span><br><span class="line">-p(小写):是容器内部端口绑定到指定的主机端口</span><br></pre></td></tr></table></figure></li><li><p>进入容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql-server /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>访问</p><p><code>docker exec -it mysql-server mysql -uroot -p</code></p></li><li><p>修改root 可以通过任何客户端连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;</span><br></pre></td></tr></table></figure></li><li><p>从外部访问docker mysql-server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h127.0.0.1 -P3306 -uroot -p</span><br></pre></td></tr></table></figure></li><li><p>导入sql文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先将文件导入到容器</span><br><span class="line">#docker cp **.sql 容器名:/root/</span><br><span class="line">进入容器</span><br><span class="line">#docker exec -ti 容器名或ID sh</span><br><span class="line">登录数据库</span><br><span class="line"># mysql -uroot -p </span><br><span class="line">将文件导入数据库</span><br><span class="line">source 数据库名 &lt; /root/***.sql</span><br></pre></td></tr></table></figure></li><li><p>导出数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it  mysql-server（容器名） mysqldump -uroot -p123456 数据库名称 &gt; /opt/sql_bak/test_db.sql（导出表格路径）</span><br></pre></td></tr></table></figure></li></ul><h3 id="portainer"><a href="#portainer" class="headerlink" title="portainer"></a>portainer</h3><ul><li><p>密码重置</p><ul><li><p>下载帮助镜像portainer/helper-reset-password</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull portainer/helper-reset-password</span><br></pre></td></tr></table></figure></li><li><p>停止运行的portainer</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop &quot;id-portainer-container&quot;</span><br></pre></td></tr></table></figure></li><li><p>运行重置命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -v portainer_data:/data portainer/helper-reset-password</span><br></pre></td></tr></table></figure></li><li><p>结果</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2020</span>/<span class="number">06</span>/<span class="number">04</span> <span class="number">00</span>:<span class="number">13</span>:<span class="number">58</span> Password successfully updated <span class="keyword">for</span> user: admin</span><br><span class="line"><span class="number">2020</span>/<span class="number">06</span>/<span class="number">04</span> <span class="number">00</span>:<span class="number">13</span>:<span class="number">58</span> Use the following password to login: &amp;<span class="number">_4</span>#\<span class="number">3</span>^<span class="number">5</span>V8vLTd)E<span class="string">&quot;NWiJBs26G*9HPl1</span></span><br></pre></td></tr></table></figure></li><li><p>重新运行portainer,密码 为👆重置的 &amp;_4#\3^5V8vLTd)E”NWiJBs26G*9HPl1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start &quot;id-portainer-container&quot;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>现在密码为 admin/admin</p></li></ul><ul><li><p>重新安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d -p 8000:8000 -p 9443:9443 --name portainer \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">    -v portainer_data:/data \</span><br><span class="line">    cr.portainer.io/portainer/portainer-ce:2.9.3</span><br></pre></td></tr></table></figure></li></ul><h3 id="nacos"><a href="#nacos" class="headerlink" title="nacos"></a>nacos</h3><ul><li><p>run</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name nacos -p 8848:8848 -e PREFER_HOST_MODE=hostname -e MODE=standalone nacos/nacos-server</span><br></pre></td></tr></table></figure><ul><li><p>Linux memory is insufficient</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -e JVM_XMS=256m -e JVM_XMX=256m --env MODE=standalone --name nacos -d -p 8848:8848 nacos/nacos-server</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><blockquote><p>使用docker-compose up redis启动容器时，如果配置自定义配置文件 redis.conf，需要设置</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bind 0.0.0.0</span><br><span class="line">daemonize no</span><br></pre></td></tr></table></figure><blockquote><p>docker-compose.yml文件内容</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3.7&quot;</span>                                                                            <span class="attr">services:</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;redis:alpine&quot;</span></span><br><span class="line">    <span class="attr">stdin_open:</span> <span class="literal">true</span> <span class="comment">#打开标准输入，可以接受外部输入。</span></span><br><span class="line">    <span class="attr">tty:</span> <span class="literal">true</span>  <span class="comment">#模拟一个伪终端。</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/docker/projects/test/redis.conf:/data/redis.conf</span> <span class="comment"># 主机路径:容器路径</span></span><br><span class="line">    <span class="comment">#   - /docker/projects/test/redis/data:/data</span></span><br><span class="line">    <span class="comment">#   - /docker/projects/test/redis/logs:/logs</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">redis-server</span> <span class="string">--include</span> <span class="string">/data/redis.conf</span></span><br></pre></td></tr></table></figure><blockquote><p>使用 docker-compose –verbose up redis启动，可查看启动详情</p></blockquote><h3 id="修改已有容器的端口映射"><a href="#修改已有容器的端口映射" class="headerlink" title="修改已有容器的端口映射"></a>修改已有容器的端口映射</h3><ol><li><p>停止容器 </p></li><li><p>停止docker服务(systemctl stop docker) </p></li><li><p>修改这个容器的hostconfig.json文件中的端口（原帖有人提到，如果config.v2.json里面也记录了端口，也要修改）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /var/lib/docker/3b6ef264a040* #这里是CONTAINER ID</span><br><span class="line">vi hostconfig.json</span><br><span class="line">如果之前没有端口映射, 应该有这样的一段:</span><br><span class="line">&quot;PortBindings&quot;:&#123;&#125;</span><br><span class="line">增加一个映射, 这样写:</span><br><span class="line">&quot;PortBindings&quot;:&#123;&quot;3306/tcp&quot;:[&#123;&quot;HostIp&quot;:&quot;&quot;,&quot;HostPort&quot;:&quot;3307&quot;&#125;]&#125;</span><br><span class="line">前一个数字是容器端口, 后一个是宿主机端口. </span><br><span class="line">而修改现有端口映射更简单, 把端口号改掉就行.</span><br></pre></td></tr></table></figure></li><li><p>启动docker服务(systemctl start docker) </p></li><li><p>启动容器</p></li></ol><h3 id="配置容器的镜像源（安装vim）"><a href="#配置容器的镜像源（安装vim）" class="headerlink" title="配置容器的镜像源（安装vim）"></a>配置容器的镜像源（安装vim）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line"></span><br><span class="line">echo &quot;deb http://mirrors.163.com/debian/ jessie main non-free contrib&quot; &gt;/etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">echo &quot;deb http://mirrors.163.com/debian/ jessie-proposed-updates main non-free contrib&quot; &gt;&gt;/etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">echo &quot;deb-src http://mirrors.163.com/debian/ jessie main non-free contrib&quot; &gt;&gt;/etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">echo &quot;deb-src http://mirrors.163.com/debian/ jessie-proposed-updates main non-free contrib&quot; &gt;&gt;/etc/apt/sources.list </span><br><span class="line"><span class="meta">#</span><span class="bash">更新安装源</span> </span><br><span class="line">apt-get update </span><br><span class="line"><span class="meta">#</span><span class="bash">如果下载过程中卡在[waiting <span class="keyword">for</span> headers] 删除/var/cache/apt/archives/下的所有文件</span> </span><br><span class="line"><span class="meta">#</span><span class="bash">安装vim</span> </span><br><span class="line">apt-get install vim</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;容器&quot;&gt;&lt;a href=&quot;#容器&quot; class=&quot;headerlink&quot; title=&quot;容器&quot;&gt;&lt;/a&gt;容器&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span </summary>
      
    
    
    
    <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>jconsole snippet</title>
    <link href="http://yoursite.com/2021/12/11/jconsole%20snippet/"/>
    <id>http://yoursite.com/2021/12/11/jconsole%20snippet/</id>
    <published>2021-12-11T13:05:29.000Z</published>
    <updated>2021-12-11T13:13:07.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="jconsole配置远程监控"><a href="#jconsole配置远程监控" class="headerlink" title="jconsole配置远程监控"></a>jconsole配置远程监控</h3><ul><li><p>远程jvm进程需配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env.java.opts: </span><br><span class="line">-Dcom.sun.management.jmxremote </span><br><span class="line">-Dcom.sun.management.jmxremote.port=9999</span><br><span class="line">-Dcom.sun.management.jmxremote.authenticate=false </span><br><span class="line">-Dcom.sun.management.jmxremote.ssl=false</span><br></pre></td></tr></table></figure><blockquote><p>其中9999为指定监控端口</p></blockquote></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;jconsole配置远程监控&quot;&gt;&lt;a href=&quot;#jconsole配置远程监控&quot; class=&quot;headerlink&quot; title=&quot;jconsole配置远程监控&quot;&gt;&lt;/a&gt;jconsole配置远程监控&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;远程jvm进程需配置&lt;/p</summary>
      
    
    
    
    <category term="java" scheme="http://yoursite.com/categories/java/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>ignite issue</title>
    <link href="http://yoursite.com/2021/12/09/ignite%20issue/"/>
    <id>http://yoursite.com/2021/12/09/ignite%20issue/</id>
    <published>2021-12-09T07:22:02.000Z</published>
    <updated>2021-12-10T07:08:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="BinaryObjectException-Conflicting-enum-values"><a href="#BinaryObjectException-Conflicting-enum-values" class="headerlink" title="BinaryObjectException: Conflicting enum values"></a>BinaryObjectException: Conflicting enum values</h3><ul><li><p>原因</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">存入ignite的数据格式</span><br><span class="line">key: String , value: Map&lt;Enum, BigDecimal&gt;</span><br><span class="line">Enum类型包含</span><br><span class="line">&#123;A,B,C&#125;</span><br><span class="line"></span><br><span class="line">在之后由于业务变更，需要新增新的enum项目，并添加D在A与B之间</span><br><span class="line">&#123;A,D,B,C&#125;</span><br></pre></td></tr></table></figure></li><li><p>分析</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于在数据存入ignite之后，ignite会保存数据相关的schema信息,此时在enum项目之间修改item，会打乱之前的index</span><br></pre></td></tr></table></figure></li><li><p>解决 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">方法一：</span><br><span class="line">更改enum类的名称，不再使用原有的schema信息</span><br><span class="line">方法二：</span><br><span class="line">enum类新增项目时，需要在最后面添加，避免打乱已有的schema索引</span><br><span class="line">方法三（未验证）：</span><br><span class="line">删除 $IGNITE_HOME/work/binary_meta/Nodex里面的文件</span><br></pre></td></tr></table></figure></li><li><p>官方说明</p><blockquote><ul><li>You cannot change the types of existing fields.</li><li>You cannot change the order of enum values or add new constants at the beginning or in the middle of the list of enum’s values. You can add new constants to the end of the list though.</li></ul></blockquote></li></ul><ul><li><strong>处理conflict enum values, 需要清除数据</strong></li></ul><blockquote><p>需要清理 $IGNITE_HOME/work/db目录下的 binary_meta、marshaller</p><p>需要验证是否清理 storagePath、walPath、walArchivePath</p></blockquote><h3 id="gc"><a href="#gc" class="headerlink" title="gc"></a>gc</h3><p><img src="img/ignite/gc_collect.png"></p><blockquote><p>3s进行gc  (110060-52672)/1024=56.04G</p></blockquote><p><img src="img/ignite/gc_2.png"></p><blockquote><p>17s进行gc (109952-52658)/1024=55.95G</p></blockquote><h3 id="IgniteCacheException"><a href="#IgniteCacheException" class="headerlink" title="IgniteCacheException"></a>IgniteCacheException</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ERROR org<span class="variable">.apache</span><span class="variable">.ignite</span><span class="variable">.spi</span><span class="variable">.communication</span><span class="variable">.tcp</span><span class="variable">.TcpCommunicationSpi</span>  [] - Failed to send message to remote node [node=ZookeeperClusterNode [id=<span class="number">1</span>c8a032d-<span class="number">042</span>e-<span class="number">4386</span>-<span class="number">9</span>ce8-<span class="number">2605</span>c0699304, addrs=[<span class="number">17</span><span class="variable">.9</span><span class="variable">.11</span><span class="variable">.11</span>], order=<span class="number">1</span>, loc=false, client=false], msg=GridIoMessage [plc=<span class="number">2</span>, topic=TOPIC_CACHE, topicOrd=<span class="number">8</span>, ordered=false, timeout=<span class="number">0</span>, skipOnTimeout=false, msg=GridNearLockRequest [topVer=AffinityTopologyVersion [topVer=<span class="number">358</span>, minorTopVer=<span class="number">0</span>], miniId=<span class="number">1</span>, dhtVers=GridCacheVersion[] [<span class="literal">null</span>], subjId=a5dbdc1d-e76e-<span class="number">49</span>c2-<span class="number">85</span>d7-ed7f1c7db7bd, taskNameHash=<span class="number">0</span>, createTtl=-<span class="number">1</span>, accessTtl=-<span class="number">1</span>, flags=<span class="number">3</span>, txLbl=<span class="literal">null</span>, filter=<span class="literal">null</span>, <span class="keyword">super</span>=GridDistributedLockRequest [nodeId=a5dbdc1d-e76e-<span class="number">49</span>c2-<span class="number">85</span>d7-ed7f1c7db7bd, nearXidVer=GridCacheVersion [topVer=<span class="number">245500806</span>, order=<span class="number">1638786801426</span>, nodeOrder=<span class="number">336</span>], threadId=<span class="number">11960694</span>, futId=<span class="number">96</span>c1bf42d71-<span class="number">90702925</span>-<span class="number">3</span>ef9-<span class="number">4</span>c70-b7a7-<span class="number">4</span>be2fb6d75ba, timeout=<span class="number">0</span>, isInTx=true, isInvalidate=false, isRead=true, isolation=REPEATABLE_READ, retVals=[true], txSize=<span class="number">0</span>, flags=<span class="number">0</span>, keysCnt=<span class="number">1</span>, <span class="keyword">super</span>=GridDistributedBaseMessage [ver=GridCacheVersion [topVer=<span class="number">245500806</span>, order=<span class="number">1638786801426</span>, nodeOrder=<span class="number">336</span>], committedVers=<span class="literal">null</span>, rolledbackVers=<span class="literal">null</span>, cnt=<span class="number">1</span>, <span class="keyword">super</span>=GridCacheIdMessage [cacheId=-<span class="number">182240380</span>, <span class="keyword">super</span>=GridCacheMessage [msgId=<span class="number">1360862</span>, depInfo=<span class="literal">null</span>, lastAffChangedTopVer=AffinityTopologyVersion [topVer=<span class="number">336</span>, minorTopVer=<span class="number">0</span>], err=<span class="literal">null</span>, skipPrepare=false]]]]]]]</span><br><span class="line"></span><br><span class="line">org<span class="variable">.apache</span><span class="variable">.ignite</span><span class="variable">.IgniteCheckedException</span>: Failed to connect to node due to unrecoverable exception (is node still alive?). Make sure that each ComputeTask <span class="keyword">and</span> cache Transaction has a timeout set in order to prevent parties from waiting <span class="keyword">forever</span> in <span class="keyword">case</span> of network issues [nodeId=d0a258e5-ec1b-<span class="number">4</span>f79-<span class="number">89</span>ad-<span class="number">80</span>c27708f895, addrs=[x/x<span class="variable">.x</span><span class="variable">.x</span><span class="variable">.x</span>:<span class="number">47100</span>], err= <span class="keyword">class</span> org<span class="variable">.apache</span><span class="variable">.ignite</span><span class="variable">.IgniteCheckedException</span>: Remote node does <span class="keyword">not</span> observe current node in topology : d0a258e5-ec1b-<span class="number">4</span>f79-<span class="number">89</span>ad-<span class="number">80</span>c27708f895]</span><br><span class="line">    </span><br><span class="line">Caused by: org<span class="variable">.apache</span><span class="variable">.ignite</span><span class="variable">.IgniteCheckedException</span>: Remote node does <span class="keyword">not</span> observe current node in topology : d0a258e5-ec1b-<span class="number">4</span>f79-<span class="number">89</span>ad-<span class="number">80</span>c27708f895</span><br></pre></td></tr></table></figure><ul><li>gc的策略</li><li>ignite client的异常捕获</li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;BinaryObjectException-Conflicting-enum-values&quot;&gt;&lt;a href=&quot;#BinaryObjectExce</summary>
      
    
    
    
    <category term="ignite" scheme="http://yoursite.com/categories/ignite/"/>
    
    
    <category term="issue" scheme="http://yoursite.com/tags/issue/"/>
    
  </entry>
  
  <entry>
    <title>Elastic Stack learn</title>
    <link href="http://yoursite.com/2021/12/03/elasticstatic/"/>
    <id>http://yoursite.com/2021/12/03/elasticstatic/</id>
    <published>2021-12-03T01:26:30.000Z</published>
    <updated>2021-12-17T07:31:01.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>ELK</strong></p><blockquote><p>Elasticsearch 是一个搜索和分析引擎。Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。Kibana 则可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化。</p></blockquote><p>Elasticsearch is the living heart of what is today’s the most popular log analytics platform — the ELK Stack (Elasticsearch, Logstash and Kibana). Elasticsearch’s role is so central that it has become synonymous with the name of the stack itself.</p><p>Elasticsearch behaves like a REST API, so you can use either the <code>POST</code> or the <code>PUT</code> method to add data to it. You use <code>PUT</code> when you know the or want to specify the <code>id</code> of the data item, or <code>POST</code> if you want Elasticsearch to generate an <code>id</code> for the data item:</p><h3 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h3><h4 id="max-file-descriptors-4096-for-elasticsearch-process-is-too-low-increase-to-at-least-65535"><a href="#max-file-descriptors-4096-for-elasticsearch-process-is-too-low-increase-to-at-least-65535" class="headerlink" title="max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]"></a>max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]</h4><p>If you want to increase the limit shown by <code>ulimit -n</code>, you should:</p><ul><li><p>Modify <code>/etc/systemd/user.conf</code> and <code>/etc/systemd/system.conf</code> with the following line (this takes care of graphical login):</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DefaultLimitNOFILE=65535</span><br></pre></td></tr></table></figure></li><li><p>Modify <code>/etc/security/limits.conf</code> with the following lines (this takes care of non-GUI login):</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* hard nofile 65535</span><br><span class="line">* soft nofile 65535</span><br></pre></td></tr></table></figure></li><li><p>Reboot your computer for changes to take effect.</p></li><li><p>check</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ulimit -Hn</span><br><span class="line">ulimit -Sn</span><br></pre></td></tr></table></figure></li></ul><h4 id="max-virtual-memory-areas-vm-max-map-count-65530-is-too-low-increase-to-at-least-262144"><a href="#max-virtual-memory-areas-vm-max-map-count-65530-is-too-low-increase-to-at-least-262144" class="headerlink" title="max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]"></a>max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</h4><ul><li><p><code>vim /etc/sysctl.conf </code></p><p>新增<strong>vm.max_map_count=655360</strong></p></li><li><p><code>sysctl -p</code></p></li></ul><h4 id="the-default-discovery-settings-are-unsuitable-for-production-use-at-least-one-of-discovery-seed-hosts-discovery-seed-providers-cluster-initial-master-nodes-must-be-configured"><a href="#the-default-discovery-settings-are-unsuitable-for-production-use-at-least-one-of-discovery-seed-hosts-discovery-seed-providers-cluster-initial-master-nodes-must-be-configured" class="headerlink" title="the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured"></a>the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured</h4><p>in short, if you are running Elasticsearch locally(single node) or just with a single node on the cloud then just use below config in your <code>elasticsearch.yml</code> to avoid the production check, and to make it work, more info about this config in <a href="https://stackoverflow.com/a/60426167/4039431">this SO</a> answer:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">discovery.type:</span> <span class="string">single-node</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;ELK&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elasticsearch 是一个搜索和分析引擎。Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”</summary>
      
    
    
    
    <category term="Elasticsearch" scheme="http://yoursite.com/categories/Elasticsearch/"/>
    
    
    <category term="learn" scheme="http://yoursite.com/tags/learn/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes learn</title>
    <link href="http://yoursite.com/2021/12/02/Kubernetes%20snippet/"/>
    <id>http://yoursite.com/2021/12/02/Kubernetes%20snippet/</id>
    <published>2021-12-02T08:57:50.000Z</published>
    <updated>2021-12-02T09:02:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes is pronounced <strong>coo-ber-net-ees</strong>, not coo-ber-neats. People also use the shortened version k8s a lot. Please don’t pronounce that one k-eights—it is still coo-ber-net-ees.</p><h3 id="Difference-between-Docker-and-Kubernetes"><a href="#Difference-between-Docker-and-Kubernetes" class="headerlink" title="Difference between Docker and Kubernetes"></a>Difference between Docker and Kubernetes</h3><blockquote><p>Docker is a containerization platform, and Kubernetes is a container orchestrator for container platforms like Docker. </p></blockquote><h3 id="Docker-Container-Problems"><a href="#Docker-Container-Problems" class="headerlink" title="Docker Container Problems:"></a>Docker Container Problems:</h3><ul><li>How would all of these containers be coordinated and scheduled? </li><li>How do you seamlessly upgrade an application without any interruption of service? </li><li>How do you monitor the health of an application, know when something goes wrong and seamlessly restart it? </li></ul><p>When most people talk about “Kubernetes vs. Docker,” what they really mean is “Kubernetes vs. Docker Swarm.” </p><h3 id="Kubernetes-architecture-and-its-components"><a href="#Kubernetes-architecture-and-its-components" class="headerlink" title="Kubernetes architecture and its components"></a>Kubernetes architecture and its components</h3><p>We can break down the components into three main parts.</p><ol><li>The Control Plane - The Master.</li><li>Nodes - Where pods get scheduled.</li><li>Pods - Holds containers.</li></ol><p><img src="./img/kubernetes_cluster.png"></p><p>Docker is a platform and tool for building, distributing, and running Docker containers. It offers its own native clustering tool that can be used to orchestrate and schedule containers on machine clusters. Kubernetes is a container orchestration system for Docker containers that is more extensive than Docker Swarm and is meant to coordinate clusters of nodes at scale in production in an efficient manner. It works around the concept of pods, which are scheduling units (and can contain one or more containers) in the Kubernetes ecosystem, and they are distributed among nodes to provide high availability. One can easily run a Docker build on a Kubernetes cluster, but Kubernetes itself is not a complete solution and is meant to include custom plugins.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Kubernetes is pronounced &lt;strong&gt;coo-ber-net-ees&lt;/strong&gt;, not coo-ber-neats. People also use the shortened version k8s a lot. Please don</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
    <category term="learn" scheme="http://yoursite.com/tags/learn/"/>
    
  </entry>
  
  <entry>
    <title>nacos snippet</title>
    <link href="http://yoursite.com/2021/12/01/nacos%20snippet/"/>
    <id>http://yoursite.com/2021/12/01/nacos%20snippet/</id>
    <published>2021-12-01T07:14:40.000Z</published>
    <updated>2021-12-01T07:15:43.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><ul><li><p>2.0.3版本启动，需要添加 -m属性</p><p><code>./startup.sh -m standalone</code></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;启动&quot;&gt;&lt;a href=&quot;#启动&quot; class=&quot;headerlink&quot; title=&quot;启动&quot;&gt;&lt;/a&gt;启动&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2.0.3版本启动，需要添加 -m属性&lt;/p&gt;
&lt;p&gt;&lt;code&gt;./startup.sh -m standalone&lt;/</summary>
      
    
    
    
    <category term="spring cloud" scheme="http://yoursite.com/categories/spring-cloud/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>postgresql snippet</title>
    <link href="http://yoursite.com/2021/12/01/postgresql%20snippet/"/>
    <id>http://yoursite.com/2021/12/01/postgresql%20snippet/</id>
    <published>2021-12-01T02:30:22.000Z</published>
    <updated>2021-12-01T08:30:09.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="终端登录pg"><a href="#终端登录pg" class="headerlink" title="终端登录pg"></a>终端登录pg</h3><ul><li><p>如果之前没有登录过，需要设置当前用户进行登录操作</p><p>There is no default username and password without you creating one. The simplest possible setup is to follow these steps to set up your own user as a superuser.</p><p>At a terminal prompt, create a postgres user with your own username</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u postgres createuser --superuser $USER# $USER无须替换</span><br></pre></td></tr></table></figure><p>Start the postgresql command prompt as your username but running as root since you didn’t set a password yet;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u postgres psql</span><br></pre></td></tr></table></figure><p>At the postgresql prompt, set your password;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\password $USER    # 其中$USER需要替换成当前用户名</span><br></pre></td></tr></table></figure><p>After that, you should be able to log on just fine.</p></li></ul><ul><li><p>如果之前设置了上面的步骤，可直接运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psql postgres</span><br></pre></td></tr></table></figure></li></ul><h3 id="导入文件"><a href="#导入文件" class="headerlink" title="导入文件"></a>导入文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">psql postgres# login command</span><br><span class="line">\c some_database# choose database</span><br><span class="line">\i \path\TO\file_name.sql# execute sql</span><br></pre></td></tr></table></figure><h3 id="连接指定schema"><a href="#连接指定schema" class="headerlink" title="连接指定schema"></a>连接指定schema</h3><ul><li><p>如果不指定schema的话，会默认访问public的schema</p></li><li><p>指定schema</p><p><code>jdbc:postgresql://localhost:5432/mydatabase?currentSchema=myschema</code></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;终端登录pg&quot;&gt;&lt;a href=&quot;#终端登录pg&quot; class=&quot;headerlink&quot; title=&quot;终端登录pg&quot;&gt;&lt;/a&gt;终端登录pg&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果之前没有登录过，需要设置当前用户进行登录操作&lt;/p&gt;
&lt;p&gt;There is no d</summary>
      
    
    
    
    <category term="fragment" scheme="http://yoursite.com/categories/fragment/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
  <entry>
    <title>gradle snippet</title>
    <link href="http://yoursite.com/2021/11/23/gradle/"/>
    <id>http://yoursite.com/2021/11/23/gradle/</id>
    <published>2021-11-23T09:03:43.000Z</published>
    <updated>2021-11-23T09:04:48.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol><li><strong>Process ‘command ‘/opt/jdk8/bin/java’’ finished with non-zero exit value 1</strong></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Process ‘command ‘/opt/jdk8/bin/java’’ finished with non</summary>
      
    
    
    
    <category term="tool" scheme="http://yoursite.com/categories/tool/"/>
    
    
    <category term="snippet" scheme="http://yoursite.com/tags/snippet/"/>
    
  </entry>
  
</feed>
